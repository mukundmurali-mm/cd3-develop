{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CD3 Automation Toolkit Introduction The CD3 toolkit reads input data in the form of CD3 Excel sheet and generates Terraform files which can be used to provision the resources in OCI instead of handling the task through the OCI console manually. The toolkit also reverse engineers the components in OCI back to the Excel sheet and Terraform configuration. The toolkit can be used throughout the lifecycle of tenancy to continuously create or modify existing resources. The generated Terraform code can be used by the OCI Resource Manager or can be integrated into organization's existing devops CI/CD ecosystem. \ud83d\udccc OCI Services Currently Supported by Automation Toolkit OCI Services Details IAM/Identity Compartments, Groups, Dynamic Groups, Policies, Users, Network Sources Governance Tags (Namespaces, Tag Keys, Defined Tags, Default Tags, Cost Tracking) Network VCNs, Subnets, VLANs, DRGs, IGWs, NGWs, LPGs, Route Tables, DRG Route, Tables, Security Lists, Network Security Groups, Remote Peering Connections, Application Load Balancer, Network Load Balancers DNS Management Private DNS - Views, Zones, rrsets/records and Resolvers Compute Instances supporting Market Place Images, Remote Exec, Cloud-Init scripts, Dedicated VM Hosts Storage FSS, Block and Boot Volumes, Backup Policies, Object Storage Buckets and logging for write events Database Exa Infra, ExaCS, DB Systems VM and BM, ATP, ADW Management Services Events, Notifications, Alarms, Service Connector Hub (SCH) Developer Services Resource Manager, Oracle Kubernetes Engine (OKE) Logging Services VCN Flow Logs, LBaaS access and error Logs, OSS Buckets write Logs SDDCs Oracle Cloud VMWare Solutions (Single Cluster is supported as of now. Multi-cluster support will be included in the upcoming release) CIS Landing Zone Compliance Download and Execute CIS Compliance Check Script, Cloud Guard, Key Vault, Budget Policy Enforcement OPA - Open Policy Agent \ud83d\udd04 High level workflow First, The Excel file is input to the CD3 Automation toolkit using either CLI or Jenkins. Note: \ud83d\udcd6 Detailed documentations and videos are provided for both options. Please check the left panel for navigation. Based on the workflow selected (create/export), the toolkit processes the next steps. Create resources: The input Excel file is processed by the toolkit and terraform *auto.tfvars files are generated for all those reosurces. The generated terraform files can be used to deploy resources in OCI by generating a terraform plan and approving the plan for apply. Export resources The input Excel (preferably the Blank template) is processed by the toolkit and resources are exported to CD3 Excel template. The toolkit then generates *auto.tfvars from the exported data in Excel file and also generates shell scripts with terraform import commands for all the reosurces. The shell scripts have to be executed in order to have the updated state file to manage the resources further. Please refer to the \"GETTING-STARTED\" sections in the navigation panel to start your journey with the toolkit. Happy Automation!!","title":"Introduction"},{"location":"#cd3-automation-toolkit","text":"","title":"CD3 Automation Toolkit"},{"location":"#introduction","text":"The CD3 toolkit reads input data in the form of CD3 Excel sheet and generates Terraform files which can be used to provision the resources in OCI instead of handling the task through the OCI console manually. The toolkit also reverse engineers the components in OCI back to the Excel sheet and Terraform configuration. The toolkit can be used throughout the lifecycle of tenancy to continuously create or modify existing resources. The generated Terraform code can be used by the OCI Resource Manager or can be integrated into organization's existing devops CI/CD ecosystem.","title":"Introduction"},{"location":"#oci-services-currently-supported-by-automation-toolkit","text":"OCI Services Details IAM/Identity Compartments, Groups, Dynamic Groups, Policies, Users, Network Sources Governance Tags (Namespaces, Tag Keys, Defined Tags, Default Tags, Cost Tracking) Network VCNs, Subnets, VLANs, DRGs, IGWs, NGWs, LPGs, Route Tables, DRG Route, Tables, Security Lists, Network Security Groups, Remote Peering Connections, Application Load Balancer, Network Load Balancers DNS Management Private DNS - Views, Zones, rrsets/records and Resolvers Compute Instances supporting Market Place Images, Remote Exec, Cloud-Init scripts, Dedicated VM Hosts Storage FSS, Block and Boot Volumes, Backup Policies, Object Storage Buckets and logging for write events Database Exa Infra, ExaCS, DB Systems VM and BM, ATP, ADW Management Services Events, Notifications, Alarms, Service Connector Hub (SCH) Developer Services Resource Manager, Oracle Kubernetes Engine (OKE) Logging Services VCN Flow Logs, LBaaS access and error Logs, OSS Buckets write Logs SDDCs Oracle Cloud VMWare Solutions (Single Cluster is supported as of now. Multi-cluster support will be included in the upcoming release) CIS Landing Zone Compliance Download and Execute CIS Compliance Check Script, Cloud Guard, Key Vault, Budget Policy Enforcement OPA - Open Policy Agent","title":"\ud83d\udccc OCI Services Currently Supported by Automation Toolkit"},{"location":"#high-level-workflow","text":"First, The Excel file is input to the CD3 Automation toolkit using either CLI or Jenkins. Note: \ud83d\udcd6 Detailed documentations and videos are provided for both options. Please check the left panel for navigation. Based on the workflow selected (create/export), the toolkit processes the next steps. Create resources: The input Excel file is processed by the toolkit and terraform *auto.tfvars files are generated for all those reosurces. The generated terraform files can be used to deploy resources in OCI by generating a terraform plan and approving the plan for apply. Export resources The input Excel (preferably the Blank template) is processed by the toolkit and resources are exported to CD3 Excel template. The toolkit then generates *auto.tfvars from the exported data in Excel file and also generates shell scripts with terraform import commands for all the reosurces. The shell scripts have to be executed in order to have the updated state file to manage the resources further. Please refer to the \"GETTING-STARTED\" sections in the navigation panel to start your journey with the toolkit. Happy Automation!!","title":"\ud83d\udd04 High level workflow"},{"location":"Auth_Mechanisms_in_OCI/","text":"OCI SDK Authentication Methods Choose one of the below authentication mechanisms to be used for the toolkit execution - API key-based authentication Session token-based authentication Instance principal API key-based authentication Follow below steps to use API key-based authentication - Create API PEM Key RSA key pair in PEM format (minimum 2048 bits) is needed to use OCI APIs. If the key pair does not exist, create them using below command inside docker container: cd /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/ python createAPIKey.py \u2192 This will generate the public/private key pair (oci_api_public.pem and oci_api_private.pem) at /cd3user/tenancies/keys/ In case you already have the keys, you can copy the private key file inside the container at /cd3user/tenancies/keys/ Upload Public Key Upload the Public key to \"APIkeys\" under user settings in OCI Console. Open the Console, and sign in as the user. View the details for the user who will be calling the API with the key pair. Open the Profile menu (User menu icon) and click User Settings. Click Add Public Key. Paste the contents of the PEM public key in the dialog box and click Add. Note: Please note down these details for next step - User OCID, Private Key path, Fingerprint, Tenancy OCID. The User should have administrator access to the tenancy to use complete functionality of the toolkit. Session token-based authentication Follow below steps to use Session token-based authentication - Use below command to create config inside the container. This is needed to generate session token. You can skip this step, if you already have a valid config(with API key) and uploaded the public key to OCI for a user. In that case, you can copy the config file and private API Key inside the container at /cd3user/.oci oci setup config Execute oci session authenticate --no-browser to generate session token for the private key. Follow the questions. Enter 'DEFAULT' for the profile name and proceed to update the config file with session token information at default location /cd3user/.oci 3. Token will be generated at default location /cd3user/.oci Note: createTenancyConfig.py script will use the config file located at /cd3user/.oci path. And toolkit supports profile name as DEFAULT only. Generated session token will have maximum 60 minutes validity. You will have to follow from step 1 if new session token is required after expiry. The User should have administrator access to the tenancy to use complete functionality of the toolkit. Instance principal Follow below steps to use Instance Principal authentication - Launch an Instance in the tenancy and set up the toolkit docker container on that instance. Create Dynamic Group for this instance. Write IAM policy to assign privileges to this dynamic group. The dynamic group(containing the instance) should have administrator access to the tenancy to use complete functionality of the toolkit.","title":"OCI SDK Authentication Methods"},{"location":"Auth_Mechanisms_in_OCI/#oci-sdk-authentication-methods","text":"Choose one of the below authentication mechanisms to be used for the toolkit execution - API key-based authentication Session token-based authentication Instance principal","title":"OCI SDK Authentication Methods"},{"location":"Auth_Mechanisms_in_OCI/#api-key-based-authentication","text":"Follow below steps to use API key-based authentication - Create API PEM Key RSA key pair in PEM format (minimum 2048 bits) is needed to use OCI APIs. If the key pair does not exist, create them using below command inside docker container: cd /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/ python createAPIKey.py \u2192 This will generate the public/private key pair (oci_api_public.pem and oci_api_private.pem) at /cd3user/tenancies/keys/ In case you already have the keys, you can copy the private key file inside the container at /cd3user/tenancies/keys/ Upload Public Key Upload the Public key to \"APIkeys\" under user settings in OCI Console. Open the Console, and sign in as the user. View the details for the user who will be calling the API with the key pair. Open the Profile menu (User menu icon) and click User Settings. Click Add Public Key. Paste the contents of the PEM public key in the dialog box and click Add. Note: Please note down these details for next step - User OCID, Private Key path, Fingerprint, Tenancy OCID. The User should have administrator access to the tenancy to use complete functionality of the toolkit.","title":"API key-based authentication"},{"location":"Auth_Mechanisms_in_OCI/#session-token-based-authentication","text":"Follow below steps to use Session token-based authentication - Use below command to create config inside the container. This is needed to generate session token. You can skip this step, if you already have a valid config(with API key) and uploaded the public key to OCI for a user. In that case, you can copy the config file and private API Key inside the container at /cd3user/.oci oci setup config Execute oci session authenticate --no-browser to generate session token for the private key. Follow the questions. Enter 'DEFAULT' for the profile name and proceed to update the config file with session token information at default location /cd3user/.oci 3. Token will be generated at default location /cd3user/.oci Note: createTenancyConfig.py script will use the config file located at /cd3user/.oci path. And toolkit supports profile name as DEFAULT only. Generated session token will have maximum 60 minutes validity. You will have to follow from step 1 if new session token is required after expiry. The User should have administrator access to the tenancy to use complete functionality of the toolkit.","title":"Session token-based authentication"},{"location":"Auth_Mechanisms_in_OCI/#instance-principal","text":"Follow below steps to use Instance Principal authentication - Launch an Instance in the tenancy and set up the toolkit docker container on that instance. Create Dynamic Group for this instance. Write IAM policy to assign privileges to this dynamic group. The dynamic group(containing the instance) should have administrator access to the tenancy to use complete functionality of the toolkit.","title":"Instance principal"},{"location":"CD3ExcelTabs/","text":"OCI Services currently supported by Automation Toolkit Click on the links below to know about the specifics of each tab in the excel sheet. IAM/Identity Compartments Groups Policies Users Network Sources Click here to view sample auto.tfvars for the Identity components Governance Tags Click here to view sample auto.tfvars for Governance components Network VCNs DRGs VCN Info DHCP SubnetsVLANs Click here to view sample auto.tfvars for all Network components- VCNs, Subnets, Gateways etc., Private-DNS DNS-Views-Zones-Records DNS-Resolvers Click here to view sample auto.tfvars for all DNS components Load Balancer LB-Hostname-Certs Tab LB-Backend Set and Backend Servers LB-RuleSet LB-PathRouteSet LB-Listener Click here to view sample auto.tfvars for all Load Balancer components- Cipher suits,Backend sets, rule sets etc., Compute DedicatedVMHosts Instances Click here to view sample auto.tfvars for Compute components-Virtual Machine Storage BlockVolumes Click here to view sample auto.tfvars for Block Volumes FSS Object Storage Buckets Click here to view sample auto.tfvars for Object Storage Buckets Database DBSystems-VM-BM ExaCS ADB Management Services Notifications Alarms Click here to view sample auto.tfvars for management services Alarms, Notifications, Events etc., ServiceConnectors Click here to view sample auto.tfvars for Service Connectors Developer Services OKE Click here to view sample auto.tfvars for OKE components- Clusters, Nodepools Logging Services VCN Flow Logs LBaaS Logs OSS Logs Click here to view sample auto.tfvars for Logging components SDDCs Tab OCVS Click here to view sample auto.tfvars for OCVS","title":"Adding resources data in Excel templates"},{"location":"CD3ExcelTabs/#oci-services-currently-supported-by-automation-toolkit","text":"Click on the links below to know about the specifics of each tab in the excel sheet.","title":"OCI Services currently supported by Automation Toolkit"},{"location":"CD3ExcelTabs/#iamidentity","text":"Compartments Groups Policies Users Network Sources Click here to view sample auto.tfvars for the Identity components","title":"IAM/Identity"},{"location":"CD3ExcelTabs/#governance","text":"Tags Click here to view sample auto.tfvars for Governance components","title":"Governance"},{"location":"CD3ExcelTabs/#network","text":"VCNs DRGs VCN Info DHCP SubnetsVLANs Click here to view sample auto.tfvars for all Network components- VCNs, Subnets, Gateways etc.,","title":"Network"},{"location":"CD3ExcelTabs/#private-dns","text":"DNS-Views-Zones-Records DNS-Resolvers Click here to view sample auto.tfvars for all DNS components","title":"Private-DNS"},{"location":"CD3ExcelTabs/#load-balancer","text":"LB-Hostname-Certs Tab LB-Backend Set and Backend Servers LB-RuleSet LB-PathRouteSet LB-Listener Click here to view sample auto.tfvars for all Load Balancer components- Cipher suits,Backend sets, rule sets etc.,","title":"Load Balancer"},{"location":"CD3ExcelTabs/#compute","text":"DedicatedVMHosts Instances Click here to view sample auto.tfvars for Compute components-Virtual Machine","title":"Compute"},{"location":"CD3ExcelTabs/#storage","text":"BlockVolumes Click here to view sample auto.tfvars for Block Volumes FSS Object Storage Buckets Click here to view sample auto.tfvars for Object Storage Buckets","title":"Storage"},{"location":"CD3ExcelTabs/#database","text":"DBSystems-VM-BM ExaCS ADB","title":"Database"},{"location":"CD3ExcelTabs/#management-services","text":"Notifications Alarms Click here to view sample auto.tfvars for management services Alarms, Notifications, Events etc., ServiceConnectors Click here to view sample auto.tfvars for Service Connectors","title":"Management Services"},{"location":"CD3ExcelTabs/#developer-services","text":"OKE Click here to view sample auto.tfvars for OKE components- Clusters, Nodepools","title":"Developer Services"},{"location":"CD3ExcelTabs/#logging-services","text":"VCN Flow Logs LBaaS Logs OSS Logs Click here to view sample auto.tfvars for Logging components","title":"Logging Services"},{"location":"CD3ExcelTabs/#sddcs-tab","text":"OCVS Click here to view sample auto.tfvars for OCVS","title":"SDDCs Tab"},{"location":"CISFeatures/","text":"Additional CIS Compliance Features These are some additional \" CIS Compliance Features \" which are not part of CD3 Excel sheet but just included into setUpOCI Menu. 1. Run CIS compliance checker script You can choose to run CIS compliance checker script against your tennacy using the Automation Toolkit itself. It also enables you to download the latest script if needed. Folder with name <customer_name>_cis_report gets created under /cd3user/tenancies/<customer_name>/ and it contains all the reports genertaed by the script. As a best practice, the script should be executed after every deployment in the tenancy. The output report should be analysed to minimise the reported anomalies as per the design requirements. 2. Create Key/Vault: Below tf file is created File name Description cis-keyvault.auto.tfvars TF variables file for creating the key/vault in the specified compartment and region. This is created under the specified region directory. 3. Create Default Budget: This option will ask for monthly budget (in US$) and Threshold percentage of Budget and bellow tf files are created: File name Description cis-budget.auto.tfvars TF variables file for crating budget. 4. Enable Cloud guard This will enable Cloud guard for the tenancy from specified reporting region, clones the Oracle Managed detector and responder recipes. Creates a target for root compartment with the cloned recipes. Below TF file is created: File name Description cis-cloudguard.auto.tf vars TF variables file for enabling cloud guard and creating target for root compartment.","title":"Additional CIS Compliance Features"},{"location":"CISFeatures/#additional-cis-compliance-features","text":"These are some additional \" CIS Compliance Features \" which are not part of CD3 Excel sheet but just included into setUpOCI Menu.","title":"Additional CIS Compliance Features"},{"location":"CISFeatures/#1-run-cis-compliance-checker-script","text":"You can choose to run CIS compliance checker script against your tennacy using the Automation Toolkit itself. It also enables you to download the latest script if needed. Folder with name <customer_name>_cis_report gets created under /cd3user/tenancies/<customer_name>/ and it contains all the reports genertaed by the script. As a best practice, the script should be executed after every deployment in the tenancy. The output report should be analysed to minimise the reported anomalies as per the design requirements.","title":"1. Run CIS compliance checker script"},{"location":"CISFeatures/#2-create-keyvault","text":"Below tf file is created File name Description cis-keyvault.auto.tfvars TF variables file for creating the key/vault in the specified compartment and region. This is created under the specified region directory.","title":"2. Create Key/Vault:"},{"location":"CISFeatures/#3-create-default-budget","text":"This option will ask for monthly budget (in US$) and Threshold percentage of Budget and bellow tf files are created: File name Description cis-budget.auto.tfvars TF variables file for crating budget.","title":"3. Create Default Budget:"},{"location":"CISFeatures/#4-enable-cloud-guard","text":"This will enable Cloud guard for the tenancy from specified reporting region, clones the Oracle Managed detector and responder recipes. Creates a target for root compartment with the cloned recipes. Below TF file is created: File name Description cis-cloudguard.auto.tf vars TF variables file for enabling cloud guard and creating target for root compartment.","title":"4. Enable Cloud guard"},{"location":"ComputeGF/","text":"Provisioning Compute Instances on OCI Provisioning of compute instances using Automation Toolkit involves the below steps: Add the VM details to the \"Instances\" Excel Sheet. Running the toolkit to generate auto.tfvars. Executing Terraform commands to provision the resources in OCI. Note below points while adding VM details in the Instances sheet: \"Display Name\" column is case sensitive. Specified value will be the display name of Instance in OCI console. Optional columns can also be left blank - like Fault Domain, IP Address. They will take default values when left empty. Leave columns: Backup Policy, NSGs, DedicatedVMHost blank if instance doesn't need to be part of any of these. Instances can be made a part of Backup Policy and NSGs later by choosing appropriate option in setUpOCI menu. The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the instance_ssh_keys variable containing the key value. Make sure to have an entry in variables_\\ .tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_\\ .tf file as shown below: variable 'instance_ssh_keys' { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use '\\n' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = \"ssh-rsa AAXXX......yhdlo\\nssh-rsa AAxxskj...edfwf\" #START_instance_ssh_keys# # exported instance ssh keys #instance_ssh_keys_END# } } 5. Enter subnet name column value as: \\ _\\ 6. Source Details column of the excel sheet accepts both image and boot volume as the source for instance to be launched. Format: image::\\<variable containing ocid of image> or bootVolume::\\<variable containing ocid of boot volume> Make sure to have an entry in variables_\\ .tf file for the value you enter in Source Details field of the Excel sheet. Ex: If you enter the Source Details as image::Linux, make an entry in variables_\\ .tf file under the instance_source_ocids variable as shown below: variable 'instance_source_ocids' { type = map(any) Linux = \"<LATEST LINUX OCID HERE>\" Windows = \"<LATEST WINDOWS OCID HERE>\" PaloAlto = \"Palo Alto Networks VM-Series Next Generation Firewall\" #START_instance_source_ocids# # exported instance image ocids #instance_source_ocids_END# } 7. Mention shape to be used in Shape column of the excel sheet. If Flex shape is to be used format is: shape::ocpus eg: VM.Standard.E3.Flex::5 8. Custom Policy Compartment Name : Specify the compartment name where the Custom Policy is created. 9. Create_Is PV Encryption In Transit Enabled attribute should be set to True to enable encryption for new instances. Default is False. 10. Update_Is PV Encryption In Transit Enabled attribute should be set to True to enable encryption for existing instances. 11. Add any additonal attributes (not part of excel sheet already) as per this . 12. To enable or disable a plugin for the instance add new column with name as 'Plugin \\ ' eg 'Plugin Bastion'. Valid values are Enabled or Disabled Remote Execution/Cloud Init Scenarios Managing Remote Execution NOTE- Before you start with configuring remote execution for OCI instance(s) please ensure network connectivity through Bastion host or direct reach to the OCI instance(s) from where terraform is being invoked. Remote execution should be used as the last resort or only during initial provisioning for a given OCI instance(s). This feature cannot be used for export of instances. Add the 'Remote Execute' columm to the excel sheet for the corresponding instance entry. Format is: bastion_ip@ Please skip bastion_ip if there is direct connectivity with target servers via VPN. Scripts folder should have the ansible script files or shell script files and ssh-keys for instances and bastion host. The .yaml or .yml extensions will be considered for the ansible script files and .sh extensions will be considered for shell scripts files. For block-volume attachment configuration via ansible playbook the device name is must, and it's currently set to \"/dev/oracleoci/oraclevdb\" in the sample ansible playbook. Common scenarios like security hardening and other common shell scripts can be executed against the OCI instances during provisioning. Running the CD3 automation toolkit will generate auto.tfvars. Execute Terraform commands to provision the instances in OCI. Remote executioner will also run after the instance provisioning. The users can refer to the default.yaml file which is inside /cd3user/tenancies/<customer_name>/terraform_files/<region>/scripts dir for provisioning the custom playbooks. Managing Cloud Init Add the 'Cloud Init Script' column to the excel sheet for the corresponding instance entry. Scripts folder should have the relevant script files for instances. The *.sh extension(s) will be considered for the script files. Common scenarios like security hardening and other common scripts can be executed against the OCI instances during provisioning. Running the CD3 automation toolkit will generate auto.tfvars. Execute Terraform commands to provision the instances in OCI and run cloud-init scripts during provisioning. Executing the setUpOCI.py script: On choosing \"Compute\" in the SetUpOCI menu and \"Add/Modify/Delete Instances/Boot Backup Policy\" submenu will allow to launch your VM on OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_instances.auto.tfvars and <outdir>/<region>/<prefix>_boot-backup-policy.auto.tfvars under appropriate directory. Once the terraform apply is complete, view the resources under Compute -> Instances for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_instances/<Date>-<Month>-<Time>.","title":"Create Compute Instances-CLI"},{"location":"ComputeGF/#provisioning-compute-instances-on-oci","text":"Provisioning of compute instances using Automation Toolkit involves the below steps: Add the VM details to the \"Instances\" Excel Sheet. Running the toolkit to generate auto.tfvars. Executing Terraform commands to provision the resources in OCI. Note below points while adding VM details in the Instances sheet: \"Display Name\" column is case sensitive. Specified value will be the display name of Instance in OCI console. Optional columns can also be left blank - like Fault Domain, IP Address. They will take default values when left empty. Leave columns: Backup Policy, NSGs, DedicatedVMHost blank if instance doesn't need to be part of any of these. Instances can be made a part of Backup Policy and NSGs later by choosing appropriate option in setUpOCI menu. The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the instance_ssh_keys variable containing the key value. Make sure to have an entry in variables_\\ .tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_\\ .tf file as shown below: variable 'instance_ssh_keys' { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use '\\n' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = \"ssh-rsa AAXXX......yhdlo\\nssh-rsa AAxxskj...edfwf\" #START_instance_ssh_keys# # exported instance ssh keys #instance_ssh_keys_END# } } 5. Enter subnet name column value as: \\ _\\ 6. Source Details column of the excel sheet accepts both image and boot volume as the source for instance to be launched. Format: image::\\<variable containing ocid of image> or bootVolume::\\<variable containing ocid of boot volume> Make sure to have an entry in variables_\\ .tf file for the value you enter in Source Details field of the Excel sheet. Ex: If you enter the Source Details as image::Linux, make an entry in variables_\\ .tf file under the instance_source_ocids variable as shown below: variable 'instance_source_ocids' { type = map(any) Linux = \"<LATEST LINUX OCID HERE>\" Windows = \"<LATEST WINDOWS OCID HERE>\" PaloAlto = \"Palo Alto Networks VM-Series Next Generation Firewall\" #START_instance_source_ocids# # exported instance image ocids #instance_source_ocids_END# } 7. Mention shape to be used in Shape column of the excel sheet. If Flex shape is to be used format is: shape::ocpus eg: VM.Standard.E3.Flex::5 8. Custom Policy Compartment Name : Specify the compartment name where the Custom Policy is created. 9. Create_Is PV Encryption In Transit Enabled attribute should be set to True to enable encryption for new instances. Default is False. 10. Update_Is PV Encryption In Transit Enabled attribute should be set to True to enable encryption for existing instances. 11. Add any additonal attributes (not part of excel sheet already) as per this . 12. To enable or disable a plugin for the instance add new column with name as 'Plugin \\ ' eg 'Plugin Bastion'. Valid values are Enabled or Disabled","title":"Provisioning Compute Instances on OCI"},{"location":"ComputeGF/#remote-executioncloud-init-scenarios","text":"","title":"Remote Execution/Cloud Init Scenarios"},{"location":"ComputeGF/#managing-remote-execution","text":"NOTE- Before you start with configuring remote execution for OCI instance(s) please ensure network connectivity through Bastion host or direct reach to the OCI instance(s) from where terraform is being invoked. Remote execution should be used as the last resort or only during initial provisioning for a given OCI instance(s). This feature cannot be used for export of instances. Add the 'Remote Execute' columm to the excel sheet for the corresponding instance entry. Format is: bastion_ip@ Please skip bastion_ip if there is direct connectivity with target servers via VPN. Scripts folder should have the ansible script files or shell script files and ssh-keys for instances and bastion host. The .yaml or .yml extensions will be considered for the ansible script files and .sh extensions will be considered for shell scripts files. For block-volume attachment configuration via ansible playbook the device name is must, and it's currently set to \"/dev/oracleoci/oraclevdb\" in the sample ansible playbook. Common scenarios like security hardening and other common shell scripts can be executed against the OCI instances during provisioning. Running the CD3 automation toolkit will generate auto.tfvars. Execute Terraform commands to provision the instances in OCI. Remote executioner will also run after the instance provisioning. The users can refer to the default.yaml file which is inside /cd3user/tenancies/<customer_name>/terraform_files/<region>/scripts dir for provisioning the custom playbooks.","title":"Managing Remote Execution"},{"location":"ComputeGF/#managing-cloud-init","text":"Add the 'Cloud Init Script' column to the excel sheet for the corresponding instance entry. Scripts folder should have the relevant script files for instances. The *.sh extension(s) will be considered for the script files. Common scenarios like security hardening and other common scripts can be executed against the OCI instances during provisioning. Running the CD3 automation toolkit will generate auto.tfvars. Execute Terraform commands to provision the instances in OCI and run cloud-init scripts during provisioning.","title":"Managing Cloud Init"},{"location":"ComputeGF/#executing-the-setupocipy-script","text":"On choosing \"Compute\" in the SetUpOCI menu and \"Add/Modify/Delete Instances/Boot Backup Policy\" submenu will allow to launch your VM on OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_instances.auto.tfvars and <outdir>/<region>/<prefix>_boot-backup-policy.auto.tfvars under appropriate directory. Once the terraform apply is complete, view the resources under Compute -> Instances for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_instances/<Date>-<Month>-<Time>.","title":"Executing the setUpOCI.py script:"},{"location":"ComputeNGF/","text":"Exporting Compute Instances from OCI Follow the below steps to export OCI compute Instances to CD3 Excel file and create the terraform state: Use the CD3-Blank-Template.xlsx to export existing OCI VM details into the \"Instances\" sheet. Add any additonal attributes (not part of excel sheet already) which needs to be exported as per this . Make sure to export the VCNs and Subnets in which the Instances are present prior to exporting the Instance. Execute the setupOCI.py file with non_gf_tenancy parameter value to true : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Provide the region from where the Instances have to be exported. Specify comma separated values for multiple regions. From the output menu, select Option 5: Export Compute >> Option 2: Export Instances . Enter the compartment to which the Instances belong to. If you have to export Instances from multiple compartments, specify comma separated compartment values. Specify the compartment name along with hierarchy in the below format: Parent Compartment1::Parent Compartment2::MyCompartment <br> To export only specific instances, specify the required filter values Enter comma separated list of display name patterns of the instances: Enter comma separated list of ADs of the instances eg AD1,AD2,AD3: Upon executing, the \"Instances\" sheet in input CD3 Excel is populated with the VMs details. The tf_import_commands_instances_nonGF.sh script, tfvars file are generated for the Instances under folder /cd3user/tenancies/ /terraform_files/ . If you are using multiple outdirectories, they'll be located under the /cd3user/tenancies/ /terraform_files/ compute folder. The associated ssh public keys are placed under variables_ .tf under the \"instance_ssh_keys\" variable. While export of instances, it will fetch details for only the primary VNIC attached to the instance. Execute the .sh file ( sh tf_import_commands_instances_nonGF.sh ) to generate terraform state file. Please read the known behaviour of toolkit for export of instances having multiple plugins.","title":"Export Compute Instances-CLI"},{"location":"ComputeNGF/#exporting-compute-instances-from-oci","text":"Follow the below steps to export OCI compute Instances to CD3 Excel file and create the terraform state: Use the CD3-Blank-Template.xlsx to export existing OCI VM details into the \"Instances\" sheet. Add any additonal attributes (not part of excel sheet already) which needs to be exported as per this . Make sure to export the VCNs and Subnets in which the Instances are present prior to exporting the Instance. Execute the setupOCI.py file with non_gf_tenancy parameter value to true : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Provide the region from where the Instances have to be exported. Specify comma separated values for multiple regions. From the output menu, select Option 5: Export Compute >> Option 2: Export Instances . Enter the compartment to which the Instances belong to. If you have to export Instances from multiple compartments, specify comma separated compartment values. Specify the compartment name along with hierarchy in the below format: Parent Compartment1::Parent Compartment2::MyCompartment <br> To export only specific instances, specify the required filter values Enter comma separated list of display name patterns of the instances: Enter comma separated list of ADs of the instances eg AD1,AD2,AD3: Upon executing, the \"Instances\" sheet in input CD3 Excel is populated with the VMs details. The tf_import_commands_instances_nonGF.sh script, tfvars file are generated for the Instances under folder /cd3user/tenancies/ /terraform_files/ . If you are using multiple outdirectories, they'll be located under the /cd3user/tenancies/ /terraform_files/ compute folder. The associated ssh public keys are placed under variables_ .tf under the \"instance_ssh_keys\" variable. While export of instances, it will fetch details for only the primary VNIC attached to the instance. Execute the .sh file ( sh tf_import_commands_instances_nonGF.sh ) to generate terraform state file. Please read the known behaviour of toolkit for export of instances having multiple plugins.","title":"Exporting Compute Instances from OCI"},{"location":"Connect_container_to_OCI_Tenancy/","text":"Connect container to OCI Tenancy Note: It is recommended to execute createTenancyConfig.py with a single customer_name within that container. Even if it is run multiple times with different customer names, Jenkins will only be configured for <customer_name> used while first time successful execution of the script. If there is a new region subscription to the tenancy at a later stage of time, createTenancyConfig.py must be re-run by using the same tenancyconfig.properties file that was originally used to create the configuration. Re-execution will create new directory for the new region under /cd3user/tenancies/<customer_name>/terraform_files without touching the existing ones and will commit the latest terraform_files folder to DevOps GIT repo. Step 1 - Exec into the Container : Run docker ps \u2192 Note down the container ID from this cmd output. Run docker exec -it <container_id> bash Step 2 - Choose Authentication Mechanism for OCI SDK Please click here to configure any one of the available authentication mechanisms. Step 3 - Edit tenancyconfig.properties : Run cd /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/ Fill the input parameters in tenancyconfig.properties file. Ensure to: Have the details ready for the Authentication mechanism you are planning to use. Use the same customer_name for a tenancy even if the script needs to be executed multiple times. Review 'outdir_structure_file' parameter as per requirements. It is recommended to use seperate outdir structure to manage a large number of resources. Review Advanced Parameters Section for CI/CD setup and be ready with user details that will be used to connect to DevOps Repo in OCI. Specifying these parameters as 'yes' in properties file will create Object Storage Bucket and Devops Git Repo/Project/Topic in OCI and enable toolkit usage via Jenkins. The toolkit supports users in primary IDCS stripes or default domains only for DevOps GIT operations. Step 4 - Initialise the environment : Initialise your environment to use the Automation Toolkit. python createTenancyConfig.py tenancyconfig.properties Note: If you are running docker container on a linux VM host, please refer to point no. 7 under FAQ to avoid any permission issues. Running the above command immediately after adding API key to the user profile in OCI might result in Authentication Errors. In such cases, please retry after a minute. \u2192 Example execution of the script with Advanced Parameters for CI/CD: Appendix Details of the files created on successful execution of above steps - Files Generated At File Path Comment/Purpose setUpOCI.properties /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Customer Specific properties outdir_structure_file.properties /cd3user/tenancies/<customer_name>/<customer_name>_outdir_structure_file Customer Specific properties file for outdir structure. This file will not be generated if 'outdir_structure_file' parameter was set to empty(single outdir) in tenancyconfig.properties while running createTenancyConfig.py Region based directories /cd3user/tenancies/<customer_name>/terraform_files Tenancy's subscribed regions based directories for the generation of terraform files. Each region directory will contain individual directory for each service based on the parameter 'outdir_structure_file' Variables File,Provider File, Root and Sub terraform modules /cd3user/tenancies/<customer_name>/terraform_files/<region> Required for terraform to work. Variables file and Provider file will be genrated based on authentication mechanism chosen. out file /cd3user/tenancies/<customer_name>/createTenancyConfig.out This file contains a copy of information displayed as the console output. OCI Config File /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_oci_config Customer specific Config file for OCI API calls. This will have data based on authentication mechanism chosen. Public and Private Key Pair Copied from /cd3user/tenancies/keys/ to /cd3user/tenancies/<customer_name>/.config_files API Key for authentication mechanism as API_Key are copied to customer specific out directory locations for easy access. GIT Config File /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_git_config Customer specific GIT Config file for OCI Dev Ops GIT operations. This is generated only if use_oci_devops_git is set to yes S3 Credentials File /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_s3_credentials This file contains access key and secret for S3 compatible OS bucket to manage remote terraform state. This is generated only if use_remote_state is set to yes Jenkins Home /cd3user/tenancies/jenkins_home This folder contains jenkins specific data. Single Jenkins instance can be setup for a single container. tenancyconfig.properties /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_tenancyconfig.properties The input properties file used to execute the script is copied to custome folder to retain for future reference. This can be used when the script needs to be re-run with same parameters at later stage. The next pages will guide you to use the toolkit either via CLI or via Jenkins. Please proceed further.","title":"Connect container to OCI tenancy"},{"location":"Connect_container_to_OCI_Tenancy/#connect-container-to-oci-tenancy","text":"Note: It is recommended to execute createTenancyConfig.py with a single customer_name within that container. Even if it is run multiple times with different customer names, Jenkins will only be configured for <customer_name> used while first time successful execution of the script. If there is a new region subscription to the tenancy at a later stage of time, createTenancyConfig.py must be re-run by using the same tenancyconfig.properties file that was originally used to create the configuration. Re-execution will create new directory for the new region under /cd3user/tenancies/<customer_name>/terraform_files without touching the existing ones and will commit the latest terraform_files folder to DevOps GIT repo.","title":"Connect container to OCI Tenancy"},{"location":"Connect_container_to_OCI_Tenancy/#step-1-exec-into-the-container","text":"Run docker ps \u2192 Note down the container ID from this cmd output. Run docker exec -it <container_id> bash","title":"Step 1 - Exec into the Container:"},{"location":"Connect_container_to_OCI_Tenancy/#step-2-choose-authentication-mechanism-for-oci-sdk","text":"Please click here to configure any one of the available authentication mechanisms.","title":"Step 2 - Choose Authentication Mechanism for OCI SDK"},{"location":"Connect_container_to_OCI_Tenancy/#step-3-edit-tenancyconfigproperties","text":"Run cd /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/ Fill the input parameters in tenancyconfig.properties file. Ensure to: Have the details ready for the Authentication mechanism you are planning to use. Use the same customer_name for a tenancy even if the script needs to be executed multiple times. Review 'outdir_structure_file' parameter as per requirements. It is recommended to use seperate outdir structure to manage a large number of resources. Review Advanced Parameters Section for CI/CD setup and be ready with user details that will be used to connect to DevOps Repo in OCI. Specifying these parameters as 'yes' in properties file will create Object Storage Bucket and Devops Git Repo/Project/Topic in OCI and enable toolkit usage via Jenkins. The toolkit supports users in primary IDCS stripes or default domains only for DevOps GIT operations.","title":"Step 3 - Edit tenancyconfig.properties:"},{"location":"Connect_container_to_OCI_Tenancy/#step-4-initialise-the-environment","text":"Initialise your environment to use the Automation Toolkit. python createTenancyConfig.py tenancyconfig.properties Note: If you are running docker container on a linux VM host, please refer to point no. 7 under FAQ to avoid any permission issues. Running the above command immediately after adding API key to the user profile in OCI might result in Authentication Errors. In such cases, please retry after a minute. \u2192 Example execution of the script with Advanced Parameters for CI/CD:","title":"Step 4 - Initialise the environment:"},{"location":"Connect_container_to_OCI_Tenancy/#appendix","text":"Details of the files created on successful execution of above steps - Files Generated At File Path Comment/Purpose setUpOCI.properties /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Customer Specific properties outdir_structure_file.properties /cd3user/tenancies/<customer_name>/<customer_name>_outdir_structure_file Customer Specific properties file for outdir structure. This file will not be generated if 'outdir_structure_file' parameter was set to empty(single outdir) in tenancyconfig.properties while running createTenancyConfig.py Region based directories /cd3user/tenancies/<customer_name>/terraform_files Tenancy's subscribed regions based directories for the generation of terraform files. Each region directory will contain individual directory for each service based on the parameter 'outdir_structure_file' Variables File,Provider File, Root and Sub terraform modules /cd3user/tenancies/<customer_name>/terraform_files/<region> Required for terraform to work. Variables file and Provider file will be genrated based on authentication mechanism chosen. out file /cd3user/tenancies/<customer_name>/createTenancyConfig.out This file contains a copy of information displayed as the console output. OCI Config File /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_oci_config Customer specific Config file for OCI API calls. This will have data based on authentication mechanism chosen. Public and Private Key Pair Copied from /cd3user/tenancies/keys/ to /cd3user/tenancies/<customer_name>/.config_files API Key for authentication mechanism as API_Key are copied to customer specific out directory locations for easy access. GIT Config File /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_git_config Customer specific GIT Config file for OCI Dev Ops GIT operations. This is generated only if use_oci_devops_git is set to yes S3 Credentials File /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_s3_credentials This file contains access key and secret for S3 compatible OS bucket to manage remote terraform state. This is generated only if use_remote_state is set to yes Jenkins Home /cd3user/tenancies/jenkins_home This folder contains jenkins specific data. Single Jenkins instance can be setup for a single container. tenancyconfig.properties /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_tenancyconfig.properties The input properties file used to execute the script is copied to custome folder to retain for future reference. This can be used when the script needs to be re-run with same parameters at later stage. The next pages will guide you to use the toolkit either via CLI or via Jenkins. Please proceed further.","title":"Appendix"},{"location":"ExcelTemplates/","text":"Excel Sheet Templates CD3 Excel sheet is the main input for Automation Toolkit. Below are the CD3 templates for the latest release having standardised IAM Components (compartments, groups and policies), Network Components and Events & Notifications Rules as per CIS Foundations Benchmark for Oracle Cloud. Details on how to fill data into the Excel sheet can be found in the Blue section of each sheet inside the Excel file. Make appropriate changes to the templates eg region and use for deployment. CD3 Excel templates for OCI core services: Excel Sheet Purpose CD3-Blank-template.xlsx Choose this template while exporting the existing resources from OCI into the CD3 and Terraform. CD3-CIS-template.xlsx This template has auto-filled in data of CIS Landing Zone for DRGv2. Choose this template to create Core OCI Objects (IAM, Tags, Networking, Instances, LBR, Storage, Databases) CD3-HubSpoke-template.xlsx This template has auto-filled in data for a Hub and Spoke model of networking. Choose this template to create Core OCI Objects (IAM, Tags, Networking, Instances, LBR, Storage, Databases) CD3-SingleVCN-template.xlsx This template has auto-filled in data for a Single VCN model of networking. Choose this template to create Core OCI Objects (IAM, Tags, Networking, Instances, LBR, Storage, Databases) CD3 Excel template for OCI Management services: Excel Sheet Purpose CD3-CIS-ManagementServices-template.xlsx This template has auto-filled in data of CIS Landing Zone. Choose this template while creating the components of Events, Alarms, Notifications and Service Connectors NOTE: The Excel Templates can also be found at \" /cd3user/oci_tools/cd3_automation_toolkit/example \" folder inside the container. After deploying the infra using any of the templates, please run CIS compliance checker script","title":"Excel Templates"},{"location":"ExcelTemplates/#excel-sheet-templates","text":"CD3 Excel sheet is the main input for Automation Toolkit. Below are the CD3 templates for the latest release having standardised IAM Components (compartments, groups and policies), Network Components and Events & Notifications Rules as per CIS Foundations Benchmark for Oracle Cloud. Details on how to fill data into the Excel sheet can be found in the Blue section of each sheet inside the Excel file. Make appropriate changes to the templates eg region and use for deployment. CD3 Excel templates for OCI core services: Excel Sheet Purpose CD3-Blank-template.xlsx Choose this template while exporting the existing resources from OCI into the CD3 and Terraform. CD3-CIS-template.xlsx This template has auto-filled in data of CIS Landing Zone for DRGv2. Choose this template to create Core OCI Objects (IAM, Tags, Networking, Instances, LBR, Storage, Databases) CD3-HubSpoke-template.xlsx This template has auto-filled in data for a Hub and Spoke model of networking. Choose this template to create Core OCI Objects (IAM, Tags, Networking, Instances, LBR, Storage, Databases) CD3-SingleVCN-template.xlsx This template has auto-filled in data for a Single VCN model of networking. Choose this template to create Core OCI Objects (IAM, Tags, Networking, Instances, LBR, Storage, Databases) CD3 Excel template for OCI Management services: Excel Sheet Purpose CD3-CIS-ManagementServices-template.xlsx This template has auto-filled in data of CIS Landing Zone. Choose this template while creating the components of Events, Alarms, Notifications and Service Connectors NOTE: The Excel Templates can also be found at \" /cd3user/oci_tools/cd3_automation_toolkit/example \" folder inside the container. After deploying the infra using any of the templates, please run CIS compliance checker script","title":"Excel Sheet Templates"},{"location":"FAQ/","text":"Frequently Asked Questions 1. Is there a way to verify my input CD3 Excel sheet for any typos/miskates? Yes, please choose 'Validate CD3' option in setUpOCI menu in GreenField workflow. It validates specific tabs of the excel sheet. Please see CD3 Validator Features for more details. 2. Can I use an existing outdir to export the data from OCI? Make sure to use a clean outdir without any .tfvars or .tfstate file. Also use a blank CD3 Excel sheet as export process will overwrite the data in the respective tab. 3. If I am already using the toolkit and my OCI tenancy has been subscribed to a new region, how do i use the new region with toolkit? Re-run createTenancyConfig.py with same details in tenancyConfig.properties file. It will keep existing region directories as is and create new directory for newly subscribed region. 4. How do I upgrade an existing version of the toolkit to the new one without disrupting my existing tenancy files/directories? Please look at Steps to Upgrade Your Toolkit. 5. How do I export instances in batches using different filters? Follow below steps: Modify the setUpOCI.properties file to set non_gf_tenancy to \"true\". Choose \"Export Compute\". Specify the filter - prefix of the instances or specific AD to export. Once the execution completes, take a backup of the files generated for instances in out directory( <customer_name>_instances. tfvars and tf_import_cmds _instances_nonGF.sh ) and a backup of the 'Instances' tab of the Input CD3 Excel Sheet. Repeat steps from 1 to 4 to export next set of Instances using another filter. Once you export all the required instances using multiple filters, move the files from backup to the out directory and then execute all the shell scripts generated for Instances. Consolidate the data of exported instances from the Excel sheet backups. 6. How do I delete a compartment from OCI using the toolkit? Terraform destroy on compartments or removing the compartments details from <customer_name>_compartments.auto.tfvars will not delete them from OCI Console by default. Inorder to destroy them from OCI . Either - - Add an additional column - enable_delete to Compartments Tab of CD3 Excel sheet with the value TRUE for the compartments that needs to be deleted on terraform destroy. Execute the toolkit menu option to Create Compartments. (OR) - Add enable_delete = true parameter to each of the compartment that needs to be deleted in <customer_name>_compartments.auto.tfvars 7. I am getting 'Permission Denied' error while executing any commands inside the container. When you are running the docker container from a Linux OS, if the outdir is on the root, you may get a permission denied error while executing steps like createAPIKey.py. In such scenarios, please follow the steps given below - Error Screenshot: Solution: Please change: - selinux mode from Enforcing to Permissive - change the owner of folders in /cd3user/tenancies to that of cd3user. Please refer the screenshots below - - Alternately, please attach a data disk and map the container (/cd3user/tenancies) on a folder that is created on the data disk (your local folder).","title":"FAQs"},{"location":"FAQ/#frequently-asked-questions","text":"1. Is there a way to verify my input CD3 Excel sheet for any typos/miskates? Yes, please choose 'Validate CD3' option in setUpOCI menu in GreenField workflow. It validates specific tabs of the excel sheet. Please see CD3 Validator Features for more details. 2. Can I use an existing outdir to export the data from OCI? Make sure to use a clean outdir without any .tfvars or .tfstate file. Also use a blank CD3 Excel sheet as export process will overwrite the data in the respective tab. 3. If I am already using the toolkit and my OCI tenancy has been subscribed to a new region, how do i use the new region with toolkit? Re-run createTenancyConfig.py with same details in tenancyConfig.properties file. It will keep existing region directories as is and create new directory for newly subscribed region. 4. How do I upgrade an existing version of the toolkit to the new one without disrupting my existing tenancy files/directories? Please look at Steps to Upgrade Your Toolkit. 5. How do I export instances in batches using different filters? Follow below steps: Modify the setUpOCI.properties file to set non_gf_tenancy to \"true\". Choose \"Export Compute\". Specify the filter - prefix of the instances or specific AD to export. Once the execution completes, take a backup of the files generated for instances in out directory( <customer_name>_instances. tfvars and tf_import_cmds _instances_nonGF.sh ) and a backup of the 'Instances' tab of the Input CD3 Excel Sheet. Repeat steps from 1 to 4 to export next set of Instances using another filter. Once you export all the required instances using multiple filters, move the files from backup to the out directory and then execute all the shell scripts generated for Instances. Consolidate the data of exported instances from the Excel sheet backups. 6. How do I delete a compartment from OCI using the toolkit? Terraform destroy on compartments or removing the compartments details from <customer_name>_compartments.auto.tfvars will not delete them from OCI Console by default. Inorder to destroy them from OCI . Either - - Add an additional column - enable_delete to Compartments Tab of CD3 Excel sheet with the value TRUE for the compartments that needs to be deleted on terraform destroy. Execute the toolkit menu option to Create Compartments. (OR) - Add enable_delete = true parameter to each of the compartment that needs to be deleted in <customer_name>_compartments.auto.tfvars 7. I am getting 'Permission Denied' error while executing any commands inside the container. When you are running the docker container from a Linux OS, if the outdir is on the root, you may get a permission denied error while executing steps like createAPIKey.py. In such scenarios, please follow the steps given below - Error Screenshot: Solution: Please change: - selinux mode from Enforcing to Permissive - change the owner of folders in /cd3user/tenancies to that of cd3user. Please refer the screenshots below - - Alternately, please attach a data disk and map the container (/cd3user/tenancies) on a folder that is created on the data disk (your local folder).","title":"Frequently Asked Questions"},{"location":"GF-Jenkins/","text":"Provisioning of Instances/OKE/SDDC/Database on OCI via Jenkins To provision OCI resources which require input ssh keys and source image details, update variables_<region>.tf file using CLI. Step 1 : Update required data in /cd3user/tenancies/<customer_name>/terraform_files/<region>/<service_dir>/variables_<region>.tf Step 2 : Execute GIT commands to sync these local changes with DevOps GIT Repo. Here are the Steps Step 3 : Execute setUpOCI pipeline from Jenkins dashboard with workflow type as Create Resources in OCI(Greenfield Workflow) and choose the respective options to create required services.","title":"Create compute Instances/OKE/SDDC/Database-Jenkins"},{"location":"GF-Jenkins/#provisioning-of-instancesokesddcdatabase-on-oci-via-jenkins","text":"To provision OCI resources which require input ssh keys and source image details, update variables_<region>.tf file using CLI. Step 1 : Update required data in /cd3user/tenancies/<customer_name>/terraform_files/<region>/<service_dir>/variables_<region>.tf Step 2 : Execute GIT commands to sync these local changes with DevOps GIT Repo. Here are the Steps Step 3 : Execute setUpOCI pipeline from Jenkins dashboard with workflow type as Create Resources in OCI(Greenfield Workflow) and choose the respective options to create required services.","title":"Provisioning of Instances/OKE/SDDC/Database on OCI via Jenkins"},{"location":"GreenField-Jenkins/","text":"Create resources in OCI via Jenkins(Greenfield Workflow) Execute setUpOCI Pipeline Step 1 : Choose the appropriate CD3 Excel sheet template from Excel Templates . Fill the CD3 Excel with appropriate values. Step 2 : Login to Jenkins URL with the user created after initialization and click on setUpOCI pipeline from Dashboard. Click on 'Build with Parameters' from left side menu. Note - Only one user at a time using the Jenkins setup is supported in the current release of the toolkit. Step 3 : Upload the above filled Excel sheet in Excel_Template section. This will copy the Excel file at /cd3user/tenancies/<customer_name> inside the container. It will also take backup of existing Excel on the container by appending the current datetime if same filename is uploaded in multiple executions. Step 4: Select the workflow as Create Resources in OCI(Greenfield Workflow) . Choose single or multiple MainOptions as required and then corresponding SubOptions. Please read while selcting multiple options simultaneously. Below screenshot shows creation of Compartments (under Identity) and Tags. Click on Build at the bottom. Step 5: setUpOCI pipeline is triggered and stages are executed as shown below. This will run the python script to generate the terraform auto.tfvars. Once created, it will commit to the OCI Devops GIT Repo and then it will also launch terraform-apply pipelines for the services chosen (Stage:phoenix/identity and Stage:phoenix/tagging in the below screenshot). Execute terraform Pipelines Terraform pipelines are auto triggered parallely from setUpOCI pipeline based on the services selected (the last two stages in above screenshot show trigger of terraform pipelines). Step 1 : Click on 'Logs' for Stage: phoenix/identity and click on the pipeline link. Note - Navigating to Dashboard displays pipelines that are in running state at the bottom left corner. Or you can also navigate from Dashboard using the region based view (Dashboard -> phoenix View -> service specific pipeline) In this example it would be: \u2003 terraform_files \u00bb phoenix \u00bb tagging \u00bb terraform-apply \u2003 terraform_files \u00bb phoenix \u00bb identity \u00bb terraform-apply Step 2 : Stages of the terraform pipeline for apply are shown below: Step 3 : Review Logs for Terraform Plan and OPA stages by clicking on the stage and then 'Logs'. Step 4 : 'Get Approval' stage has timeout of 24 hours, if no action is taken the pipeline will be aborted after 24 hours. Click on this stage and click 'Proceed' to proceed with terraform apply or 'Abort' to cancel the terraform apply. Step 5 : Below screenshot shows Stage View after clicking on 'Proceed'. Login to the OCI console and verify that resources got created as required. Step 6 : Similarly click on 'Logs' for Stage: phoenix/tagging and click on the pipeline link and 'Proceed' or 'Abort' the terraform apply","title":"Create resources in OCI using Jenkins(Greenfield Workflow)"},{"location":"GreenField-Jenkins/#create-resources-in-oci-via-jenkinsgreenfield-workflow","text":"","title":"Create resources in OCI via Jenkins(Greenfield Workflow)"},{"location":"GreenField-Jenkins/#execute-setupoci-pipeline","text":"Step 1 : Choose the appropriate CD3 Excel sheet template from Excel Templates . Fill the CD3 Excel with appropriate values. Step 2 : Login to Jenkins URL with the user created after initialization and click on setUpOCI pipeline from Dashboard. Click on 'Build with Parameters' from left side menu. Note - Only one user at a time using the Jenkins setup is supported in the current release of the toolkit. Step 3 : Upload the above filled Excel sheet in Excel_Template section. This will copy the Excel file at /cd3user/tenancies/<customer_name> inside the container. It will also take backup of existing Excel on the container by appending the current datetime if same filename is uploaded in multiple executions. Step 4: Select the workflow as Create Resources in OCI(Greenfield Workflow) . Choose single or multiple MainOptions as required and then corresponding SubOptions. Please read while selcting multiple options simultaneously. Below screenshot shows creation of Compartments (under Identity) and Tags. Click on Build at the bottom. Step 5: setUpOCI pipeline is triggered and stages are executed as shown below. This will run the python script to generate the terraform auto.tfvars. Once created, it will commit to the OCI Devops GIT Repo and then it will also launch terraform-apply pipelines for the services chosen (Stage:phoenix/identity and Stage:phoenix/tagging in the below screenshot).","title":"Execute setUpOCI Pipeline"},{"location":"GreenField-Jenkins/#execute-terraform-pipelines","text":"Terraform pipelines are auto triggered parallely from setUpOCI pipeline based on the services selected (the last two stages in above screenshot show trigger of terraform pipelines). Step 1 : Click on 'Logs' for Stage: phoenix/identity and click on the pipeline link. Note - Navigating to Dashboard displays pipelines that are in running state at the bottom left corner. Or you can also navigate from Dashboard using the region based view (Dashboard -> phoenix View -> service specific pipeline) In this example it would be: \u2003 terraform_files \u00bb phoenix \u00bb tagging \u00bb terraform-apply \u2003 terraform_files \u00bb phoenix \u00bb identity \u00bb terraform-apply Step 2 : Stages of the terraform pipeline for apply are shown below: Step 3 : Review Logs for Terraform Plan and OPA stages by clicking on the stage and then 'Logs'. Step 4 : 'Get Approval' stage has timeout of 24 hours, if no action is taken the pipeline will be aborted after 24 hours. Click on this stage and click 'Proceed' to proceed with terraform apply or 'Abort' to cancel the terraform apply. Step 5 : Below screenshot shows Stage View after clicking on 'Proceed'. Login to the OCI console and verify that resources got created as required. Step 6 : Similarly click on 'Logs' for Stage: phoenix/tagging and click on the pipeline link and 'Proceed' or 'Abort' the terraform apply","title":"Execute terraform Pipelines"},{"location":"GreenField/","text":"Create resources in OCI (Greenfield Workflow) Step 1 : Choose the appropriate Excel sheet template from Excel Templates Step 2 : Fill the Excel with appropriate values and put at the appropriate location. Modify/Review /cd3user/tenancies /<customer_name>/<customer_name>_setUpOCI.properties with workflow_type set to create_resources as shown below: #Input variables required to run setUpOCI script #path to output directory where terraform file will be generated. eg /cd3user/tenancies/<customer_name>/terraform_files outdir=/cd3user/tenancies/demotenancy/terraform_files/ #prefix for output terraform files eg <customer_name> like demotenancy prefix=demotenancy # auth mechanism for OCI APIs - api_key,instance_principal,session_token auth_mechanism=api_key #input config file for Python API communication with OCI eg /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_config; config_file=/cd3user/tenancies/demotenancy/.config_files/demotenancy_oci_config # Leave it blank if you want single outdir or specify outdir_structure_file.properties containing directory structure for OCI services. outdir_structure_file=/cd3user/tenancies/demotenancy/demotenancy_outdir_structure_file.properties #path to cd3 excel eg /cd3user/tenancies/<customer_name>/CD3-Customer.xlsx cd3file=/cd3user/tenancies/demotenancy/CD3-Blank-template.xlsx #specify create_resources to create new resources in OCI(greenfield workflow) #specify export_resources to export resources from OCI(non-greenfield workflow) workflow_type=create_resources Step 3 : Execute the SetUpOCI.py script to start creating the terraform configuration files. Command to Execute: cd /cd3user/oci_tools/cd3_automation_toolkit/ python setUpOCI.py <path_to_setupOCI.properties> ie python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties \u2192 Example execution of the wrapper script: Choose the resources by specifying a single option (for choosing one of these resources) or comma-separated values (to choose multiple resources) as shown in the sample screenshot above. Step 4: Change your directory to /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/ and Execute: terraform init - To initialize and prepare your working/out directory so Terraform can run the configuration. terraform plan - To preview any changes before you apply them. Run the plan against OPA policies for compliance against CIS. terraform apply - To make the changes defined by Terraform configuration to create, update, or destroy resources in OCI. Note: Execute \"Fetch Compartments OCIDs to variables file\" from CD3 Services in setUpOCI menu after you create Compartments. This is a required step everytime you create a compartment via toolkit or via the OCI console. Example: Create a Compartment Follow the below steps to quickly provision a compartment on OCI. Use the excel CD3-SingleVCN-template and fill the required Compartment details in the 'Compartments' tab. Make appropriate changes to the template. For Eg: Update the Region value to your tenancy's home region. Once all the required data is filled in the Excel sheet, place it at the location /cd3user/tenancies/<customer_name>/ which is also mapped to your local directory. Edit the setUpOCI.properties at location: /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties with appropriate values. Update the cd3file parameter to specify the CD3 excel sheet path. Set the workflow_type parameter value to create_resources . (for Greenfield Workflow.) Change Directory to 'cd3_automation_toolkit' : cd /cd3user/oci_tools/cd3_automation_toolkit/ and execute the setupOCI.py file: python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose option to create compartments under 'Identity' from the displayed menu. Once the execution is successful, <customer_name>_compartments.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply Choose Fetch Compartments OCIDs to variables file under CD3 Services in setUpOCI menu. Execute the command to fetch the details of the compartments if it already exists/created in OCI. These details will be written to the terraform variables file. Repeat the above process (except Step 5) to create other components in OCI.","title":"Create resources in OCI using CLI (Greenfield Workflow)"},{"location":"GreenField/#create-resources-in-oci-greenfield-workflow","text":"Step 1 : Choose the appropriate Excel sheet template from Excel Templates Step 2 : Fill the Excel with appropriate values and put at the appropriate location. Modify/Review /cd3user/tenancies /<customer_name>/<customer_name>_setUpOCI.properties with workflow_type set to create_resources as shown below: #Input variables required to run setUpOCI script #path to output directory where terraform file will be generated. eg /cd3user/tenancies/<customer_name>/terraform_files outdir=/cd3user/tenancies/demotenancy/terraform_files/ #prefix for output terraform files eg <customer_name> like demotenancy prefix=demotenancy # auth mechanism for OCI APIs - api_key,instance_principal,session_token auth_mechanism=api_key #input config file for Python API communication with OCI eg /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_config; config_file=/cd3user/tenancies/demotenancy/.config_files/demotenancy_oci_config # Leave it blank if you want single outdir or specify outdir_structure_file.properties containing directory structure for OCI services. outdir_structure_file=/cd3user/tenancies/demotenancy/demotenancy_outdir_structure_file.properties #path to cd3 excel eg /cd3user/tenancies/<customer_name>/CD3-Customer.xlsx cd3file=/cd3user/tenancies/demotenancy/CD3-Blank-template.xlsx #specify create_resources to create new resources in OCI(greenfield workflow) #specify export_resources to export resources from OCI(non-greenfield workflow) workflow_type=create_resources Step 3 : Execute the SetUpOCI.py script to start creating the terraform configuration files. Command to Execute: cd /cd3user/oci_tools/cd3_automation_toolkit/ python setUpOCI.py <path_to_setupOCI.properties> ie python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties \u2192 Example execution of the wrapper script: Choose the resources by specifying a single option (for choosing one of these resources) or comma-separated values (to choose multiple resources) as shown in the sample screenshot above. Step 4: Change your directory to /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/ and Execute: terraform init - To initialize and prepare your working/out directory so Terraform can run the configuration. terraform plan - To preview any changes before you apply them. Run the plan against OPA policies for compliance against CIS. terraform apply - To make the changes defined by Terraform configuration to create, update, or destroy resources in OCI. Note: Execute \"Fetch Compartments OCIDs to variables file\" from CD3 Services in setUpOCI menu after you create Compartments. This is a required step everytime you create a compartment via toolkit or via the OCI console. Example: Create a Compartment Follow the below steps to quickly provision a compartment on OCI. Use the excel CD3-SingleVCN-template and fill the required Compartment details in the 'Compartments' tab. Make appropriate changes to the template. For Eg: Update the Region value to your tenancy's home region. Once all the required data is filled in the Excel sheet, place it at the location /cd3user/tenancies/<customer_name>/ which is also mapped to your local directory. Edit the setUpOCI.properties at location: /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties with appropriate values. Update the cd3file parameter to specify the CD3 excel sheet path. Set the workflow_type parameter value to create_resources . (for Greenfield Workflow.) Change Directory to 'cd3_automation_toolkit' : cd /cd3user/oci_tools/cd3_automation_toolkit/ and execute the setupOCI.py file: python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose option to create compartments under 'Identity' from the displayed menu. Once the execution is successful, <customer_name>_compartments.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply Choose Fetch Compartments OCIDs to variables file under CD3 Services in setUpOCI menu. Execute the command to fetch the details of the compartments if it already exists/created in OCI. These details will be written to the terraform variables file. Repeat the above process (except Step 5) to create other components in OCI.","title":"Create resources in OCI (Greenfield Workflow)"},{"location":"Intro-Jenkins/","text":"Introduction to Jenkins with the toolkit Jenkins Dashbord setUpOCI Pipeline terraform_files Folder Region based Views (including Global directory) 1. setUpOCI Pipeline This is equivalent to running setUpOCI.py from CLI. This will generate the terraform .auto.tfvars files based on the CD3 Excel sheet input for the services chosen and commit them to OCI Devops GIT repo. This will also trigger terraform-apply pipelines for the corresponding services chosen in setUpOCI pipeline. Below table shows the stages executed in this pipeline along with their description: setUpOCI Pipeline Stages Stage Name Description Possible Outcomes Validate Input Parameters Validates input file name/size, selected parameters Displays Unstable if any of the validation fails. Pipeline stops further execution in that case. Update setUpOCI.properties Updates _setUpOCI.properties with input filename and workflow_type Displays Failed if any issue during execution Execute setUpOCI Executes python code to generate required tfvars files. The console output for this stage is similar to setUpOCI.py execution via CLI. Multiple options selected will be processed sequentially in this stage. Displays Failed if any issue occurs during its execution. Further stages are skipped in that case. Run Import Commands Based on the workflow_type as 'Export Resources from OCI', this stage invokes execution of tf_import_commands_\\<resource>_nonGF.sh shell scripts which will import the exported objects into tfstate. tf_import_commands for multiple options selected will be processed sequentially in this stage. This stage is skipped for 'Create Resources in OCI' workflow Displays Failed if any issue occurs during its execution. Further stages are skipped in that case. Git Commit Commits the terraform_files folder to OCI DevOps GIT Repo. This will trigger respective terraform_pipelines Pipeline stops further execution if there is nothing to commit. In some cases when tfvars was generated in previous execution, you can navigate to terrafom-apply pipeline and trigger that manually Trigger Terraform Pipelines Corresponding terraform apply pipelines are auto triggered based on the service chosen 2. terraform_files Folder This is equivalent to /cd3user/tenancies/<customer_name>/terraform_files folder on your local system. The region directories along with all service directories, are present under this terraform_files folder. The toolkit will generate the .tfvars files for all resources under the service directory. Inside each service directory, pipelines for terraform-apply and terraform-destroy are present. The terraform pipelines are either triggered automatically from setUpOCI pipeline or they can be triggered manually by navigating to any service directory path. terraform-apply Pipeline Stages Stage Name Description Possible Outcomes Checkout SCM Checks out the latest terraform_files folder from DevOps GIT repo Terraform Plan Runs terraform plan against the checked out code and saves it in tfplan.out Pipeline stops further execution if terraform plan shows no changes. Displays Failed if any issue while executing terraform plan OPA Runs the above genrated terraform plan against Open Policies and displays the violations if any Displays Unstable if any OPA rule is violated Get Approval Approval Stage for reviewing the terraform plan. There is 24 hours timeout for this stage. Proceed - goes ahead with Terraform Apply stage. Abort - pipeline is aborted and stops furter execution Terraform Apply Applies the terraform configurations Displays Failed if any issue while executing terraform apply terraform-destroy Pipeline Stages Stage Name Description Possible Outcomes Checkout SCM Checks out the latest terraform_files folder from DevOps GIT repo Terraform Destroy Plan Runs terraform plan -destroy against the checked out code Displays Failed if any issue in plan output Get Approval Approval Stage for reviewing the terraform plan. There is 24 hours timeout for this stage. Proceed - goes ahead with Terraform Destroy stage. Abort - pipeline is aborted and stops furter execution Terraform Destroy Destroys the terraform configurations Displays Failed if any issue while executing terraform destroy 3. Region Based Views When you click on any of the views, it displays all terraform-apply and terraform-destroy pipelines in single screen. This can also be used to trigger the terraform pipelines. This also includes Global view for global services like RPC.","title":"Overview"},{"location":"Intro-Jenkins/#introduction-to-jenkins-with-the-toolkit","text":"","title":"Introduction to Jenkins with the toolkit"},{"location":"Intro-Jenkins/#jenkins-dashbord","text":"setUpOCI Pipeline terraform_files Folder Region based Views (including Global directory)","title":"Jenkins Dashbord"},{"location":"Intro-Jenkins/#1-setupoci-pipeline","text":"This is equivalent to running setUpOCI.py from CLI. This will generate the terraform .auto.tfvars files based on the CD3 Excel sheet input for the services chosen and commit them to OCI Devops GIT repo. This will also trigger terraform-apply pipelines for the corresponding services chosen in setUpOCI pipeline. Below table shows the stages executed in this pipeline along with their description: setUpOCI Pipeline Stages Stage Name Description Possible Outcomes Validate Input Parameters Validates input file name/size, selected parameters Displays Unstable if any of the validation fails. Pipeline stops further execution in that case. Update setUpOCI.properties Updates _setUpOCI.properties with input filename and workflow_type Displays Failed if any issue during execution Execute setUpOCI Executes python code to generate required tfvars files. The console output for this stage is similar to setUpOCI.py execution via CLI. Multiple options selected will be processed sequentially in this stage. Displays Failed if any issue occurs during its execution. Further stages are skipped in that case. Run Import Commands Based on the workflow_type as 'Export Resources from OCI', this stage invokes execution of tf_import_commands_\\<resource>_nonGF.sh shell scripts which will import the exported objects into tfstate. tf_import_commands for multiple options selected will be processed sequentially in this stage. This stage is skipped for 'Create Resources in OCI' workflow Displays Failed if any issue occurs during its execution. Further stages are skipped in that case. Git Commit Commits the terraform_files folder to OCI DevOps GIT Repo. This will trigger respective terraform_pipelines Pipeline stops further execution if there is nothing to commit. In some cases when tfvars was generated in previous execution, you can navigate to terrafom-apply pipeline and trigger that manually Trigger Terraform Pipelines Corresponding terraform apply pipelines are auto triggered based on the service chosen","title":"1. setUpOCI Pipeline"},{"location":"Intro-Jenkins/#2-terraform_files-folder","text":"This is equivalent to /cd3user/tenancies/<customer_name>/terraform_files folder on your local system. The region directories along with all service directories, are present under this terraform_files folder. The toolkit will generate the .tfvars files for all resources under the service directory. Inside each service directory, pipelines for terraform-apply and terraform-destroy are present. The terraform pipelines are either triggered automatically from setUpOCI pipeline or they can be triggered manually by navigating to any service directory path. terraform-apply Pipeline Stages Stage Name Description Possible Outcomes Checkout SCM Checks out the latest terraform_files folder from DevOps GIT repo Terraform Plan Runs terraform plan against the checked out code and saves it in tfplan.out Pipeline stops further execution if terraform plan shows no changes. Displays Failed if any issue while executing terraform plan OPA Runs the above genrated terraform plan against Open Policies and displays the violations if any Displays Unstable if any OPA rule is violated Get Approval Approval Stage for reviewing the terraform plan. There is 24 hours timeout for this stage. Proceed - goes ahead with Terraform Apply stage. Abort - pipeline is aborted and stops furter execution Terraform Apply Applies the terraform configurations Displays Failed if any issue while executing terraform apply terraform-destroy Pipeline Stages Stage Name Description Possible Outcomes Checkout SCM Checks out the latest terraform_files folder from DevOps GIT repo Terraform Destroy Plan Runs terraform plan -destroy against the checked out code Displays Failed if any issue in plan output Get Approval Approval Stage for reviewing the terraform plan. There is 24 hours timeout for this stage. Proceed - goes ahead with Terraform Destroy stage. Abort - pipeline is aborted and stops furter execution Terraform Destroy Destroys the terraform configurations Displays Failed if any issue while executing terraform destroy","title":"2. terraform_files Folder"},{"location":"Intro-Jenkins/#3-region-based-views","text":"When you click on any of the views, it displays all terraform-apply and terraform-destroy pipelines in single screen. This can also be used to trigger the terraform pipelines. This also includes Global view for global services like RPC.","title":"3. Region Based Views"},{"location":"Jobs_Migration/","text":"Migrate Jobs from Automation Toolkit Jenkins to Customer Jenkins Environment Copy Jobs Folder Copy the folders from the Automation Toolkit Jenkins home path /cd3user/tenancies/jenkins_home/jobs/ to the corresponding home directory in the Customer Jenkins instance (typically /var/jenkins_home ). Set up OCI Devops repository SSH Authentication Ensure SSH authentication is configured and operational on the Customer Jenkins instance. For detailed instructions, refer to the OCI Code Repository documentation . Note - Steps to change the GIT repo are explained in next section. Ensure Availability of Ansi Color Plugin Confirm the presence of the Ansi color plugin in the Customer Jenkins instance. This plugin is utilized in Automation Toolkit pipeline Groovy code and is necessary if not already installed. Plugin link: Ansicolor Plugin Install Terraform Binary Make sure the Terraform binary is installed and accessible for the Jenkins user within the Jenkins instance. Installation guide: Terraform Installation Update Optional Attribute Field inside Terraform Provider Block at /cd3user/tenancies/<customer_name>/terraform_files/<region><service_dir>/provider.tf Include an attribute as highlighted below within the Terraform provider block. This is optional but necessary in case Terraform plan encounters an error. experiments = [module_variable_optional_attrs] Update the correct value for private_key_path variable in /cd3user/tenancies/<customer_name>/terraform_files/<region><service_dir>/variables_<region>.tf Configure S3 Backend Credentials in Customer Jenkins Instance Update the correct path within the backend.tf file for Terraform. Push the above changes to Devops GIT repository so that pipline can get the latest commits/changes and execute it. Stop/Start the Customer Jenkins Instance for the changes to take effect. This is applicable for any configuration changes in Jenkins. Job and Pipeline Configuration Verify that the specified jobs and pipelines, initialized by the Automation Toolkit, are visible in the Customer Jenkins instance. Pipeline Job Output Update the Git URL for all pipeline jobs in the Customer Jenkins(if required). Remove terraform_files folder under /jobs folder Create jenkins.properties File Copy the jenkins.properties file from Automation Toolkit Jenkins home folder /cd3users/tenancies/jenkins_home/ to the customer jenkins home (typically /var/jenkins_home/ ) directory in customer Jenkins Instance (Below is sample content): git_url= \"ssh://devops.scmservice.us-phoenix-1.oci.oraclecloud.com/namespaces/ /projects/toolkitdemo-automation-toolkit-project/repositories/toolkitdemo-automation-toolkit-repo\" regions=['london', 'phoenix'] services=['identity', 'tagging', 'network', 'loadbalancer', 'vlan', 'nsg', 'compute', 'database', 'fss', 'oke', 'ocvs', 'security', 'managementservices', 'budget', 'cis', 'oss', 'dns'] outdir_structure=[\"Multiple_Outdir\"] Update the git_url in the jenkins.properties File Open the jenkins.properties file located in the /var/jenkins_home/ directory. Update the git_url in the file with the new Git server URL. Copy 01_jenkins-config.groovy File Copy the 01_jenkins-config.groovy file from the Automation Toolkit Jenkins path ( /cd3user/tenancies/jenkins_home/init.groovy.d ) to the init path of the Customer Jenkins instance. Update the path to the groovy file accordingly. Restart Customer Jenkins Instance Stop and start the Customer Jenkins instance to apply the changes. After that, all Git URLs will be updated and point to new Git Url inside pipeline jobs. Ensure SSH Authentication Confirm that SSH authentication is enabled for the new GIT repository from the Jenkins instance. Alternatively, use the respective authentication method if relying on other methods.","title":"Migrate jobs to User's Jenkins environment"},{"location":"Jobs_Migration/#migrate-jobs-from-automation-toolkit-jenkins-to-customer-jenkins-environment","text":"Copy Jobs Folder Copy the folders from the Automation Toolkit Jenkins home path /cd3user/tenancies/jenkins_home/jobs/ to the corresponding home directory in the Customer Jenkins instance (typically /var/jenkins_home ). Set up OCI Devops repository SSH Authentication Ensure SSH authentication is configured and operational on the Customer Jenkins instance. For detailed instructions, refer to the OCI Code Repository documentation . Note - Steps to change the GIT repo are explained in next section. Ensure Availability of Ansi Color Plugin Confirm the presence of the Ansi color plugin in the Customer Jenkins instance. This plugin is utilized in Automation Toolkit pipeline Groovy code and is necessary if not already installed. Plugin link: Ansicolor Plugin Install Terraform Binary Make sure the Terraform binary is installed and accessible for the Jenkins user within the Jenkins instance. Installation guide: Terraform Installation Update Optional Attribute Field inside Terraform Provider Block at /cd3user/tenancies/<customer_name>/terraform_files/<region><service_dir>/provider.tf Include an attribute as highlighted below within the Terraform provider block. This is optional but necessary in case Terraform plan encounters an error. experiments = [module_variable_optional_attrs] Update the correct value for private_key_path variable in /cd3user/tenancies/<customer_name>/terraform_files/<region><service_dir>/variables_<region>.tf Configure S3 Backend Credentials in Customer Jenkins Instance Update the correct path within the backend.tf file for Terraform. Push the above changes to Devops GIT repository so that pipline can get the latest commits/changes and execute it. Stop/Start the Customer Jenkins Instance for the changes to take effect. This is applicable for any configuration changes in Jenkins. Job and Pipeline Configuration Verify that the specified jobs and pipelines, initialized by the Automation Toolkit, are visible in the Customer Jenkins instance. Pipeline Job Output","title":"Migrate Jobs from Automation Toolkit Jenkins to Customer Jenkins Environment"},{"location":"Jobs_Migration/#update-the-git-url-for-all-pipeline-jobs-in-the-customer-jenkinsif-required","text":"Remove terraform_files folder under /jobs folder Create jenkins.properties File Copy the jenkins.properties file from Automation Toolkit Jenkins home folder /cd3users/tenancies/jenkins_home/ to the customer jenkins home (typically /var/jenkins_home/ ) directory in customer Jenkins Instance (Below is sample content): git_url= \"ssh://devops.scmservice.us-phoenix-1.oci.oraclecloud.com/namespaces/ /projects/toolkitdemo-automation-toolkit-project/repositories/toolkitdemo-automation-toolkit-repo\" regions=['london', 'phoenix'] services=['identity', 'tagging', 'network', 'loadbalancer', 'vlan', 'nsg', 'compute', 'database', 'fss', 'oke', 'ocvs', 'security', 'managementservices', 'budget', 'cis', 'oss', 'dns'] outdir_structure=[\"Multiple_Outdir\"] Update the git_url in the jenkins.properties File Open the jenkins.properties file located in the /var/jenkins_home/ directory. Update the git_url in the file with the new Git server URL. Copy 01_jenkins-config.groovy File Copy the 01_jenkins-config.groovy file from the Automation Toolkit Jenkins path ( /cd3user/tenancies/jenkins_home/init.groovy.d ) to the init path of the Customer Jenkins instance. Update the path to the groovy file accordingly. Restart Customer Jenkins Instance Stop and start the Customer Jenkins instance to apply the changes. After that, all Git URLs will be updated and point to new Git Url inside pipeline jobs. Ensure SSH Authentication Confirm that SSH authentication is enabled for the new GIT repository from the Jenkins instance. Alternatively, use the respective authentication method if relying on other methods.","title":"Update the Git URL for all pipeline jobs in the Customer Jenkins(if required)."},{"location":"KnownBehaviour/","text":"Expected Behaviour Of Automation Toolkit NOTE Automation Tool Kit DOES NOT support the creation/export of duplicate resources. Automation Tool Kit DOES NOT support sharing of Block Volumes. WARNING!! DO NOT modify/remove any commented rows or column names. You may re-arrange the columns if needed. A double colon (::) or Semi-Colon (;) has a special meaning in the Tool Kit. Do not use them in the OCI data / values. Do not include any unwanted space in cells you fill in; do not place any empty rows in between. Any entry made/moved post \\ in any of the tabs of CD3 will not be processed. Any resources created by the automation & then moved after the \\ will cause the resources to be removed. IMPORTANT!! The components that get created as part of VCNs Tab (Example: IGW, SGW, LPG, NGW, DRG) will have the same set of Tags attached to them. Some points to consider while modifying networking components are: Converting the exported VCN to Hub/Spoke/Peer VCN is not allowed. Route Table rules based on the peering for new LPGs to existing VCNs will not be auto populated. Users are requested to add an entry to the RouteRulesInOCI sheet to support the peering rules. Adding a new VCN as Hub and other new VCNs as Spoke/Peer is allowed. Gateways will be created as specified in VCNs sheet. Adding new VCNs as None is allowed. Gateways will be created as specified in VCNs sheet. The addition of new Subnets to exported VCNs and new VCNs is allowed. You might come across below error during export of NSGs(while runnig terraform import commands for NSGs). It occurs when NSG and the VCN are in different compartments. In such cases, please modify \\<customer_name>_nsgs.auto.tfvars, specify the compartment name of the VCN in network_compartment_id field of the problematic NSG. 6. When you have exported Identity and Network services together in single outdirectory for the first time and executing identity import script. You might see import failure with below error message. Execute Major network import script first then run Identity import script. ``` !!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!! 4 problems: - Failed to serialize resource instance in state: Instance data.oci_core_drg_route_distributions.drg_route_distributions[\"DRG-ASH_Autogenerated-Import-Route-Distribution-for-ALL-routes\"] has status ObjectPlanned, which cannot be saved in state. - Failed to serialize resource instance in state: Instance data.oci_core_drg_route_distributions.drg_route_distributions[\"DRG-ASH_Autogenerated-Import-Route-Distribution-for-VCN-Routes\"] has status ObjectPlanned, which cannot be saved in state. ``` Terraform Behavior 1. Create a Load Balancer with Reserved IP: When you create a LBaaS with reserved ip as \"Y\" and do a terraform apply, everything will go smooth and be in sync for the first time. If you do a terraform plan immediately (post apply), you will find that the plan changes the private ip of load balancer to null. This is a behaviour of Terraform. In these scenarios, please add the private ip ocid to the auto.tfvars as shown below before you run a terraform plan again. Once you do the above change, and then execute a terraform plan/apply, you will get the below error and it can be ignored. 2. While exporting and synching the tfstate file for LBaaS Objects, the user may be notified that a few components will be modified on apply. In such scenarios, add the attributes that the Terraform notifies to be changed to the appropriate CD3 Tab of Load Balancer and uncomment the parameter from Jinja2 Templates and Terraform (.tf) files. Re-run the export. 3. Add a new column - \"Freeform Tags\" to the CD3 Excel Sheets as per necessity, to export the tags associated with the resource as well. If executed as-is, Terraform may prompt you to modify resources based on Tags. Example: 4. Toolkit will create TF for only those DRGs which are part of CD3 and skip Route Tables for the DRGs created outside of CD3. This will also synch DRG rules in your tenancy with the terraform state. Note When there are changes made in the OCI console manually, the above options of export and modify can be helpful to sync up the contents/objects in OCI to TF. 5. Match All criteria specified for Route Distribution Statement In DRGs sheet will show below output each time you do terraform plan: The service api is designed in such a way that it expects an empty list for match all. And it sends back an empty list in the response every time. Hence this behaviour from terraform side. This can be safely ignored. 6. Export process for non greenfield tenancies v6.0 or higher will try to revert SGW for a VCN to point to all services if it was existing for just object storage. You will get output similiar to below when terraform plan is run (Option 3 with workflow_type set to export_resources). # oci_core_service_gateway.VCN_sgw will be updated in-place ~ resource \"oci_core_service_gateway\" \"VCN_sgw\" { block_traffic = false compartment_id = \"ocid1.compartment.oc1..aaaaaaaahsesjfw5hhftccsvndbufdlf5ca2c3q3clyvwg4wngj4ej26i3ya\" display_name = \"VCN_sgw\" freeform_tags = {} id = \"ocid1.servicegateway.oc1.iad.aaaaaaaajqtpjqy7ihgikmug5kbz55pztymt7m6t4ijlqek5ujqg3qxeaxma\" state = \"AVAILABLE\" time_created = \"2019-03-19 16:46:33.859 +0000 UTC\" vcn_id = \"ocid1.vcn.oc1.iad.aaaaaaaazjup6ahpesjgrjyaxr2bcnx44tpn3ygvx2tjylytgkub5ikl6rha\" - services { - service_id = \"ocid1.service.oc1.iad.aaaaaaaa74z6sqsezqf6znyomdp5jkvfwb4j2ol33abgosvnhxcqphyl3eaq\" -> null - service_name = \"OCI IAD Object Storage\" -> null } + services { + service_id = \"ocid1.service.oc1.iad.aaaaaaaam4zfmy2rjue6fmglumm3czgisxzrnvrwqeodtztg7hwa272mlfna\" + service_name = (known after apply) } timeouts {} } 7. If the description field is having any newlines in the tenancy then the export of the component and tf synch will show output similar to below: # module.iam-policies[\u201cConnectorPolicy_notifications_2023-03-06T21-54-41-655Z\u201d].oci_identity_policy.policy will be updated in-place ~ resource \u201coci_identity_policy\u201d \u201cpolicy\u201d { ~ description = <<-**EOT** This policy is created for the \u2018OCI_To_Sentinel\u2019 service connector Date: Mon, 06 Mar 2023 21:54:41 GMT User: oracleidentitycloudservice/abc@oracle.com Tenant: test Connection Source: notifications **EOT** id = \u201cocid1.policy.oc1..aaaaaaaa5gct2n6vz4arggmeow27rivu5vro6jjb6ccuq5u2phulghgwx\u201d name = \u201cConnectorPolicy_notifications_2023-03-06T21.54.41.655Z\u201d # (9 unchanged attributes hidden) # (1 unchanged block hidden) } Plan: 0 to add, 1 to change, 0 to destroy. This is how terraform handles newlines in the fields. Pleage ignore this and proceed with terraform apply. 8. Terraform ordering changes observed during plan phase for OCI compute plugins. It changes the order of plugin's in terraform state file and doesn't change anything in OCI console for compute resource.","title":"Expected Behaviour of CD3"},{"location":"KnownBehaviour/#expected-behaviour-of-automation-toolkit","text":"NOTE Automation Tool Kit DOES NOT support the creation/export of duplicate resources. Automation Tool Kit DOES NOT support sharing of Block Volumes. WARNING!! DO NOT modify/remove any commented rows or column names. You may re-arrange the columns if needed. A double colon (::) or Semi-Colon (;) has a special meaning in the Tool Kit. Do not use them in the OCI data / values. Do not include any unwanted space in cells you fill in; do not place any empty rows in between. Any entry made/moved post \\ in any of the tabs of CD3 will not be processed. Any resources created by the automation & then moved after the \\ will cause the resources to be removed. IMPORTANT!! The components that get created as part of VCNs Tab (Example: IGW, SGW, LPG, NGW, DRG) will have the same set of Tags attached to them. Some points to consider while modifying networking components are: Converting the exported VCN to Hub/Spoke/Peer VCN is not allowed. Route Table rules based on the peering for new LPGs to existing VCNs will not be auto populated. Users are requested to add an entry to the RouteRulesInOCI sheet to support the peering rules. Adding a new VCN as Hub and other new VCNs as Spoke/Peer is allowed. Gateways will be created as specified in VCNs sheet. Adding new VCNs as None is allowed. Gateways will be created as specified in VCNs sheet. The addition of new Subnets to exported VCNs and new VCNs is allowed. You might come across below error during export of NSGs(while runnig terraform import commands for NSGs). It occurs when NSG and the VCN are in different compartments. In such cases, please modify \\<customer_name>_nsgs.auto.tfvars, specify the compartment name of the VCN in network_compartment_id field of the problematic NSG. 6. When you have exported Identity and Network services together in single outdirectory for the first time and executing identity import script. You might see import failure with below error message. Execute Major network import script first then run Identity import script. ``` !!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!! 4 problems: - Failed to serialize resource instance in state: Instance data.oci_core_drg_route_distributions.drg_route_distributions[\"DRG-ASH_Autogenerated-Import-Route-Distribution-for-ALL-routes\"] has status ObjectPlanned, which cannot be saved in state. - Failed to serialize resource instance in state: Instance data.oci_core_drg_route_distributions.drg_route_distributions[\"DRG-ASH_Autogenerated-Import-Route-Distribution-for-VCN-Routes\"] has status ObjectPlanned, which cannot be saved in state. ```","title":"Expected Behaviour Of Automation Toolkit"},{"location":"KnownBehaviour/#terraform-behavior","text":"","title":"Terraform Behavior"},{"location":"KnownBehaviour/#1","text":"Create a Load Balancer with Reserved IP: When you create a LBaaS with reserved ip as \"Y\" and do a terraform apply, everything will go smooth and be in sync for the first time. If you do a terraform plan immediately (post apply), you will find that the plan changes the private ip of load balancer to null. This is a behaviour of Terraform. In these scenarios, please add the private ip ocid to the auto.tfvars as shown below before you run a terraform plan again. Once you do the above change, and then execute a terraform plan/apply, you will get the below error and it can be ignored.","title":"1."},{"location":"KnownBehaviour/#2","text":"While exporting and synching the tfstate file for LBaaS Objects, the user may be notified that a few components will be modified on apply. In such scenarios, add the attributes that the Terraform notifies to be changed to the appropriate CD3 Tab of Load Balancer and uncomment the parameter from Jinja2 Templates and Terraform (.tf) files. Re-run the export.","title":"2."},{"location":"KnownBehaviour/#3","text":"Add a new column - \"Freeform Tags\" to the CD3 Excel Sheets as per necessity, to export the tags associated with the resource as well. If executed as-is, Terraform may prompt you to modify resources based on Tags. Example:","title":"3."},{"location":"KnownBehaviour/#4","text":"Toolkit will create TF for only those DRGs which are part of CD3 and skip Route Tables for the DRGs created outside of CD3. This will also synch DRG rules in your tenancy with the terraform state. Note When there are changes made in the OCI console manually, the above options of export and modify can be helpful to sync up the contents/objects in OCI to TF.","title":"4."},{"location":"KnownBehaviour/#5","text":"Match All criteria specified for Route Distribution Statement In DRGs sheet will show below output each time you do terraform plan: The service api is designed in such a way that it expects an empty list for match all. And it sends back an empty list in the response every time. Hence this behaviour from terraform side. This can be safely ignored.","title":"5."},{"location":"KnownBehaviour/#6","text":"Export process for non greenfield tenancies v6.0 or higher will try to revert SGW for a VCN to point to all services if it was existing for just object storage. You will get output similiar to below when terraform plan is run (Option 3 with workflow_type set to export_resources). # oci_core_service_gateway.VCN_sgw will be updated in-place ~ resource \"oci_core_service_gateway\" \"VCN_sgw\" { block_traffic = false compartment_id = \"ocid1.compartment.oc1..aaaaaaaahsesjfw5hhftccsvndbufdlf5ca2c3q3clyvwg4wngj4ej26i3ya\" display_name = \"VCN_sgw\" freeform_tags = {} id = \"ocid1.servicegateway.oc1.iad.aaaaaaaajqtpjqy7ihgikmug5kbz55pztymt7m6t4ijlqek5ujqg3qxeaxma\" state = \"AVAILABLE\" time_created = \"2019-03-19 16:46:33.859 +0000 UTC\" vcn_id = \"ocid1.vcn.oc1.iad.aaaaaaaazjup6ahpesjgrjyaxr2bcnx44tpn3ygvx2tjylytgkub5ikl6rha\" - services { - service_id = \"ocid1.service.oc1.iad.aaaaaaaa74z6sqsezqf6znyomdp5jkvfwb4j2ol33abgosvnhxcqphyl3eaq\" -> null - service_name = \"OCI IAD Object Storage\" -> null } + services { + service_id = \"ocid1.service.oc1.iad.aaaaaaaam4zfmy2rjue6fmglumm3czgisxzrnvrwqeodtztg7hwa272mlfna\" + service_name = (known after apply) } timeouts {} }","title":"6."},{"location":"KnownBehaviour/#7","text":"If the description field is having any newlines in the tenancy then the export of the component and tf synch will show output similar to below: # module.iam-policies[\u201cConnectorPolicy_notifications_2023-03-06T21-54-41-655Z\u201d].oci_identity_policy.policy will be updated in-place ~ resource \u201coci_identity_policy\u201d \u201cpolicy\u201d { ~ description = <<-**EOT** This policy is created for the \u2018OCI_To_Sentinel\u2019 service connector Date: Mon, 06 Mar 2023 21:54:41 GMT User: oracleidentitycloudservice/abc@oracle.com Tenant: test Connection Source: notifications **EOT** id = \u201cocid1.policy.oc1..aaaaaaaa5gct2n6vz4arggmeow27rivu5vro6jjb6ccuq5u2phulghgwx\u201d name = \u201cConnectorPolicy_notifications_2023-03-06T21.54.41.655Z\u201d # (9 unchanged attributes hidden) # (1 unchanged block hidden) } Plan: 0 to add, 1 to change, 0 to destroy. This is how terraform handles newlines in the fields. Pleage ignore this and proceed with terraform apply.","title":"7."},{"location":"KnownBehaviour/#8","text":"Terraform ordering changes observed during plan phase for OCI compute plugins. It changes the order of plugin's in terraform state file and doesn't change anything in OCI console for compute resource.","title":"8."},{"location":"Launch_Docker_container/","text":"Launch the Container To ease the execution of toolkit, we have provided the steps to build an image which encloses the code base and its package dependencies. Follow the steps provided below to clone the repo, build the image and finally launch the container. Clone the repo Open your terminal and navigate to the directory where you plan to download the Git repo. Run the git clone command as shown below: git clone https://github.com/oracle-devrel/cd3-automation-toolkit Once the cloning command is executed successfully, the repo will replicate to the local directory. Build an image Change directory to 'cd3-automation-toolkit'(i.e. the cloned repo in your local). Run docker build --platform linux/amd64 -t cd3toolkit:${image_tag} -f Dockerfile --pull --no-cache . Note : ${image_tag} should be replaced with suitable tag as per your requirements/standards. eg v2024.1.0 The period (.) at the end of the docker build command is required. Save the image (Optional) Run docker save cd3toolkit:${image_tag} | gzip > cd3toolkit_${image_tag}.tar.gz Run the container alongwith VPN (Applicable for VPN users only) Connect to the VPN. Make sure you are using version 1.9 for Rancher deskop , if not please install the latest. Make sure to Enable Networking Tunnel under Rancher settings as shown in the screenshot below, Login to the CD3 docker container using next section and set the proxies(if any) which helps to connect internet from the container. Run the container Run docker run --platform linux/amd64 -it -p <port_number_in_local_system>:8443 -d -v <directory_in_local_system_where_the_files_must_be_generated>:/cd3user/tenancies <image_name>:<image_tag> Eg for Mac: docker run --platform linux/amd64 -it -p 8443:8443 -d -v /Users/<user_name>/mount_path:/cd3user/tenancies cd3toolkit:v2024.1.0 Eg for Windows: docker run --platform linux/amd64 -it -p 8443:8443 -d -v D:/mount_path:/cd3user/tenancies cd3toolkit:v2024.1.0 If you are launching the container on cloud, Please make sure to use a private server or a bastion connected server with restricted access(i.e. not publicly available) to host the container. Run docker ps","title":"Launch the container"},{"location":"Launch_Docker_container/#launch-the-container","text":"To ease the execution of toolkit, we have provided the steps to build an image which encloses the code base and its package dependencies. Follow the steps provided below to clone the repo, build the image and finally launch the container.","title":"Launch the Container"},{"location":"Launch_Docker_container/#clone-the-repo","text":"Open your terminal and navigate to the directory where you plan to download the Git repo. Run the git clone command as shown below: git clone https://github.com/oracle-devrel/cd3-automation-toolkit Once the cloning command is executed successfully, the repo will replicate to the local directory.","title":"Clone the repo"},{"location":"Launch_Docker_container/#build-an-image","text":"Change directory to 'cd3-automation-toolkit'(i.e. the cloned repo in your local). Run docker build --platform linux/amd64 -t cd3toolkit:${image_tag} -f Dockerfile --pull --no-cache . Note : ${image_tag} should be replaced with suitable tag as per your requirements/standards. eg v2024.1.0 The period (.) at the end of the docker build command is required.","title":"Build an image"},{"location":"Launch_Docker_container/#save-the-image-optional","text":"Run docker save cd3toolkit:${image_tag} | gzip > cd3toolkit_${image_tag}.tar.gz","title":"Save the image (Optional)"},{"location":"Launch_Docker_container/#run-the-container-alongwith-vpn-applicable-for-vpn-users-only","text":"Connect to the VPN. Make sure you are using version 1.9 for Rancher deskop , if not please install the latest. Make sure to Enable Networking Tunnel under Rancher settings as shown in the screenshot below, Login to the CD3 docker container using next section and set the proxies(if any) which helps to connect internet from the container.","title":"Run the container alongwith VPN (Applicable for VPN users only)"},{"location":"Launch_Docker_container/#run-the-container","text":"Run docker run --platform linux/amd64 -it -p <port_number_in_local_system>:8443 -d -v <directory_in_local_system_where_the_files_must_be_generated>:/cd3user/tenancies <image_name>:<image_tag> Eg for Mac: docker run --platform linux/amd64 -it -p 8443:8443 -d -v /Users/<user_name>/mount_path:/cd3user/tenancies cd3toolkit:v2024.1.0 Eg for Windows: docker run --platform linux/amd64 -it -p 8443:8443 -d -v D:/mount_path:/cd3user/tenancies cd3toolkit:v2024.1.0 If you are launching the container on cloud, Please make sure to use a private server or a bastion connected server with restricted access(i.e. not publicly available) to host the container. Run docker ps","title":"Run the container"},{"location":"LearningVideos/","text":"Automation Toolkit Learning Videos CD3 Automation Toolkit Explained in 1 minute Part 1- Introduction to CD3 Automation Toolkit Part 2 - How to Install and Configure Rancher Desktop with CD3 Part 3 - How to Create the Image and Run the Container with CD3 Part 4 - Connect CD3 Automation toolkit container to OCI tenancy Part 5 - How to use CD3 Excel Sheets Part 6 - Execute the CD3 Automation toolkit using CLI to create resources in OCI Part 7 - Execute CD3 Automation Toolkit using CLI to export resources in OCI Part 8 - Execute the CD3 Automation toolkit using Jenkins to create resources in OCI Part 9 - Execute the CD3 Automation toolkit using Jenkins to export resources from OCI Part 10 - Support for Additional Attributes - CD3 Automation toolkit Configuring OCI Private DNS using CD3 Provisioning OCI Object storage Buckets using CD3 Here is the Youtube playlist link for all these videos.","title":"Automation Toolkit Learning Videos"},{"location":"LearningVideos/#automation-toolkit-learning-videos","text":"CD3 Automation Toolkit Explained in 1 minute Part 1- Introduction to CD3 Automation Toolkit Part 2 - How to Install and Configure Rancher Desktop with CD3 Part 3 - How to Create the Image and Run the Container with CD3 Part 4 - Connect CD3 Automation toolkit container to OCI tenancy Part 5 - How to use CD3 Excel Sheets Part 6 - Execute the CD3 Automation toolkit using CLI to create resources in OCI Part 7 - Execute CD3 Automation Toolkit using CLI to export resources in OCI Part 8 - Execute the CD3 Automation toolkit using Jenkins to create resources in OCI Part 9 - Execute the CD3 Automation toolkit using Jenkins to export resources from OCI Part 10 - Support for Additional Attributes - CD3 Automation toolkit Configuring OCI Private DNS using CD3 Provisioning OCI Object storage Buckets using CD3 Here is the Youtube playlist link for all these videos.","title":"Automation Toolkit Learning Videos"},{"location":"NetworkingScenariosGF-Jenkins/","text":"Executing Networking Scenarios using toolkit via Jenkins Managing Network for Greenfield Workflow Create Network Modify Network Modify Security Rules, Route Rules and DRG Route Rules Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Add/Modify/Delete NSGs Add/Modify/Delete VLANs Remote Peering Connections NOTE- Create Network Creation of Networking components using Automation Toolkit involves four simple steps. - Add the networking resource details to appropriate Excel Sheets. - Running the setUpOCI pipeline in the toolkit to generate auto.tfvars. - Executing terraform pipeline to provision the resources in OCI. - Exporting the automatically generated Security Rules and Route Rules by the toolkit to CD3 Excel Sheet. Below are the steps to create Network that includes VCNs, Subnets, DHCP, DRG, Security List, Route Tables, DRG Route Tables, NSGs, etc. Choose appropriate excel sheet from Excel Templates and fill the required Network details in the Networking Tabs - VCNs, DRGs, VCN Info, DHCP, Subnets, NSGs tabs. Execute the setupOCI pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose option 'Validate CD3' and then 'Validate Networks' to check for syntax errors in Excel sheet. Examine the log file generated at /cd3user/tenancies/<customer_name>/<customer_name>_cd3validator.log . If there are errors, please rectify them accordingly and proceed to the next step. Choose 'Create Network' under 'Network' from the displayed options. Click on Build. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the creation of Networking components in OCI. Verify the components in console. However the details of the default security lists and default route tables are not available in the CD3 Excel sheet yet. Inorder to export that data please follow the below steps: Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed options. Choose below sub-options: (Make sure to choose all the three optionsfor the first time) Security Rules Export Security Rules (From OCI into SecRulesinOCI sheet) Route Rules Export Route Rules (From OCI into RouteRulesinOCI sheet) DRG Route Rules Export DRG Route Rules (From OCI into DRGRouteRulesinOCI sheet) Click on Build. This completes the steps for Creating the Network in OCI and exporting the default rules to the CD3 Excel Sheet using the Automation Toolkit. Go back to Networking Scenarios Modify Network Modifying the Networking components using Automation Toolkit involves three simple steps. Add/modify the details of networking components like the VCNs, Subnets, DHCP and DRG in Excel Sheet. Running the the setUpOCI pipeline in the toolkit to generate auto.tfvars. Executing Terraform pipeline to provision/modify the resources in OCI. Note : Follow these steps to modify Security Rules, Route Rules and DRG Route Rules Steps in detail : Modify your excel sheet to update required data in the Tabs - VCNs, DRGs, VCN Info, DHCP and Subnets. Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) To Validate the CD3 excel Tabs - Choose option 'Validate CD3' and 'Validate Networks' from sub-menu to check for syntax errors in Excel sheet. Examine the log file generated at /cd3user/tenancies/<customer_name>/<customer_name>_cd3validator.logs . If there are errors, please rectify them accordingly and proceed to the next step. Choose option to 'Modify Network' under 'Network' from the displayed options. Once the execution is successful, multiple .tfvars related to networking like <customer_name>_major-objects.auto.tfvars and more will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> . Existing files will move into respective backup folders. Note- : Make sure to export Sec Rules, Route Rules, DRG Route Rules to CD3 Excel Sheet before executing this option. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the modification of Networking components in OCI. Verify the components in console. Go back to Networking Scenarios Modify Security Rules, Route Rules and DRG Route Rules Follow the below steps to add, update or delete the following components: - Security Lists and Security Rules - Route Table and Route Rules - DRG Route Table and DRG Route Rules Modify your excel sheet to update required data in the Tabs - RouteRulesInOCI, SecRulesInOCI, DRGRouteRulesInOCI tabs. Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed options. Choose below sub-options: Security Rules: Add/Modify/Delete Security Rules (Reads SecRulesinOCI sheet) Route Rules: Add/Modify/Delete Route Rules (Reads RouteRulesinOCI sheet) DRG Route Rules: Add/Modify/Delete DRG Route Rules (Reads DRGRouteRulesinOCI sheet) Once the execution is successful, _seclists.auto.tfvars , _routetables.auto.tfvars and _drg-routetables.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir> . Existing files will move into respective backup folders. NOTE : This will create TF for only those Security Lists and Route Tables in VCNs which are part of cd3 and skip any VCNs that have been created outside of cd3 execution. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the modification of Security Rules, Route Rules and DRG Route Rules in OCI. Verify the components in console. Go back to Networking Scenarios Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Follow the below process to export the rules to the same CD3 Excel Sheet as the one used to Create Network, and to sync the Terraform files with OCI whenever an user adds, modifies or deletes rules in OCI Console manually. NOTE : Make sure to close your Excel sheet during the export process. Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed menu. Choose below sub-options: Security Rules Export Security Rules (From OCI into SecRulesinOCI sheet) Add/Modify/Delete Security Rules (Reads SecRulesinOCI sheet) Route Rules Export Route Rules (From OCI into RouteRulesinOCI sheet) Add/Modify/Delete Route Rules (Reads RouteRulesinOCI sheet) DRG Route Rules Export DRG Route Rules (From OCI into DRGRouteRulesinOCI sheet) Add/Modify/Delete DRG Route Rules (Reads DRGRouteRulesinOCI sheet) Once the execution is successful, 'RouteRulesInOCI', 'SecRulesInOCI', 'DRGRouteRulesInOCI' tabs of the excel sheet will be updated with the rules exported from OCI. And <customer_name>_seclists.auto.tfvars , <customer_name>_routetables.auto.tfvars and <customer_name>_drg-routetables.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir> It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the export of Security Rules, Route Rules and DRG Route Rules from OCI. Terraform plan/apply should be in sync with OCI. Go back to Networking Scenarios Add/Modify/Delete NSGs Follow the below steps to update NSGs. Modify your excel sheet to update required data in the Tabs - NSGs. Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed menu. Choose below sub-option: Network Security Groups Add/Modify/Delete NSGs (Reads NSGs sheet) Once the execution is successful, <customer_name>_nsgs.auto.tfvars will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> . Existing files will move into respective backup folders. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'nsg'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the modification of NSGs in OCI. Verify the components in console. Go back to Networking Scenarios Add/Modify/Delete VLANs Follow the below steps to update VLANs. Modify your excel sheet to update required data in the Tabs - SubnetsVLANs. Make sure that the RouteRulesinOCI sheet and corresponing terraform is in synch with route rules in OCI console. If not, please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed menu. Choose below sub-option: Add/Modify/Delete VLANs (Reads SubnetsVLANs sheet) Once the execution is successful, <customer_name>_vlans.auto.tfvars will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> . Existing files will move into respective backup folders. <customer_name>\\routetables.auto.tfvars file will also be updated with the route table information specified for each VLAN. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'vlan' and 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. Again make sure to export the Route Rules in OCI into excel and terraform. Please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform This completes the modification of VLANs in OCI. Verify the components in console. RPCs Remote VCN peering is the process of connecting two VCNs in different regions (but the same tenancy). The peering allows the VCNs' resources to communicate using private IP addresses without routing the traffic over the internet or through your on-premises network. Modify your excel sheet to update required data in the Tabs - DRGs. The source and target RPC details to be entered in DRG sheet for establishing a connection. Please check the example in excel file for reference. Make sure that the DRGRouteRulesinOCI sheet and corresponding to terraform is in synch with DRG route rules in OCI console. If not, please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Global directory which is inside the customer outdir will have all RPC related files and scripts. The RPC resources(modules,provider configurations etc) are generated dynamically for the tenancy and can work along only with CD3 automation toolkit. Choose option 'Network' and then 'Customer Connectivity' for creating RPC in GreenField workflow. Output files are created under /cd3user/tenancies/<customer_name>/terraform_files/global/rpc directory Go back to Networking Scenarios","title":"Create Network resources-Jenkins"},{"location":"NetworkingScenariosGF-Jenkins/#executing-networking-scenarios-using-toolkit-via-jenkins","text":"","title":"Executing Networking Scenarios using toolkit via Jenkins"},{"location":"NetworkingScenariosGF-Jenkins/#managing-network-for-greenfield-workflow","text":"Create Network Modify Network Modify Security Rules, Route Rules and DRG Route Rules Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Add/Modify/Delete NSGs Add/Modify/Delete VLANs Remote Peering Connections NOTE-","title":"Managing Network for Greenfield Workflow"},{"location":"NetworkingScenariosGF-Jenkins/#create-network","text":"Creation of Networking components using Automation Toolkit involves four simple steps. - Add the networking resource details to appropriate Excel Sheets. - Running the setUpOCI pipeline in the toolkit to generate auto.tfvars. - Executing terraform pipeline to provision the resources in OCI. - Exporting the automatically generated Security Rules and Route Rules by the toolkit to CD3 Excel Sheet. Below are the steps to create Network that includes VCNs, Subnets, DHCP, DRG, Security List, Route Tables, DRG Route Tables, NSGs, etc. Choose appropriate excel sheet from Excel Templates and fill the required Network details in the Networking Tabs - VCNs, DRGs, VCN Info, DHCP, Subnets, NSGs tabs. Execute the setupOCI pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose option 'Validate CD3' and then 'Validate Networks' to check for syntax errors in Excel sheet. Examine the log file generated at /cd3user/tenancies/<customer_name>/<customer_name>_cd3validator.log . If there are errors, please rectify them accordingly and proceed to the next step. Choose 'Create Network' under 'Network' from the displayed options. Click on Build. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the creation of Networking components in OCI. Verify the components in console. However the details of the default security lists and default route tables are not available in the CD3 Excel sheet yet. Inorder to export that data please follow the below steps: Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed options. Choose below sub-options: (Make sure to choose all the three optionsfor the first time) Security Rules Export Security Rules (From OCI into SecRulesinOCI sheet) Route Rules Export Route Rules (From OCI into RouteRulesinOCI sheet) DRG Route Rules Export DRG Route Rules (From OCI into DRGRouteRulesinOCI sheet) Click on Build. This completes the steps for Creating the Network in OCI and exporting the default rules to the CD3 Excel Sheet using the Automation Toolkit. Go back to Networking Scenarios","title":"Create Network"},{"location":"NetworkingScenariosGF-Jenkins/#modify-network","text":"Modifying the Networking components using Automation Toolkit involves three simple steps. Add/modify the details of networking components like the VCNs, Subnets, DHCP and DRG in Excel Sheet. Running the the setUpOCI pipeline in the toolkit to generate auto.tfvars. Executing Terraform pipeline to provision/modify the resources in OCI. Note : Follow these steps to modify Security Rules, Route Rules and DRG Route Rules Steps in detail : Modify your excel sheet to update required data in the Tabs - VCNs, DRGs, VCN Info, DHCP and Subnets. Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) To Validate the CD3 excel Tabs - Choose option 'Validate CD3' and 'Validate Networks' from sub-menu to check for syntax errors in Excel sheet. Examine the log file generated at /cd3user/tenancies/<customer_name>/<customer_name>_cd3validator.logs . If there are errors, please rectify them accordingly and proceed to the next step. Choose option to 'Modify Network' under 'Network' from the displayed options. Once the execution is successful, multiple .tfvars related to networking like <customer_name>_major-objects.auto.tfvars and more will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> . Existing files will move into respective backup folders. Note- : Make sure to export Sec Rules, Route Rules, DRG Route Rules to CD3 Excel Sheet before executing this option. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the modification of Networking components in OCI. Verify the components in console. Go back to Networking Scenarios","title":"Modify Network"},{"location":"NetworkingScenariosGF-Jenkins/#modify-security-rules-route-rules-and-drg-route-rules","text":"Follow the below steps to add, update or delete the following components: - Security Lists and Security Rules - Route Table and Route Rules - DRG Route Table and DRG Route Rules Modify your excel sheet to update required data in the Tabs - RouteRulesInOCI, SecRulesInOCI, DRGRouteRulesInOCI tabs. Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed options. Choose below sub-options: Security Rules: Add/Modify/Delete Security Rules (Reads SecRulesinOCI sheet) Route Rules: Add/Modify/Delete Route Rules (Reads RouteRulesinOCI sheet) DRG Route Rules: Add/Modify/Delete DRG Route Rules (Reads DRGRouteRulesinOCI sheet) Once the execution is successful, _seclists.auto.tfvars , _routetables.auto.tfvars and _drg-routetables.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir> . Existing files will move into respective backup folders. NOTE : This will create TF for only those Security Lists and Route Tables in VCNs which are part of cd3 and skip any VCNs that have been created outside of cd3 execution. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the modification of Security Rules, Route Rules and DRG Route Rules in OCI. Verify the components in console. Go back to Networking Scenarios","title":"Modify Security Rules, Route Rules and DRG Route Rules"},{"location":"NetworkingScenariosGF-Jenkins/#sync-manual-changes-done-in-oci-of-security-rules-route-rules-and-drg-route-rules-with-cd3-excel-sheet-and-terraform","text":"Follow the below process to export the rules to the same CD3 Excel Sheet as the one used to Create Network, and to sync the Terraform files with OCI whenever an user adds, modifies or deletes rules in OCI Console manually. NOTE : Make sure to close your Excel sheet during the export process. Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed menu. Choose below sub-options: Security Rules Export Security Rules (From OCI into SecRulesinOCI sheet) Add/Modify/Delete Security Rules (Reads SecRulesinOCI sheet) Route Rules Export Route Rules (From OCI into RouteRulesinOCI sheet) Add/Modify/Delete Route Rules (Reads RouteRulesinOCI sheet) DRG Route Rules Export DRG Route Rules (From OCI into DRGRouteRulesinOCI sheet) Add/Modify/Delete DRG Route Rules (Reads DRGRouteRulesinOCI sheet) Once the execution is successful, 'RouteRulesInOCI', 'SecRulesInOCI', 'DRGRouteRulesInOCI' tabs of the excel sheet will be updated with the rules exported from OCI. And <customer_name>_seclists.auto.tfvars , <customer_name>_routetables.auto.tfvars and <customer_name>_drg-routetables.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir> It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the export of Security Rules, Route Rules and DRG Route Rules from OCI. Terraform plan/apply should be in sync with OCI. Go back to Networking Scenarios","title":"Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform"},{"location":"NetworkingScenariosGF-Jenkins/#addmodifydelete-nsgs","text":"Follow the below steps to update NSGs. Modify your excel sheet to update required data in the Tabs - NSGs. Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed menu. Choose below sub-option: Network Security Groups Add/Modify/Delete NSGs (Reads NSGs sheet) Once the execution is successful, <customer_name>_nsgs.auto.tfvars will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> . Existing files will move into respective backup folders. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'nsg'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. This completes the modification of NSGs in OCI. Verify the components in console. Go back to Networking Scenarios","title":"Add/Modify/Delete NSGs"},{"location":"NetworkingScenariosGF-Jenkins/#addmodifydelete-vlans","text":"Follow the below steps to update VLANs. Modify your excel sheet to update required data in the Tabs - SubnetsVLANs. Make sure that the RouteRulesinOCI sheet and corresponing terraform is in synch with route rules in OCI console. If not, please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Execute the setupOCI.py pipeline with Workflow as Create Resources in OCI(Greenfield Workflow) Choose 'Network' from the displayed menu. Choose below sub-option: Add/Modify/Delete VLANs (Reads SubnetsVLANs sheet) Once the execution is successful, <customer_name>_vlans.auto.tfvars will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> . Existing files will move into respective backup folders. <customer_name>\\routetables.auto.tfvars file will also be updated with the route table information specified for each VLAN. It will show different stages of execution of setUpOCI pipeline and also launch the terraform-apply pipeline for 'vlan' and 'network'. Click on Proceed for 'Get Approval' stage of the terraform pipeline. Again make sure to export the Route Rules in OCI into excel and terraform. Please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform This completes the modification of VLANs in OCI. Verify the components in console.","title":"Add/Modify/Delete VLANs"},{"location":"NetworkingScenariosGF-Jenkins/#rpcs","text":"Remote VCN peering is the process of connecting two VCNs in different regions (but the same tenancy). The peering allows the VCNs' resources to communicate using private IP addresses without routing the traffic over the internet or through your on-premises network. Modify your excel sheet to update required data in the Tabs - DRGs. The source and target RPC details to be entered in DRG sheet for establishing a connection. Please check the example in excel file for reference. Make sure that the DRGRouteRulesinOCI sheet and corresponding to terraform is in synch with DRG route rules in OCI console. If not, please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Global directory which is inside the customer outdir will have all RPC related files and scripts. The RPC resources(modules,provider configurations etc) are generated dynamically for the tenancy and can work along only with CD3 automation toolkit. Choose option 'Network' and then 'Customer Connectivity' for creating RPC in GreenField workflow. Output files are created under /cd3user/tenancies/<customer_name>/terraform_files/global/rpc directory Go back to Networking Scenarios","title":"RPCs"},{"location":"NetworkingScenariosGF/","text":"Networking Scenarios Managing Network for Greenfield Workflow Create Network Use an existing DRG in OCI while creating the network Modify Network Modify Security Rules, Route Rules and DRG Route Rules Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Add/Modify/Delete NSGs Add/Modify/Delete VLANs Remote Peering Connections NOTE- Before you start with Network Creation, make sure you have run 'Fetch Compartments OCIDs to variables file'. Create Network Creation of Networking components using Automation Toolkit involves four simple steps. Add the networking resource details to appropriate Excel Sheets. Running the toolkit to generate auto.tfvars. Executing Terraform commands to provision the resources in OCI. Exporting the automatically generated Security Rules and Route Rules by the toolkit to CD3 Excel Sheet. Below are the steps in detail to create Network that includes VCNs, Subnets, DHCP, DRG, Security List, Route Tables, DRG Route Tables, NSGs, etc. Choose appropriate excel sheet from Excel Templates and fill the required Network details in the Networking Tabs - VCNs, DRGs, VCN Info, DHCP, Subnets, NSGs tabs. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose option 'Validate CD3' and then 'Validate Network(VCNs, Subnets, DHCP, DRGs)' to check for syntax errors in Excel sheet. Examine the log file generated at /cd3user/tenancies/<customer_name>/<customer_name>_cd3validator.log. If there are errors, please rectify them accordingly and proceed to the next step. Choose option to 'Create Network' under 'Network' from the displayed menu. Once the execution is successful, multiple .tfvars related to networking like _major-objects.auto.tfvars and more will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the creation of Networking components in OCI. Verify the components in console. However the details of the default security lists and default route tables may not be available in the CD3 Excel sheet yet. Inorder to export that data please follow the below steps: Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-options: (Make sure to choose all the three options for the first time) Security Rules -Export Security Rules (From OCI into SecRulesinOCI sheet) Route Rules -Export Route Rules (From OCI into RouteRulesinOCI sheet) DRG Route Rules -Export DRG Route Rules (From OCI into DRGRouteRulesinOCI sheet) This completes the steps for Creating the Network in OCI and exporting the default rules to the CD3 Excel Sheet using the Automation Toolkit. Use an existing DRG in OCI while creating the network In some scenarios, a DRG has already been created in the tenancy and rest of the Network components still need to be created. In such cases, generate the networking related tfvars using same process mentioned above till Step 4. Use same name for DRG in DRGs tab as present in OCI console. For Step 5, Navigate to the outdir path and execute the terraform commands: terraform init terraform import \"module.drgs[\\\"<<drgs terraform variable name>>\\\"].oci_core_drg.drg\" <<drg-ocid>> \u2192 This will Import the DRG into your state file. terraform plan \u2192 Terraform Plan will indicate to add all the other components except DRG. terraform apply Continue executing the remaining steps (from Step 6) of Create Network . Go back to Networking Scenarios Modify Network Modifying the Networking components using Automation Toolkit involves three simple steps. Add/modify the details of networking components like the VCNs, Subnets, DHCP and DRG in Excel Sheet. Running the toolkit to generate auto.tfvars. Executing Terraform commands to provision/modify the resources in OCI. Note : Follow these steps to modify Security Rules, Route Rules and DRG Route Rules Steps in detail : Modify your excel sheet to update required data in the Tabs - VCNs, DRGs, VCN Info, DHCP and Subnets. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties To Validate the CD3 excel Tabs - Choose option 'Validate CD3' and 'Validate Network(VCNs, Subnets, DHCP, DRGs)' from sub-menu to check for syntax errors in Excel sheet. Examine the log file generated at /cd3user/tenancies/<customer_name>/<customer_name>\\_cd3validator.logs_. If there are errors, please rectify them accordingly and proceed to the next step. Choose option to 'Modify Network' under 'Network' from the displayed menu. Once the execution is successful, multiple .tfvars related to networking like _major-objects.auto.tfvars and more will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir>_ . Existing files will move into respective backup folders. Note- : Make sure to export Sec Rules, Route Rules, DRG Route Rules to CD3 Excel Sheet before executing this option. Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the modification of Networking components in OCI. Verify the components in console. Go back to Networking Scenarios Modify Security Rules, Route Rules and DRG Route Rules Follow the below steps to add, update or delete the following components: Security Lists and Security Rules Route Table and Route Rules DRG Route Table and DRG Route Rules Modify your excel sheet to update required data in the Tabs - RouteRulesInOCI, SecRulesInOCI, DRGRouteRulesInOCI tabs. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-options: Security Rules - Add/Modify/Delete Security Rules (Reads SecRulesinOCI sheet) Route Rules - Add/Modify/Delete Route Rules (Reads RouteRulesinOCI sheet) DRG Route Rules - Add/Modify/Delete DRG Route Rules (Reads DRGRouteRulesinOCI sheet) Once the execution is successful, _seclists.auto.tfvars , _routetables.auto.tfvars and _drg-routetables.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>. Existing files will move into respective backup folders. NOTE : This will create TF for only those Security Lists and Route Tables in VCNs which are part of cd3 and skip any VCNs that have been created outside of cd3 execution. Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the modification of Security Rules, Route Rules and DRG Route Rules in OCI. Verify the components in console. Go back to Networking Scenarios Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Follow the below process to export the rules to the same CD3 Excel Sheet as the one used to Create Network, and to sync the Terraform files with OCI whenever an user adds, modifies or deletes rules in OCI Console manually. NOTE : Make sure to close your Excel sheet during the export process. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-options: Security Rules -Export Security Rules (From OCI into SecRulesinOCI sheet) Route Rules -Export Route Rules (From OCI into RouteRulesinOCI sheet) DRG Route Rules -Export DRG Route Rules (From OCI into DRGRouteRulesinOCI sheet) Once the execution is successful, 'RouteRulesInOCI', 'SecRulesInOCI', 'DRGRouteRulesInOCI' tabs of the excel sheet will be updated with the rules exported from OCI. At this point, we only have our Excel sheet Tabs updated, proceed to the next step to create the Terraform Files for the same. Choose 'Network' from the displayed menu. Choose below sub-options: Security Rules -Add/Modify/Delete Security Rules (Reads SecRulesinOCI sheet) Route Rules - Add/Modify/Delete Route Rules (Reads RouteRulesinOCI sheet) DRG Route Rules - Add/Modify/Delete DRG Route Rules (Reads DRGRouteRulesinOCI sheet) Once the execution is successful, <customer_name>_seclists.auto.tfvars, <customer_name>_routetables.auto.tfvars and <customer_name>drg-routetables.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the export of Security Rules, Route Rules and DRG Route Rules from OCI. Terraform plan/apply should be in sync with OCI. Go back to Networking Scenarios Add/Modify/Delete NSGs Follow the below steps to update NSGs. Modify your excel sheet to update required data in the Tabs - NSGs. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-option: Network Security Groups -Add/Modify/Delete NSGs (Reads NSGs sheet) Once the execution is successful, <customer_name>_nsgs.auto.tfvars will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> . Existing files will move into respective backup folders. Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the modification of NSGs in OCI. Verify the components in console. Go back to Networking Scenarios Add/Modify/Delete VLANs Follow the below steps to update VLANs. Modify your excel sheet to update required data in the Tabs - SubnetsVLANs. Make sure that the RouteRulesinOCI sheet and corresponing terraform is in synch with route rules in OCI console. If not, please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-option: Add/Modify/Delete VLANs (Reads SubnetsVLANs sheet) Once the execution is successful, <customer_name>\\_vlans.auto.tfvars will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir>. Existing files will move into respective backup folders. <customer_name>_routetables.auto.tfvars file will also be updated with the route table information specified for each VLAN. Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply Again make sure to export the Route Rules in OCI into excel and terraform. Please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform This completes the modification of VLANs in OCI. Verify the components in console. RPCs Remote VCN peering is the process of connecting two VCNs in different regions (but the same tenancy). The peering allows the VCNs' resources to communicate using private IP addresses without routing the traffic over the internet or through your on-premises network. Modify your excel sheet to update required data in the Tabs - DRGs. The source and target RPC details to be entered in DRG sheet for establishing a connection. Please check the example in excel file for reference. Make sure that the DRGRouteRulesinOCI sheet and corresponding to terraform is in synch with DRG route rules in OCI console. If not, please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Global directory which is inside the customer outdir will have all RPC related files and scripts. The RPC resources(modules,provider configurations etc) are generated dynamically for the tenancy and can work along only with CD3 automation toolkit. Choose option 'Network' and then 'Customer Connectivity' for creating RPC in GreenField workflow. Output files are created under /cd3user/tenancies/<customer_name>/terraform_files/global/rpc directory Go back to Networking Scenarios","title":"Create Network resources-CLI"},{"location":"NetworkingScenariosGF/#networking-scenarios","text":"","title":"Networking Scenarios"},{"location":"NetworkingScenariosGF/#managing-network-for-greenfield-workflow","text":"Create Network Use an existing DRG in OCI while creating the network Modify Network Modify Security Rules, Route Rules and DRG Route Rules Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Add/Modify/Delete NSGs Add/Modify/Delete VLANs Remote Peering Connections NOTE- Before you start with Network Creation, make sure you have run 'Fetch Compartments OCIDs to variables file'.","title":"Managing Network for Greenfield Workflow"},{"location":"NetworkingScenariosGF/#create-network","text":"Creation of Networking components using Automation Toolkit involves four simple steps. Add the networking resource details to appropriate Excel Sheets. Running the toolkit to generate auto.tfvars. Executing Terraform commands to provision the resources in OCI. Exporting the automatically generated Security Rules and Route Rules by the toolkit to CD3 Excel Sheet. Below are the steps in detail to create Network that includes VCNs, Subnets, DHCP, DRG, Security List, Route Tables, DRG Route Tables, NSGs, etc. Choose appropriate excel sheet from Excel Templates and fill the required Network details in the Networking Tabs - VCNs, DRGs, VCN Info, DHCP, Subnets, NSGs tabs. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose option 'Validate CD3' and then 'Validate Network(VCNs, Subnets, DHCP, DRGs)' to check for syntax errors in Excel sheet. Examine the log file generated at /cd3user/tenancies/<customer_name>/<customer_name>_cd3validator.log. If there are errors, please rectify them accordingly and proceed to the next step. Choose option to 'Create Network' under 'Network' from the displayed menu. Once the execution is successful, multiple .tfvars related to networking like _major-objects.auto.tfvars and more will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the creation of Networking components in OCI. Verify the components in console. However the details of the default security lists and default route tables may not be available in the CD3 Excel sheet yet. Inorder to export that data please follow the below steps: Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-options: (Make sure to choose all the three options for the first time) Security Rules -Export Security Rules (From OCI into SecRulesinOCI sheet) Route Rules -Export Route Rules (From OCI into RouteRulesinOCI sheet) DRG Route Rules -Export DRG Route Rules (From OCI into DRGRouteRulesinOCI sheet) This completes the steps for Creating the Network in OCI and exporting the default rules to the CD3 Excel Sheet using the Automation Toolkit.","title":"Create Network"},{"location":"NetworkingScenariosGF/#use-an-existing-drg-in-oci-while-creating-the-network","text":"In some scenarios, a DRG has already been created in the tenancy and rest of the Network components still need to be created. In such cases, generate the networking related tfvars using same process mentioned above till Step 4. Use same name for DRG in DRGs tab as present in OCI console. For Step 5, Navigate to the outdir path and execute the terraform commands: terraform init terraform import \"module.drgs[\\\"<<drgs terraform variable name>>\\\"].oci_core_drg.drg\" <<drg-ocid>> \u2192 This will Import the DRG into your state file. terraform plan \u2192 Terraform Plan will indicate to add all the other components except DRG. terraform apply Continue executing the remaining steps (from Step 6) of Create Network . Go back to Networking Scenarios","title":"Use an existing DRG in OCI while creating the network"},{"location":"NetworkingScenariosGF/#modify-network","text":"Modifying the Networking components using Automation Toolkit involves three simple steps. Add/modify the details of networking components like the VCNs, Subnets, DHCP and DRG in Excel Sheet. Running the toolkit to generate auto.tfvars. Executing Terraform commands to provision/modify the resources in OCI. Note : Follow these steps to modify Security Rules, Route Rules and DRG Route Rules Steps in detail : Modify your excel sheet to update required data in the Tabs - VCNs, DRGs, VCN Info, DHCP and Subnets. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties To Validate the CD3 excel Tabs - Choose option 'Validate CD3' and 'Validate Network(VCNs, Subnets, DHCP, DRGs)' from sub-menu to check for syntax errors in Excel sheet. Examine the log file generated at /cd3user/tenancies/<customer_name>/<customer_name>\\_cd3validator.logs_. If there are errors, please rectify them accordingly and proceed to the next step. Choose option to 'Modify Network' under 'Network' from the displayed menu. Once the execution is successful, multiple .tfvars related to networking like _major-objects.auto.tfvars and more will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir>_ . Existing files will move into respective backup folders. Note- : Make sure to export Sec Rules, Route Rules, DRG Route Rules to CD3 Excel Sheet before executing this option. Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the modification of Networking components in OCI. Verify the components in console. Go back to Networking Scenarios","title":"Modify Network"},{"location":"NetworkingScenariosGF/#modify-security-rules-route-rules-and-drg-route-rules","text":"Follow the below steps to add, update or delete the following components: Security Lists and Security Rules Route Table and Route Rules DRG Route Table and DRG Route Rules Modify your excel sheet to update required data in the Tabs - RouteRulesInOCI, SecRulesInOCI, DRGRouteRulesInOCI tabs. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-options: Security Rules - Add/Modify/Delete Security Rules (Reads SecRulesinOCI sheet) Route Rules - Add/Modify/Delete Route Rules (Reads RouteRulesinOCI sheet) DRG Route Rules - Add/Modify/Delete DRG Route Rules (Reads DRGRouteRulesinOCI sheet) Once the execution is successful, _seclists.auto.tfvars , _routetables.auto.tfvars and _drg-routetables.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>. Existing files will move into respective backup folders. NOTE : This will create TF for only those Security Lists and Route Tables in VCNs which are part of cd3 and skip any VCNs that have been created outside of cd3 execution. Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the modification of Security Rules, Route Rules and DRG Route Rules in OCI. Verify the components in console. Go back to Networking Scenarios","title":"Modify Security Rules, Route Rules and DRG Route Rules"},{"location":"NetworkingScenariosGF/#sync-manual-changes-done-in-oci-of-security-rules-route-rules-and-drg-route-rules-with-cd3-excel-sheet-and-terraform","text":"Follow the below process to export the rules to the same CD3 Excel Sheet as the one used to Create Network, and to sync the Terraform files with OCI whenever an user adds, modifies or deletes rules in OCI Console manually. NOTE : Make sure to close your Excel sheet during the export process. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-options: Security Rules -Export Security Rules (From OCI into SecRulesinOCI sheet) Route Rules -Export Route Rules (From OCI into RouteRulesinOCI sheet) DRG Route Rules -Export DRG Route Rules (From OCI into DRGRouteRulesinOCI sheet) Once the execution is successful, 'RouteRulesInOCI', 'SecRulesInOCI', 'DRGRouteRulesInOCI' tabs of the excel sheet will be updated with the rules exported from OCI. At this point, we only have our Excel sheet Tabs updated, proceed to the next step to create the Terraform Files for the same. Choose 'Network' from the displayed menu. Choose below sub-options: Security Rules -Add/Modify/Delete Security Rules (Reads SecRulesinOCI sheet) Route Rules - Add/Modify/Delete Route Rules (Reads RouteRulesinOCI sheet) DRG Route Rules - Add/Modify/Delete DRG Route Rules (Reads DRGRouteRulesinOCI sheet) Once the execution is successful, <customer_name>_seclists.auto.tfvars, <customer_name>_routetables.auto.tfvars and <customer_name>drg-routetables.auto.tfvars file will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the export of Security Rules, Route Rules and DRG Route Rules from OCI. Terraform plan/apply should be in sync with OCI. Go back to Networking Scenarios","title":"Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform"},{"location":"NetworkingScenariosGF/#addmodifydelete-nsgs","text":"Follow the below steps to update NSGs. Modify your excel sheet to update required data in the Tabs - NSGs. Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-option: Network Security Groups -Add/Modify/Delete NSGs (Reads NSGs sheet) Once the execution is successful, <customer_name>_nsgs.auto.tfvars will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> . Existing files will move into respective backup folders. Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply This completes the modification of NSGs in OCI. Verify the components in console. Go back to Networking Scenarios","title":"Add/Modify/Delete NSGs"},{"location":"NetworkingScenariosGF/#addmodifydelete-vlans","text":"Follow the below steps to update VLANs. Modify your excel sheet to update required data in the Tabs - SubnetsVLANs. Make sure that the RouteRulesinOCI sheet and corresponing terraform is in synch with route rules in OCI console. If not, please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Execute the setupOCI.py file with workflow_type parameter value to create_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose 'Network' from the displayed menu. Choose below sub-option: Add/Modify/Delete VLANs (Reads SubnetsVLANs sheet) Once the execution is successful, <customer_name>\\_vlans.auto.tfvars will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir>. Existing files will move into respective backup folders. <customer_name>_routetables.auto.tfvars file will also be updated with the route table information specified for each VLAN. Navigate to the above path and execute the terraform commands: terraform init terraform plan terraform apply Again make sure to export the Route Rules in OCI into excel and terraform. Please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform This completes the modification of VLANs in OCI. Verify the components in console.","title":"Add/Modify/Delete VLANs"},{"location":"NetworkingScenariosGF/#rpcs","text":"Remote VCN peering is the process of connecting two VCNs in different regions (but the same tenancy). The peering allows the VCNs' resources to communicate using private IP addresses without routing the traffic over the internet or through your on-premises network. Modify your excel sheet to update required data in the Tabs - DRGs. The source and target RPC details to be entered in DRG sheet for establishing a connection. Please check the example in excel file for reference. Make sure that the DRGRouteRulesinOCI sheet and corresponding to terraform is in synch with DRG route rules in OCI console. If not, please follow procedure specified in Sync manual changes done in OCI of Security Rules, Route Rules and DRG Route Rules with CD3 Excel Sheet and Terraform Global directory which is inside the customer outdir will have all RPC related files and scripts. The RPC resources(modules,provider configurations etc) are generated dynamically for the tenancy and can work along only with CD3 automation toolkit. Choose option 'Network' and then 'Customer Connectivity' for creating RPC in GreenField workflow. Output files are created under /cd3user/tenancies/<customer_name>/terraform_files/global/rpc directory Go back to Networking Scenarios","title":"RPCs"},{"location":"NetworkingScenariosNGF/","text":"Networking Scenarios Managing Network for Non-Greenfield Workflow Export Network Add a new or modify the existing networking components NOTE- Before you start with Network Export, make sure you have run 'Fetch Compartments OCIDs to variables file'. Export Network Follow the below steps to export the Networking components that includes VCNs, Subnets, DHCP, DRG, Security List, Route Tables, DRG Route Tables, NSGs, etc to CD3 Excel Sheet and create the Terraform state. Use the CD3-Blank-Template.xlsx to export the networking resources into the Tabs - VCNs, DRGs, VCN Info, DHCP, Subnets, NSGs, RouteRulesInOCI, SecRulesInOCI,DRGRouteRulesInOCI tabs. Execute the setupOCI.py file with workflow_type parameter value to export_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose one of the below available sub-options from 'Export Network' of the main menu. Export all Network Components Export Network components for VCNs, DRGs and DRGRouteRulesinOCI Tabs Export Network components for DHCP Tab Export Network components for SecRulesinOCI Tab Export Network components for RouteRulesinOCI Tab Export Network components for SubnetsVLANs Tab Export Network components for NSGs Tab Once the execution is successful, networking related .tfvars files and .sh files containing import statements will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> Also,The RPC related .tfvars and .sh files containing import statements will be generated in global directory which is inside the /cd3user/tenancies/<customer_name>/terraform_files/ folder. NOTE: The oci_core_drg_attachment_management for RPC resources will be shown as created at the end of import process, but it doesn't actually create any resources and can be safely ignored. Navigate to the above path and execute the terraform commands: terraform init Execute the shell scirpts of networking components terraform plan \u2192 Terraform Plan must show that all the components are in sync. This completes the export of Networking components from OCI. Sample of CD3 Excel after export: (DO NOT Modify the highlighted columns) (Showing old images below) VCNs tab: Subnets tab: Go back to Networking Scenarios Add a new or modify the existing networking components Export the Networking components by following the steps above . (Note that here workflow_type flag is set to export_resources) Follow the process to add new components such as VCN/DHCP/DRG/IGW/NGW/SGW/LPG/Subnet etc. (Note that here workflow_type flag is set to create_resources) Go back to Networking Scenarios","title":"Export Network resources-CLI"},{"location":"NetworkingScenariosNGF/#networking-scenarios","text":"","title":"Networking Scenarios"},{"location":"NetworkingScenariosNGF/#managing-network-for-non-greenfield-workflow","text":"Export Network Add a new or modify the existing networking components NOTE- Before you start with Network Export, make sure you have run 'Fetch Compartments OCIDs to variables file'.","title":"Managing Network for Non-Greenfield Workflow"},{"location":"NetworkingScenariosNGF/#export-network","text":"Follow the below steps to export the Networking components that includes VCNs, Subnets, DHCP, DRG, Security List, Route Tables, DRG Route Tables, NSGs, etc to CD3 Excel Sheet and create the Terraform state. Use the CD3-Blank-Template.xlsx to export the networking resources into the Tabs - VCNs, DRGs, VCN Info, DHCP, Subnets, NSGs, RouteRulesInOCI, SecRulesInOCI,DRGRouteRulesInOCI tabs. Execute the setupOCI.py file with workflow_type parameter value to export_resources : python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose one of the below available sub-options from 'Export Network' of the main menu. Export all Network Components Export Network components for VCNs, DRGs and DRGRouteRulesinOCI Tabs Export Network components for DHCP Tab Export Network components for SecRulesinOCI Tab Export Network components for RouteRulesinOCI Tab Export Network components for SubnetsVLANs Tab Export Network components for NSGs Tab Once the execution is successful, networking related .tfvars files and .sh files containing import statements will be generated under the folder /cd3user/tenancies/<customer_name>/terraform_files/<region_dir>/<service_dir> Also,The RPC related .tfvars and .sh files containing import statements will be generated in global directory which is inside the /cd3user/tenancies/<customer_name>/terraform_files/ folder. NOTE: The oci_core_drg_attachment_management for RPC resources will be shown as created at the end of import process, but it doesn't actually create any resources and can be safely ignored. Navigate to the above path and execute the terraform commands: terraform init Execute the shell scirpts of networking components terraform plan \u2192 Terraform Plan must show that all the components are in sync. This completes the export of Networking components from OCI. Sample of CD3 Excel after export: (DO NOT Modify the highlighted columns) (Showing old images below) VCNs tab: Subnets tab: Go back to Networking Scenarios","title":"Export Network"},{"location":"NetworkingScenariosNGF/#add-a-new-or-modify-the-existing-networking-components","text":"Export the Networking components by following the steps above . (Note that here workflow_type flag is set to export_resources) Follow the process to add new components such as VCN/DHCP/DRG/IGW/NGW/SGW/LPG/Subnet etc. (Note that here workflow_type flag is set to create_resources) Go back to Networking Scenarios","title":"Add a new or modify the existing networking components"},{"location":"NonGreenField-Jenkins/","text":"Export Resources from OCI via Jenkins(Non-Greenfield Workflow) Note: : Please make sure that service for which export is done does not have existing tfvars/state file. Step 1 : Choose the appropriate CD3 Excel sheet template from Excel Templates Choose CD3-Blank-template.xlsx for an empty sheet. Step 2 : Login to Jenkins URL with user created after initialization and click on setUpOCI pipeline from Dashboard. Click on Build with Parameters from left side menu. Note: - Only one user at a time using the Jenkins setup is supported in the current release of the toolkit. Step 3 : Upload the above chosen Excel sheet in Excel_Template section. This will copy the Excel file at /cd3user/tenancies/<customer_name> inside the container. It will also take backup of existing Excel on the container by appending the current datetime if same filename is uploaded in multiple executions. Step 4: Select the workflow as Export Resources from OCI (Non-Greenfield Workflow). Choose single or multiple MainOptions as required and then corresponding SubOptions. Below screenshot shows export of Network and Compute. Step 5: Specify region and compartment from where you want to export the data. It also asks for service specific filters like display name patterns for compute. Leave empty if no filter is needed. Click on Build at the bottom. Step 6: setUpOCI pipeline is triggered and stages are executed as shown below: Expected Output of 'Execute setUpOCI' stage: Overwrites the specific tabs of Excel sheet with the exported resource details from OCI. Generates Terraform Configuration files - *.auto.tfvars. Generates shell scripts with import commands - tf_import_commands_<resource>_nonGF.sh Expected Output of 'Run Import Commands' stage: Executes shell scripts with import commands( tf_import_commands_<resource>_nonGF.sh ) generated in the previous stage Expected Output of Terraform Pipelines: Respective pipelines will get triggered automatically from setUpOCI pipeline based on the services chosen for export. You could also trigger manually when required. If 'Run Import Commands' stage was successful (ie.. tf_import_commands_<resource>_nonGF.sh ran successfully for all services chosen for export), respective terraform pipelines triggered should have 'Terraform Plan' stage show as 'No Changes' Note: Once you have exported the required resources and imported into tfstate, you can use the toolkit to modify them or create new on top of them using 'Create Resources in OCI' workflow.","title":"Export Resources from OCI using Jenkins(Non-Greenfield Workflow)"},{"location":"NonGreenField-Jenkins/#export-resources-from-oci-via-jenkinsnon-greenfield-workflow","text":"Note: : Please make sure that service for which export is done does not have existing tfvars/state file. Step 1 : Choose the appropriate CD3 Excel sheet template from Excel Templates Choose CD3-Blank-template.xlsx for an empty sheet. Step 2 : Login to Jenkins URL with user created after initialization and click on setUpOCI pipeline from Dashboard. Click on Build with Parameters from left side menu. Note: - Only one user at a time using the Jenkins setup is supported in the current release of the toolkit. Step 3 : Upload the above chosen Excel sheet in Excel_Template section. This will copy the Excel file at /cd3user/tenancies/<customer_name> inside the container. It will also take backup of existing Excel on the container by appending the current datetime if same filename is uploaded in multiple executions. Step 4: Select the workflow as Export Resources from OCI (Non-Greenfield Workflow). Choose single or multiple MainOptions as required and then corresponding SubOptions. Below screenshot shows export of Network and Compute. Step 5: Specify region and compartment from where you want to export the data. It also asks for service specific filters like display name patterns for compute. Leave empty if no filter is needed. Click on Build at the bottom. Step 6: setUpOCI pipeline is triggered and stages are executed as shown below: Expected Output of 'Execute setUpOCI' stage: Overwrites the specific tabs of Excel sheet with the exported resource details from OCI. Generates Terraform Configuration files - *.auto.tfvars. Generates shell scripts with import commands - tf_import_commands_<resource>_nonGF.sh Expected Output of 'Run Import Commands' stage: Executes shell scripts with import commands( tf_import_commands_<resource>_nonGF.sh ) generated in the previous stage Expected Output of Terraform Pipelines: Respective pipelines will get triggered automatically from setUpOCI pipeline based on the services chosen for export. You could also trigger manually when required. If 'Run Import Commands' stage was successful (ie.. tf_import_commands_<resource>_nonGF.sh ran successfully for all services chosen for export), respective terraform pipelines triggered should have 'Terraform Plan' stage show as 'No Changes' Note: Once you have exported the required resources and imported into tfstate, you can use the toolkit to modify them or create new on top of them using 'Create Resources in OCI' workflow.","title":"Export Resources from OCI via Jenkins(Non-Greenfield Workflow)"},{"location":"NonGreenField/","text":"Export Resources from OCI (Non-Greenfield Workflow) Note Please make sure that service for which export is done does not have existing tfvars/state file. Course of actions involved in Exporting objects from OCI- Automation Tool Kit fetches the data for the supported services. You can chose to export the data from a specific region or the compartment. Exported data is written to appropriate sheets of the CD3 Excel Sheet based on the resources being exported. Tool Kit then generates the TF configuration files/auto.tfvars files for these exported resources. It also generates a shell script - tf_import_commands_<resource>_nonGF.sh that has the import commands, to import the state of the resources to tfstate file.(This helps to manage the resources via Terraform in future). Step 1: Chose the appropriate CD3 Excel sheet template from Excel Templates Step 2: Put CD3 Excel at the appropriate location. Modify/Review /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties with workflow_type set to export_resources as shown below: #Input variables required to run setUpOCI script #path to output directory where terraform file will be generated. eg /cd3user/tenancies/<customer_name>/terraform_files outdir=/cd3user/tenancies/demotenancy/terraform_files/ #prefix for output terraform files eg <customer_name> like demotenancy prefix=demotenancy # auth mechanism for OCI APIs - api_key,instance_principal,session_token auth_mechanism=api_key #input config file for Python API communication with OCI eg /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_config; config_file=/cd3user/tenancies/demotenancy/.config_files/demotenancy_oci_config # Leave it blank if you want single outdir or specify outdir_structure_file.properties containing directory structure for OCI services. outdir_structure_file=/cd3user/tenancies/demotenancy/demotenancy_outdir_structure_file.properties #path to cd3 excel eg /cd3user/tenancies/<customer_name>/CD3-Customer.xlsx cd3file=/cd3user/tenancies/demotenancy/CD3-Blank-template.xlsx #specify create_resources to create new resources in OCI(greenfield workflow) #specify export_resources to export resources from OCI(non-greenfield workflow) workflow_type=export_resources Step 3: Execute the SetUpOCI.py script to start exporting the resources to CD3 and creating the terraform configuration files. Command to Execute: cd /cd3user/oci_tools/cd3_automation_toolkit/ python setUpOCI.py <path_to_setupOCI.properties> ie python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties \u2192 Example execution of the wrapper script: Choose the resources by specifying a single option (for choosing one of these resources) or comma-separated values (to choose multiple resources) as shown in the sample screenshot above. Make sure to execute \"Fetch Compartments OCIDs to variables file\" from CD3 Services in setUpOCI menu at least once. This will ensure that the variables file in outdir is updated with the OCID information of all the compartments. Toolkit will over-write the specific tabs of CD3 Excel sheet with exported data for that resource in OCI while the other tabs remain intact. Expected Outputs: a. Excel sheet with the resource details from OCI b. Terraform Configuration files - *.auto.tfvars c. Shell Script with import commands - tf_import_commands_ <resource> _nonGF.sh Action: Execute the tf_import_commands_ <resource> _nonGF.sh files that are generated in the outdir. The terraform plan should show that infrastructure is up-to-date with no changes required for all regions. Note Once the export (including the execution of tf_import_commands_ <resource> _nonGF.sh ) is complete, switch the value of workflow_type back to create_resources . This allows the Tool Kit to support the tenancy as Green Field from this point onwards. Example - Export Identity Follow the below steps to quickly export Identity components from OCI. Use the excel CD3-Blank-template and place it at the location /cd3user/tenancies/<customer_name> which is also mapped to your local directory. Edit the setUpOCI.properties at location: /cd3user/tenancies <customer_name>/<customer_name>_setUpOCI.properties with appropriate values. Update the cd3file parameter to specify the CD3 excel sheet path. Set the workflow_type parameter value to export_resources . (for Non Greenfield Workflow.) Change Directory to 'cd3_automation_toolkit' : cd /cd3user/oci_tools/cd3_automation_toolkit/ and execute the setupOCI.py file: python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose option 'Export Identity' from the displayed menu. Once the execution is successful, you will see: Filled in tabs - Compartments, Groups, Policies of Excel sheet tf_import_commands_identity_nonGF.sh <customer_name>_compartments.auto.tfvars, <customer_name>_groups.auto.tfvars, <customer_name>_policies.auto.tfvars Execute tf_import_commands_identity_nonGF.sh to start importing the identity components into tfstate file. Repeat the above process (except Step 5) to export other components from OCI.","title":"Export Resources from OCI using CLI (Non-Greenfield Workflow)"},{"location":"NonGreenField/#export-resources-from-oci-non-greenfield-workflow","text":"Note Please make sure that service for which export is done does not have existing tfvars/state file. Course of actions involved in Exporting objects from OCI- Automation Tool Kit fetches the data for the supported services. You can chose to export the data from a specific region or the compartment. Exported data is written to appropriate sheets of the CD3 Excel Sheet based on the resources being exported. Tool Kit then generates the TF configuration files/auto.tfvars files for these exported resources. It also generates a shell script - tf_import_commands_<resource>_nonGF.sh that has the import commands, to import the state of the resources to tfstate file.(This helps to manage the resources via Terraform in future). Step 1: Chose the appropriate CD3 Excel sheet template from Excel Templates Step 2: Put CD3 Excel at the appropriate location. Modify/Review /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties with workflow_type set to export_resources as shown below: #Input variables required to run setUpOCI script #path to output directory where terraform file will be generated. eg /cd3user/tenancies/<customer_name>/terraform_files outdir=/cd3user/tenancies/demotenancy/terraform_files/ #prefix for output terraform files eg <customer_name> like demotenancy prefix=demotenancy # auth mechanism for OCI APIs - api_key,instance_principal,session_token auth_mechanism=api_key #input config file for Python API communication with OCI eg /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_config; config_file=/cd3user/tenancies/demotenancy/.config_files/demotenancy_oci_config # Leave it blank if you want single outdir or specify outdir_structure_file.properties containing directory structure for OCI services. outdir_structure_file=/cd3user/tenancies/demotenancy/demotenancy_outdir_structure_file.properties #path to cd3 excel eg /cd3user/tenancies/<customer_name>/CD3-Customer.xlsx cd3file=/cd3user/tenancies/demotenancy/CD3-Blank-template.xlsx #specify create_resources to create new resources in OCI(greenfield workflow) #specify export_resources to export resources from OCI(non-greenfield workflow) workflow_type=export_resources Step 3: Execute the SetUpOCI.py script to start exporting the resources to CD3 and creating the terraform configuration files. Command to Execute: cd /cd3user/oci_tools/cd3_automation_toolkit/ python setUpOCI.py <path_to_setupOCI.properties> ie python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties \u2192 Example execution of the wrapper script: Choose the resources by specifying a single option (for choosing one of these resources) or comma-separated values (to choose multiple resources) as shown in the sample screenshot above. Make sure to execute \"Fetch Compartments OCIDs to variables file\" from CD3 Services in setUpOCI menu at least once. This will ensure that the variables file in outdir is updated with the OCID information of all the compartments. Toolkit will over-write the specific tabs of CD3 Excel sheet with exported data for that resource in OCI while the other tabs remain intact. Expected Outputs: a. Excel sheet with the resource details from OCI b. Terraform Configuration files - *.auto.tfvars c. Shell Script with import commands - tf_import_commands_ <resource> _nonGF.sh Action: Execute the tf_import_commands_ <resource> _nonGF.sh files that are generated in the outdir. The terraform plan should show that infrastructure is up-to-date with no changes required for all regions. Note Once the export (including the execution of tf_import_commands_ <resource> _nonGF.sh ) is complete, switch the value of workflow_type back to create_resources . This allows the Tool Kit to support the tenancy as Green Field from this point onwards.","title":"Export Resources from OCI (Non-Greenfield Workflow)"},{"location":"NonGreenField/#example-export-identity","text":"Follow the below steps to quickly export Identity components from OCI. Use the excel CD3-Blank-template and place it at the location /cd3user/tenancies/<customer_name> which is also mapped to your local directory. Edit the setUpOCI.properties at location: /cd3user/tenancies <customer_name>/<customer_name>_setUpOCI.properties with appropriate values. Update the cd3file parameter to specify the CD3 excel sheet path. Set the workflow_type parameter value to export_resources . (for Non Greenfield Workflow.) Change Directory to 'cd3_automation_toolkit' : cd /cd3user/oci_tools/cd3_automation_toolkit/ and execute the setupOCI.py file: python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Choose option 'Export Identity' from the displayed menu. Once the execution is successful, you will see: Filled in tabs - Compartments, Groups, Policies of Excel sheet tf_import_commands_identity_nonGF.sh <customer_name>_compartments.auto.tfvars, <customer_name>_groups.auto.tfvars, <customer_name>_policies.auto.tfvars Execute tf_import_commands_identity_nonGF.sh to start importing the identity components into tfstate file. Repeat the above process (except Step 5) to export other components from OCI.","title":"Example - Export Identity"},{"location":"OPAForCompliance/","text":"Open Policy Agent for Terraform: OPA is a powerful policy-as-code framework that enables you to define and enforce policies across your infrastructure-as-code (IaC) deployments. With OPA, you can seamlessly integrate policy checks into your Terraform workflows, ensuring that your infrastructure deployments adhere to your organization's security, compliance, and operational requirements. By leveraging OPA for Terraform, you can automate policy enforcement, eliminate manual checks, and enforce best practices consistently across your infrastructure-as-code projects. With OPA, you gain enhanced visibility and control over your Terraform deployments, reducing the risk of misconfigurations, security vulnerabilities, and compliance issues. As part of CD3, we have meticulously developed a comprehensive set of policies that strictly adhere to the CIS benchmarks. These policies serve as your shield, ensuring that any Infrastructure-as-Code (IAC) deployments made for Oracle Cloud Infrastructure (OCI) meet the highest security and compliance standards. Our carefully crafted policies act as gatekeepers, preventing any IAC deployments that do not align with the stringent security and compliance guidelines set by the CIS benchmarks for OCI. By leveraging our policies, you can ensure that your infrastructure deployments remain impervious to any potential vulnerabilities or non-compliance issues. Run OPA inside CD3 container Open your command line interface inside CD3 container and run OPA. You should see all available options for OPA. opa --help Currently CD3 container has OPA version 0.55.0 installed. Generate the terraform plan output in json format since OPA accepts that format alone for evaluation. terraform plan -out tfplan.binary terraform show -json tfplan.binary > tfplan.json Run the terraform plan against all the available OPA rules. It should return an empty array which means the plan has no non-compliant action against CIS benchmarks. opa eval -f pretty -b /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/OPA -i tfplan.json data.terraform.deny --fail-defined Alternatively, run the following command to evaluate just a sinle OPA rule say \"deny_ingress_for_sl.rego\" policy with a pretty output format: opa eval -f pretty -d /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/OPA/Networking/oci_deny_ingress_for_sl.rego -i tfplan.json data.terraform.deny This command will analyze the \"tfplan.json\" input file against the policy and display the evaluation results with a user-friendly format.","title":"OPA integration"},{"location":"OPAForCompliance/#open-policy-agent-for-terraform","text":"OPA is a powerful policy-as-code framework that enables you to define and enforce policies across your infrastructure-as-code (IaC) deployments. With OPA, you can seamlessly integrate policy checks into your Terraform workflows, ensuring that your infrastructure deployments adhere to your organization's security, compliance, and operational requirements. By leveraging OPA for Terraform, you can automate policy enforcement, eliminate manual checks, and enforce best practices consistently across your infrastructure-as-code projects. With OPA, you gain enhanced visibility and control over your Terraform deployments, reducing the risk of misconfigurations, security vulnerabilities, and compliance issues. As part of CD3, we have meticulously developed a comprehensive set of policies that strictly adhere to the CIS benchmarks. These policies serve as your shield, ensuring that any Infrastructure-as-Code (IAC) deployments made for Oracle Cloud Infrastructure (OCI) meet the highest security and compliance standards. Our carefully crafted policies act as gatekeepers, preventing any IAC deployments that do not align with the stringent security and compliance guidelines set by the CIS benchmarks for OCI. By leveraging our policies, you can ensure that your infrastructure deployments remain impervious to any potential vulnerabilities or non-compliance issues.","title":"Open Policy Agent for Terraform:"},{"location":"OPAForCompliance/#run-opa-inside-cd3-container","text":"Open your command line interface inside CD3 container and run OPA. You should see all available options for OPA. opa --help Currently CD3 container has OPA version 0.55.0 installed. Generate the terraform plan output in json format since OPA accepts that format alone for evaluation. terraform plan -out tfplan.binary terraform show -json tfplan.binary > tfplan.json Run the terraform plan against all the available OPA rules. It should return an empty array which means the plan has no non-compliant action against CIS benchmarks. opa eval -f pretty -b /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/OPA -i tfplan.json data.terraform.deny --fail-defined Alternatively, run the following command to evaluate just a sinle OPA rule say \"deny_ingress_for_sl.rego\" policy with a pretty output format: opa eval -f pretty -d /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/OPA/Networking/oci_deny_ingress_for_sl.rego -i tfplan.json data.terraform.deny This command will analyze the \"tfplan.json\" input file against the policy and display the evaluation results with a user-friendly format.","title":"Run OPA inside CD3 container"},{"location":"ReleaseInfo/","text":"Release-Info Automation Toolkit Release v10 & Docker Image Release v6.0 Date - Jan 13th, 2023 This is a major release with below updates: Support for new services - OKE and SCH. New tab for 'OKE' is included in CD3-CIS-template.xlsx. New tab for 'ServiceConnectors' is added to CD3-CIS-ManagementServices-template.xlsx. Added the script to fetch regions subscribed to the tenancy. This will be executed automatically every time setUpOCI is executed. Introduced a new option in setUpOCI menu called 'CD3 Services' to execute 'Fetch Compartments' and 'Fetch Protocols Scripts. Modified the setUpOCI workflow to prompt the user to execute 'Fetch Compartments' script in case it has not been executed previously. Bug fixes wrt Instances, DB Systems Automation Toolkit Release v9.2.1 & Docker Image Release v5.2.1 Date - Nov 30th, 2022 This is a minor release with below updates: Sample terraform import command included as part of output tfvars file for each OCI resource managed by Toolkit. Split Export of Network options to chose export of different components separately eg Major Objects, Subnets, NSGs etc. Include support for Marketplace Images for Instances. Few bug fixes/enhancements wrt export of Instances/NSGs, making null values for NSGs/Instances optional in tfvars Previous Versions released before making it available on GitHub Automation Toolkit Release v9.2 & Docker Image Release v5.2 Date - Oct 10th, 2022 This is a major release with existing services converted into terraform modules and also bug fixes. Terraform modules for FSS and ADB(Modified the input sheet for ADB to include more features) Included new service - Network Load Balancers Enhanced Tags sheet to include Default Tags for Multiple Compartments. Enhanced CD3 Validator for FSS, NSGs column for each tab. Included CIS compliance checker script as part of setUpOCI Menu option Cleanup of variables_ .tf file Updated CD3 templates in example folder with latest CIS data Bug fixes wrt multiple services like Instances, Notifications etc Introduced documentation folder containing terraform and toolkit user guide in outdir of each customer Automation Toolkit Release v9.1 & Docker Image Release v5.1 Date - Jun 13th, 2022 This is a major release with existing services converted into terraform modules and also bug fixes. Terraform Modules for Instances, Block Volumes, Tags, CIS features, LBaas. Introduced new columns for PV encryption in Instances/Block Volumes. Introduced new columns for Min/Max bandwidth for flexible shapes load balancers, reserved public IP. Support for OCI certificate management certificates for Listeners, BackendSets. Removed support for subnet_name_attach_cidr parameter from CD3 excel's 'VCN Info' sheet. Bug fixes Automation Toolkit Release v9.0.2 & Docker Image Release v5.0.2 Date - April 29, 2022 This is a minor release with bug fixes related to Networking and Identity Policies. Automation Toolkit Release v9.0.1 & Docker Image Release v5.0.1 Date - April 9, 2022 This is a minor release. Bug fix for data after END tag in DRGRouteRulesinOCI sheet. Bug fix for tenancies having both DRGv1 and DRGv2 Bug fix for VCN Flow Log output Bug fix for NSG ICMP rules not having only ICMP type Modified output template files to adjust the spacing Automation Toolkit Release v9.0 & Docker Image Release v5.0 Date - March 21, 2022 This is a major release. Terraform output files in modules format for IAM, Network, Database(DBSystems and Exa) and Management Services(Events, Notifications) components. Added support for Alarms with output files as terraform modules. Updated the excel sheet templates as per latest CIS compliance for IAM compartments, groups, policies, events, notifications and alarms. Added support for multiple VCN CIDRs in VCNs sheet. Added support for same VCN names across regions. Added export for Dedicated VM Hosts and Database tabs as terraform flat files Modified Database tabs to include new features. Restructured the code directories and setUpOCI menu options. Added drop downs in the excel sheet columns to make excel filling easier. Updated OCI_regions to include all regions Deprecation of OCSWorkVM. Automation Toolkit Release v8.0.3 & Docker Image Release v4.0.3 Date - Nov 26 , 2021 This is a minor release. Bug fix for Availability Domain values while export of block volumes, FSS Bug fix for DRG Route Rules export error - 'Too Many requests' Bug fix for export of Tags having spaces in the values Automation Toolkit Release v8.0.2 & Docker Image Release v4.0.2 Date - Sep 30 , 2021 This is a minor release. Bug fix for End Tag in NSGs tab Bug fix for export of LBR hosted in two subnets in different compartments. Bug fix for terraform variable name for route table names Allow case insensitive for Security Rule Types/Protocols Allow flexible shapes in LBR CD3 Automation Toolkit Release v8.0 & Docker Image Release v4.0 Date - Jul 27 , 2021 This is a major release. Support for DRGv2 - added 2 new sheets to CD3: DRGs and DRGRouteRulesinOCI Backward compatible to support DRGv1 Updated CD3 Validator to include validation for DRGs tab Optimized the code for export of objects Bug fix for export of SecRules and RouteRules for all compartments Updated the toolkit to support user access to a single compartment by allowing creation of sub-compartments under that only. Included export of additional objects for Instances and Block Volumes CD3 templates are compliant to latest CIS Landing Zone Upgrade terraform version to 1.0.0 Please note that since this release is upgrading the terraform version so previous version's terraform state will not be compatible with the new tf files. Recommendation is to keep using old code setup for eixsting customers and new code setup for new customers. Or else export everything using new code and then import into the new terraform state Automation Toolkit Release v7.2 & Docker Image Release v3.2 Date - May 26, 2021 This is a minor release. Bug fix for export of policies having newline in description eg for policies added automatically for streaming Launch Linux 7.9 as OCS VM since 7.8 is not searcheable now and enable yum repos Change os.cmd to function calls for SetUPOCI Include CIS Features to enable cloud-guard, OSS, VCN Flow Logging CD3 Validator for Identity, Networking, Instances, Block Volumes Accept key value along with key var name in Instances Sheet Updated CD3 Templates as per CIS Release v7.1.2 Date - Apr 12, 2021 This is a minor release. Bug fix for CD3 LBR - Certs/PEM keys copied to outdir- this helps to resolve the path issue while RM upload. Export LBR also modified Bug fix for Instance Export - included root compartment, AD issue and Boot Volume not found issue. Removed installation of Development Tools from shell script on OCSVM. Added CD3 validation for DNS Label length for VCN and Subnets Release v7.1.1 This is a minor release. Bug fix for CD3 Network components validator - included check for invalid CIDR range having host bits set Bug fix for Security List Rules - allow all ports for TCP/UDP Removed unwanted packages from shell script - cfgparse, ipaddr, pycrypto, gcc Release v7.1 Date - Feb 12, 2021 Below are the highlights of this release: Introduced new option in setUpOCI Menu to create RM Stack * When this option is chosen, it will ask for the compartment where the RM stack has to be created. * It will create stacks in the specified compartment in the home region * RM Stack names: ocswork- - where is the prefix mentioned in setUpOCI.properties file and is the regions tenancy is subscribed to. * Uploads all files in outdir to RM Stack and also uploads the tfstate file if existing. * Uses same RM Stack for multiple executions. Bug fix for LBR Backend sets to allow same backend servers with different ports. OCSWork VM launched with TF version 0.13.4 in sync with Resource Manager in OCI CD3 templates as per CIS Standards Release v7.0.1 This is a minor release. xrld package's latest release was not compatible and giving below issue with CD3: It has been fixed by installing lower version of xlrd package. pip3 install xlrd==1.2.0 It has been corrected and pushed to the master branch. Major Release v7.0 Date - Oct 9, 2020 Support for additional properties for OCI objects using Jinja2 templates Support for configuring Events and Notifications Support for export of Instances, block volumes, Tags, Events and Notifications etc to CD3 Updated Terraform Configuration Files to support the Latest Terraform Version Support for Resource Manager Support to create and export Dynamic Groups Support to create and export Cost Tracking and Default Tags Support to create and export LBR Components- Cipher Suites, Rule Set and Path Route Set Support to attach or export 'Custom Backup Policy Attachments' to Block and Boot Volumes Release v6.1.1 creatOCSWork.py picks up latest Linux OCID and launches the OCSWork VM. However Linux 8 does not support many packages required by automation toolkit. Hence modified the code to launch Linux 7.8 incase ocs_vm_source_image_ocid is left empty in ocswork.properties. Release v6.1 Date - July 31, 2020 Below are the highlights of this release: Addition of Description field for Security Rules and Route Rules. Keep using same CD3 but just add new column 'RuleDescription' at the end of both the sheets - SecRulesinOCI and RouteRulesinOCI New text files - OCI_Regions and OCI_Protocols have been introduced. If any new region gets supported by OCI, it can be added in OCI_Regions file. Similarly Protocol and its number mapping has been defined in OCI_Protocols file which will be used by Security Rules as well as NSG Rules. Automation Toolkit will now support duplicate compartment names like OCI does. Please refer to CD3-template.xlsx under example folder for sample data. Extra properties specific to OCItoOCI project have been removed from ocswork.properties and a new file ocswork_ocic.properties has been added to accomodate that. Support for multiple OCSWork VMs via separate config_for_delete files. A new input parameter has been added to ocswork.properties file. Support for Reserved Public IP for OCSWork VM. Reserved public IP will be assigned to OCSWork VM which you cna chose to retain also while destroying OCSWork VM. a. Regions property has been removed and now tenancy's subscribed regions will be fetched using API and terraform directories would be created based on that. It would be good to subscribe tenancy to all required regions before setting up OCS Work VM or else create the region directory manually. b. Regions property has been removed from VCN Info tab of CD3 as well. CD3 Validator has been introduced. This will validate Networking tabs as of now. It will check for any Null Values, CIDR overlaps etc. Support for Flex shapes for Instances. Please refer to CD3-template.xlsx under example folder for sample input data. Instead of LinuxLatest and WindowsLatest, OS value in CD3 template has been changed to Linux and Windows respectively. Variable names in variables_ .tf has been changed accordingly. Support for LinuxLatest and WindowsLatest templates for instance will be deprecated from next release. Support for Multiple Listeners for LBRs. Add new column 'ListenerName' in the CD3 sheet. Please refer to CD3-template.xlsx under example folder for sample input data. Support for Multiple export options for FSS. Please refer to CD3-template.xlsx under example folder for sample input data. Support for NSG export/import. Export process for non-greenfield tenancies will support export of NSGs and their import into terraform. GIT repo has been moved to OCI. SSH key needs to be setup for access to the repo. Private key is copied over to /root/.ssh folder. It is up to the developers if they want to keep the private key there for any future GIT updates or if they want to remove the key for security reasons. Release v6.0.1 Date - Mar 23, 2020 There was a bug in v6.0 where TF for Instances, FSS, LBR etc was not getting correct subnet name as created for subnets using NEtworking. It has been corrected and pushed to the master branch. Major Release v6.0 Below are the highlights of this release: Support for Non Green Field Tenancies. Removed common_seclist_name and seclist_per_subnet columns from Subnets Tab. Specify security lists to be created for a subnet as comma separated in seclist_names column. If DNS doesn't need to be enabled for a VCN or subnet, specify 'n' in dns_label column for that VCN or Subnet. Specify 'n' for route_table_name or seclist_names in Subnets tab if only Default Route Table or Default Secuirty List of VCN needs to be attached to the subnet. When you are running Modify Network, if there are any route tables or security lists which are not attached to any subnet or DRG or LPG then it will display the names in output like below: ATTENTION!!! Below RouteTables are not attached to any subnet or DRG and LPG; If you want to delete any of them, remove the TF file!!! ATTENTION!!! Below SecLists are not attached to any subnet; If you want to delete any of them, remove the TF file!!! Release v5.0.1 There was a bug in LPGs creation/peering. Corrected that and pushed as v5.0.1 Major Release v5.0 Date - Feb 7, 2020 It has many new enhancements and features added to it. Detailed explanation about CD3 excel is at: CD3 Excel release v5.0 Below are the highlights: yum utility won't break on OCS Work VM. Please use python setUpOCI.py cmd and python fetch_compartments_to_variablesTF.py cmd to execute automation. Export of rules after network creation is a mandatory step. Single sheet- 'SecRulesinOCI' and 'RouteRulesinOCI' would be used to manage rules in OCI. Color Coding has been added to the exported rules. Support for specifying LPG names has been included. Format to specify LPG names: specify either y (like for other components) - this will give default name to the LPG whch is _lpg eg ProdVCN_lpg0 or the name that you want to give to the LPG. Peering section has been removed and merged in VCNs tab. Format for specifying peering in hub_spoke_peer_none column: specify hub in the column for the VCN you want to mark as hub specify spoke: if you want to mark a VCN as spoke to hub VCN specify peer: if you want to establish normal peering between 2 VCNs specify none if the VCN is a normal standalone VCN you can specify SGW target for route rules in OCI. specify as either object_storage or all_services When VCNs are specified in hub-spoke model, Route Tables associated with DRG and LPG get created automatically. Inter subnet communication and egress communication from all subnets is opened via SecRules. Initial Subnet Route rules are controlled by flags for each target in Subnets Tab. If any change is required to be done in default sec rules or route rules, they can be modified via cd3 after exporting them. CSV support for this version is still under progress. Major Release v4.0 Note This version would require you to change your excel file and use the latest one since there is a column addition in the existing sheet. Added support to add a common security List across subnets apart from just Default Security List. This is done by adding a new column in Subnets tab \"common_seclist_name\" which specifies name of common seclist to be created and used for each subnet If left blank for a particular subnet that means the common seclist doesnt not need to be assigned to that subnet. Modified output files created for routes. Earlier tool used to create one TF file for all route tables. Now it would generate separate file for each Route Table like it does for Security List. Introduced option to create new VCN under Update Network Added Default DHCP options also to TF like for Default Security List Added support to include Description for a rule in NSGs. Added new column for this Modified DB Systems creation code. Separated tabs for DB system - VM, BM and Exa Fixed some minor issues with existing code","title":"ReleaseInfo"},{"location":"ReleaseInfo/#release-info","text":"","title":"Release-Info"},{"location":"ReleaseInfo/#automation-toolkit-release-v10-docker-image-release-v60","text":"","title":"Automation Toolkit Release v10 &amp; Docker Image Release v6.0"},{"location":"ReleaseInfo/#date-jan-13th-2023","text":"This is a major release with below updates: Support for new services - OKE and SCH. New tab for 'OKE' is included in CD3-CIS-template.xlsx. New tab for 'ServiceConnectors' is added to CD3-CIS-ManagementServices-template.xlsx. Added the script to fetch regions subscribed to the tenancy. This will be executed automatically every time setUpOCI is executed. Introduced a new option in setUpOCI menu called 'CD3 Services' to execute 'Fetch Compartments' and 'Fetch Protocols Scripts. Modified the setUpOCI workflow to prompt the user to execute 'Fetch Compartments' script in case it has not been executed previously. Bug fixes wrt Instances, DB Systems","title":"Date - Jan 13th, 2023"},{"location":"ReleaseInfo/#automation-toolkit-release-v921-docker-image-release-v521","text":"","title":"Automation Toolkit Release v9.2.1 &amp; Docker Image Release v5.2.1"},{"location":"ReleaseInfo/#date-nov-30th-2022","text":"This is a minor release with below updates: Sample terraform import command included as part of output tfvars file for each OCI resource managed by Toolkit. Split Export of Network options to chose export of different components separately eg Major Objects, Subnets, NSGs etc. Include support for Marketplace Images for Instances. Few bug fixes/enhancements wrt export of Instances/NSGs, making null values for NSGs/Instances optional in tfvars","title":"Date - Nov 30th, 2022"},{"location":"ReleaseInfo/#previous-versions-released-before-making-it-available-on-github","text":"","title":"Previous Versions released before making it available on GitHub"},{"location":"ReleaseInfo/#automation-toolkit-release-v92-docker-image-release-v52","text":"","title":"Automation Toolkit Release v9.2 &amp; Docker Image Release v5.2"},{"location":"ReleaseInfo/#date-oct-10th-2022","text":"This is a major release with existing services converted into terraform modules and also bug fixes. Terraform modules for FSS and ADB(Modified the input sheet for ADB to include more features) Included new service - Network Load Balancers Enhanced Tags sheet to include Default Tags for Multiple Compartments. Enhanced CD3 Validator for FSS, NSGs column for each tab. Included CIS compliance checker script as part of setUpOCI Menu option Cleanup of variables_ .tf file Updated CD3 templates in example folder with latest CIS data Bug fixes wrt multiple services like Instances, Notifications etc Introduced documentation folder containing terraform and toolkit user guide in outdir of each customer","title":"Date - Oct 10th, 2022"},{"location":"ReleaseInfo/#automation-toolkit-release-v91-docker-image-release-v51","text":"","title":"Automation Toolkit Release v9.1 &amp; Docker Image Release v5.1"},{"location":"ReleaseInfo/#date-jun-13th-2022","text":"This is a major release with existing services converted into terraform modules and also bug fixes. Terraform Modules for Instances, Block Volumes, Tags, CIS features, LBaas. Introduced new columns for PV encryption in Instances/Block Volumes. Introduced new columns for Min/Max bandwidth for flexible shapes load balancers, reserved public IP. Support for OCI certificate management certificates for Listeners, BackendSets. Removed support for subnet_name_attach_cidr parameter from CD3 excel's 'VCN Info' sheet. Bug fixes","title":"Date - Jun 13th, 2022"},{"location":"ReleaseInfo/#automation-toolkit-release-v902-docker-image-release-v502","text":"","title":"Automation Toolkit Release v9.0.2 &amp; Docker Image Release v5.0.2"},{"location":"ReleaseInfo/#date-april-29-2022","text":"This is a minor release with bug fixes related to Networking and Identity Policies.","title":"Date - April 29, 2022"},{"location":"ReleaseInfo/#automation-toolkit-release-v901-docker-image-release-v501","text":"","title":"Automation Toolkit Release v9.0.1 &amp; Docker Image Release v5.0.1"},{"location":"ReleaseInfo/#date-april-9-2022","text":"This is a minor release. Bug fix for data after END tag in DRGRouteRulesinOCI sheet. Bug fix for tenancies having both DRGv1 and DRGv2 Bug fix for VCN Flow Log output Bug fix for NSG ICMP rules not having only ICMP type Modified output template files to adjust the spacing","title":"Date - April 9, 2022"},{"location":"ReleaseInfo/#automation-toolkit-release-v90-docker-image-release-v50","text":"","title":"Automation Toolkit Release v9.0 &amp; Docker Image Release v5.0"},{"location":"ReleaseInfo/#date-march-21-2022","text":"This is a major release. Terraform output files in modules format for IAM, Network, Database(DBSystems and Exa) and Management Services(Events, Notifications) components. Added support for Alarms with output files as terraform modules. Updated the excel sheet templates as per latest CIS compliance for IAM compartments, groups, policies, events, notifications and alarms. Added support for multiple VCN CIDRs in VCNs sheet. Added support for same VCN names across regions. Added export for Dedicated VM Hosts and Database tabs as terraform flat files Modified Database tabs to include new features. Restructured the code directories and setUpOCI menu options. Added drop downs in the excel sheet columns to make excel filling easier. Updated OCI_regions to include all regions Deprecation of OCSWorkVM.","title":"Date - March 21, 2022"},{"location":"ReleaseInfo/#automation-toolkit-release-v803-docker-image-release-v403","text":"","title":"Automation Toolkit Release v8.0.3 &amp; Docker Image Release v4.0.3"},{"location":"ReleaseInfo/#date-nov-26-2021","text":"This is a minor release. Bug fix for Availability Domain values while export of block volumes, FSS Bug fix for DRG Route Rules export error - 'Too Many requests' Bug fix for export of Tags having spaces in the values","title":"Date - Nov 26 , 2021"},{"location":"ReleaseInfo/#automation-toolkit-release-v802-docker-image-release-v402","text":"","title":"Automation Toolkit Release v8.0.2 &amp; Docker Image Release v4.0.2"},{"location":"ReleaseInfo/#date-sep-30-2021","text":"This is a minor release. Bug fix for End Tag in NSGs tab Bug fix for export of LBR hosted in two subnets in different compartments. Bug fix for terraform variable name for route table names Allow case insensitive for Security Rule Types/Protocols Allow flexible shapes in LBR CD3","title":"Date - Sep 30 , 2021"},{"location":"ReleaseInfo/#automation-toolkit-release-v80-docker-image-release-v40","text":"","title":"Automation Toolkit Release v8.0 &amp; Docker Image Release v4.0"},{"location":"ReleaseInfo/#date-jul-27-2021","text":"This is a major release. Support for DRGv2 - added 2 new sheets to CD3: DRGs and DRGRouteRulesinOCI Backward compatible to support DRGv1 Updated CD3 Validator to include validation for DRGs tab Optimized the code for export of objects Bug fix for export of SecRules and RouteRules for all compartments Updated the toolkit to support user access to a single compartment by allowing creation of sub-compartments under that only. Included export of additional objects for Instances and Block Volumes CD3 templates are compliant to latest CIS Landing Zone Upgrade terraform version to 1.0.0 Please note that since this release is upgrading the terraform version so previous version's terraform state will not be compatible with the new tf files. Recommendation is to keep using old code setup for eixsting customers and new code setup for new customers. Or else export everything using new code and then import into the new terraform state","title":"Date - Jul 27 , 2021"},{"location":"ReleaseInfo/#automation-toolkit-release-v72-docker-image-release-v32","text":"","title":"Automation Toolkit Release v7.2 &amp; Docker Image Release v3.2"},{"location":"ReleaseInfo/#date-may-26-2021","text":"This is a minor release. Bug fix for export of policies having newline in description eg for policies added automatically for streaming Launch Linux 7.9 as OCS VM since 7.8 is not searcheable now and enable yum repos Change os.cmd to function calls for SetUPOCI Include CIS Features to enable cloud-guard, OSS, VCN Flow Logging CD3 Validator for Identity, Networking, Instances, Block Volumes Accept key value along with key var name in Instances Sheet Updated CD3 Templates as per CIS","title":"Date - May 26, 2021"},{"location":"ReleaseInfo/#release-v712","text":"","title":"Release v7.1.2"},{"location":"ReleaseInfo/#date-apr-12-2021","text":"This is a minor release. Bug fix for CD3 LBR - Certs/PEM keys copied to outdir- this helps to resolve the path issue while RM upload. Export LBR also modified Bug fix for Instance Export - included root compartment, AD issue and Boot Volume not found issue. Removed installation of Development Tools from shell script on OCSVM. Added CD3 validation for DNS Label length for VCN and Subnets","title":"Date - Apr 12, 2021"},{"location":"ReleaseInfo/#release-v711","text":"","title":"Release v7.1.1"},{"location":"ReleaseInfo/#this-is-a-minor-release","text":"Bug fix for CD3 Network components validator - included check for invalid CIDR range having host bits set Bug fix for Security List Rules - allow all ports for TCP/UDP Removed unwanted packages from shell script - cfgparse, ipaddr, pycrypto, gcc","title":"This is a minor release."},{"location":"ReleaseInfo/#release-v71","text":"Date - Feb 12, 2021 Below are the highlights of this release: Introduced new option in setUpOCI Menu to create RM Stack * When this option is chosen, it will ask for the compartment where the RM stack has to be created. * It will create stacks in the specified compartment in the home region * RM Stack names: ocswork- - where is the prefix mentioned in setUpOCI.properties file and is the regions tenancy is subscribed to. * Uploads all files in outdir to RM Stack and also uploads the tfstate file if existing. * Uses same RM Stack for multiple executions. Bug fix for LBR Backend sets to allow same backend servers with different ports. OCSWork VM launched with TF version 0.13.4 in sync with Resource Manager in OCI CD3 templates as per CIS Standards","title":"Release v7.1"},{"location":"ReleaseInfo/#release-v701","text":"This is a minor release. xrld package's latest release was not compatible and giving below issue with CD3: It has been fixed by installing lower version of xlrd package. pip3 install xlrd==1.2.0 It has been corrected and pushed to the master branch.","title":"Release v7.0.1"},{"location":"ReleaseInfo/#major-release-v70","text":"","title":"Major Release v7.0"},{"location":"ReleaseInfo/#date-oct-9-2020","text":"Support for additional properties for OCI objects using Jinja2 templates Support for configuring Events and Notifications Support for export of Instances, block volumes, Tags, Events and Notifications etc to CD3 Updated Terraform Configuration Files to support the Latest Terraform Version Support for Resource Manager Support to create and export Dynamic Groups Support to create and export Cost Tracking and Default Tags Support to create and export LBR Components- Cipher Suites, Rule Set and Path Route Set Support to attach or export 'Custom Backup Policy Attachments' to Block and Boot Volumes","title":"Date - Oct 9, 2020"},{"location":"ReleaseInfo/#release-v611","text":"creatOCSWork.py picks up latest Linux OCID and launches the OCSWork VM. However Linux 8 does not support many packages required by automation toolkit. Hence modified the code to launch Linux 7.8 incase ocs_vm_source_image_ocid is left empty in ocswork.properties.","title":"Release v6.1.1"},{"location":"ReleaseInfo/#release-v61","text":"","title":"Release v6.1"},{"location":"ReleaseInfo/#date-july-31-2020","text":"Below are the highlights of this release: Addition of Description field for Security Rules and Route Rules. Keep using same CD3 but just add new column 'RuleDescription' at the end of both the sheets - SecRulesinOCI and RouteRulesinOCI New text files - OCI_Regions and OCI_Protocols have been introduced. If any new region gets supported by OCI, it can be added in OCI_Regions file. Similarly Protocol and its number mapping has been defined in OCI_Protocols file which will be used by Security Rules as well as NSG Rules. Automation Toolkit will now support duplicate compartment names like OCI does. Please refer to CD3-template.xlsx under example folder for sample data. Extra properties specific to OCItoOCI project have been removed from ocswork.properties and a new file ocswork_ocic.properties has been added to accomodate that. Support for multiple OCSWork VMs via separate config_for_delete files. A new input parameter has been added to ocswork.properties file. Support for Reserved Public IP for OCSWork VM. Reserved public IP will be assigned to OCSWork VM which you cna chose to retain also while destroying OCSWork VM. a. Regions property has been removed and now tenancy's subscribed regions will be fetched using API and terraform directories would be created based on that. It would be good to subscribe tenancy to all required regions before setting up OCS Work VM or else create the region directory manually. b. Regions property has been removed from VCN Info tab of CD3 as well. CD3 Validator has been introduced. This will validate Networking tabs as of now. It will check for any Null Values, CIDR overlaps etc. Support for Flex shapes for Instances. Please refer to CD3-template.xlsx under example folder for sample input data. Instead of LinuxLatest and WindowsLatest, OS value in CD3 template has been changed to Linux and Windows respectively. Variable names in variables_ .tf has been changed accordingly. Support for LinuxLatest and WindowsLatest templates for instance will be deprecated from next release. Support for Multiple Listeners for LBRs. Add new column 'ListenerName' in the CD3 sheet. Please refer to CD3-template.xlsx under example folder for sample input data. Support for Multiple export options for FSS. Please refer to CD3-template.xlsx under example folder for sample input data. Support for NSG export/import. Export process for non-greenfield tenancies will support export of NSGs and their import into terraform. GIT repo has been moved to OCI. SSH key needs to be setup for access to the repo. Private key is copied over to /root/.ssh folder. It is up to the developers if they want to keep the private key there for any future GIT updates or if they want to remove the key for security reasons.","title":"Date - July 31, 2020"},{"location":"ReleaseInfo/#release-v601","text":"","title":"Release v6.0.1"},{"location":"ReleaseInfo/#date-mar-23-2020","text":"There was a bug in v6.0 where TF for Instances, FSS, LBR etc was not getting correct subnet name as created for subnets using NEtworking. It has been corrected and pushed to the master branch.","title":"Date - Mar 23, 2020"},{"location":"ReleaseInfo/#major-release-v60","text":"Below are the highlights of this release: Support for Non Green Field Tenancies. Removed common_seclist_name and seclist_per_subnet columns from Subnets Tab. Specify security lists to be created for a subnet as comma separated in seclist_names column. If DNS doesn't need to be enabled for a VCN or subnet, specify 'n' in dns_label column for that VCN or Subnet. Specify 'n' for route_table_name or seclist_names in Subnets tab if only Default Route Table or Default Secuirty List of VCN needs to be attached to the subnet. When you are running Modify Network, if there are any route tables or security lists which are not attached to any subnet or DRG or LPG then it will display the names in output like below: ATTENTION!!! Below RouteTables are not attached to any subnet or DRG and LPG; If you want to delete any of them, remove the TF file!!! ATTENTION!!! Below SecLists are not attached to any subnet; If you want to delete any of them, remove the TF file!!!","title":"Major Release v6.0"},{"location":"ReleaseInfo/#release-v501","text":"There was a bug in LPGs creation/peering. Corrected that and pushed as v5.0.1","title":"Release v5.0.1"},{"location":"ReleaseInfo/#major-release-v50","text":"","title":"Major Release v5.0"},{"location":"ReleaseInfo/#date-feb-7-2020","text":"It has many new enhancements and features added to it. Detailed explanation about CD3 excel is at: CD3 Excel release v5.0 Below are the highlights: yum utility won't break on OCS Work VM. Please use python setUpOCI.py cmd and python fetch_compartments_to_variablesTF.py cmd to execute automation. Export of rules after network creation is a mandatory step. Single sheet- 'SecRulesinOCI' and 'RouteRulesinOCI' would be used to manage rules in OCI. Color Coding has been added to the exported rules. Support for specifying LPG names has been included. Format to specify LPG names: specify either y (like for other components) - this will give default name to the LPG whch is _lpg eg ProdVCN_lpg0 or the name that you want to give to the LPG. Peering section has been removed and merged in VCNs tab. Format for specifying peering in hub_spoke_peer_none column: specify hub in the column for the VCN you want to mark as hub specify spoke: if you want to mark a VCN as spoke to hub VCN specify peer: if you want to establish normal peering between 2 VCNs specify none if the VCN is a normal standalone VCN you can specify SGW target for route rules in OCI. specify as either object_storage or all_services When VCNs are specified in hub-spoke model, Route Tables associated with DRG and LPG get created automatically. Inter subnet communication and egress communication from all subnets is opened via SecRules. Initial Subnet Route rules are controlled by flags for each target in Subnets Tab. If any change is required to be done in default sec rules or route rules, they can be modified via cd3 after exporting them. CSV support for this version is still under progress.","title":"Date - Feb 7, 2020"},{"location":"ReleaseInfo/#major-release-v40","text":"Note This version would require you to change your excel file and use the latest one since there is a column addition in the existing sheet. Added support to add a common security List across subnets apart from just Default Security List. This is done by adding a new column in Subnets tab \"common_seclist_name\" which specifies name of common seclist to be created and used for each subnet If left blank for a particular subnet that means the common seclist doesnt not need to be assigned to that subnet. Modified output files created for routes. Earlier tool used to create one TF file for all route tables. Now it would generate separate file for each Route Table like it does for Security List. Introduced option to create new VCN under Update Network Added Default DHCP options also to TF like for Default Security List Added support to include Description for a rule in NSGs. Added new column for this Modified DB Systems creation code. Separated tabs for DB system - VM, BM and Exa Fixed some minor issues with existing code","title":"Major Release v4.0"},{"location":"ResourceManagerUpload/","text":"OCI Resource Manager Upload This option will upload the created Terraform files & the tfstate (if present) to the OCI Resource Manager. When prompted, specify the Region to create/upload the terraform files to Resource Manager Stack. Multiple regions can be specified as comma separated values. Specify 'global' to upload RPC related components which reside in 'global' directory. On the next prompt, enter the Compartment where the Stack should be created if it is for the first time. The toolkit will create a Stack for the region specified previously under the specified compartment. For global resources, stack will be created in the home region. The Stack created will use Terraform 1.0.x. The upload includes terraform.tfstate file as well, if present. This is to sync the OCI Resource Manager Stack to that of your outdir. The toolkit also creates a rm_ocids.csv file in the outdir/ which has the information on the Resource Manager stack that is created. The format of the data in rm_ocids.csv is as follows - Example: To use an existing Resource Manager stack, enter the details in the format provided above into your _outdir/<region_dir>/rm_ocids.csv_ file. Sample Execution: [!IMPORTANT] If you are using remote state and upload the stack to OCI Resource Manager using Upload current terraform files/state to Resource Manager under Developer Services , then running terraform plan/apply from OCI Resource Manager will not work and show below error: You will have to remove backend.tf from the directory, bring the remote state into local and then re-upload the stack. On choosing \"Developer Services\" in the SetUpOCI menu, choose \"Upload current terraform files/state to Resource Manager\" sub-option to upload the terraform outdir into OCI Resource Manager.","title":"OCI Resource Manager Upload"},{"location":"ResourceManagerUpload/#oci-resource-manager-upload","text":"This option will upload the created Terraform files & the tfstate (if present) to the OCI Resource Manager. When prompted, specify the Region to create/upload the terraform files to Resource Manager Stack. Multiple regions can be specified as comma separated values. Specify 'global' to upload RPC related components which reside in 'global' directory. On the next prompt, enter the Compartment where the Stack should be created if it is for the first time. The toolkit will create a Stack for the region specified previously under the specified compartment. For global resources, stack will be created in the home region. The Stack created will use Terraform 1.0.x. The upload includes terraform.tfstate file as well, if present. This is to sync the OCI Resource Manager Stack to that of your outdir. The toolkit also creates a rm_ocids.csv file in the outdir/ which has the information on the Resource Manager stack that is created. The format of the data in rm_ocids.csv is as follows - Example: To use an existing Resource Manager stack, enter the details in the format provided above into your _outdir/<region_dir>/rm_ocids.csv_ file. Sample Execution: [!IMPORTANT] If you are using remote state and upload the stack to OCI Resource Manager using Upload current terraform files/state to Resource Manager under Developer Services , then running terraform plan/apply from OCI Resource Manager will not work and show below error: You will have to remove backend.tf from the directory, bring the remote state into local and then re-upload the stack. On choosing \"Developer Services\" in the SetUpOCI menu, choose \"Upload current terraform files/state to Resource Manager\" sub-option to upload the terraform outdir into OCI Resource Manager.","title":"OCI Resource Manager Upload"},{"location":"RestructuringOutDirectory/","text":"Grouping of generated Terraform files (Creating independent tfstate files for each resource) The CD3 Automation Toolkit was previously built to generate all the output Terraform files within a designated region directory. OCI components like - Network, Instances, LBaaS, Databases etc., were maintained in a single tfstate file. This was not a viable option for tenancies requiring huge infrastructure. Starting with the Automation Toolkit release v10.1, it is now possible to select separate directories for each Oracle Cloud Infrastructure (OCI) service supported by the toolkit. This can be configured while connecting your container to the OCI tenancy . A new parameter 'outdir_structure_file' has been introduced in tenancyconfig.properties using which can be used to configure single outdir or different outdir for each service. To enable independent service directories for the generated Terraform files, follow the below steps: Go to /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/tenancyconfig.properties . Enter required config details. Users have the option to enable or disable multiple service outdirectories based on their specific requirements. To enable it, uncomment the outdir_structure_file parameter which has the pre-defined path to outdir_structure_file.properties . Refer to the screenshot below: 2. Under the same user-scripts folder, open outdir_structure_file.properties and modify the directory names if required. They are in the format: OCI_Service_Name=Directory_Name . Note: * Do not modify the OCI service Names specified on the left hand side.Modify the directory name specified on Right Hand Side * Directory will be created for that service under directory. Do not provide absolute path. * To make any changes to the directory structure later, it is necessary to rerun the \"createTenancy.py\" script from scratch. * It is mandatory to specify the directory name for each service. Here, the network and nsg directories have been renamed to demo_network and demo_nsg respectively. The next steps to run the toolkit remain the same as specified in Greenfield workflow Run python createTenancyConfig.py tenancyconfig.properties from user-scripts folder. Go to /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.propertiesfile and add the CD3 Excel path. Change to the below directory cd /cd3user/oci_tools/cd3_automation_toolkit/ Run the script python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Select required options. (Here,\"Network\", \"nsg\" options have been selected to verify the files under the \"demo_network\", \"demo_nsg\" folders) auto.tfvars for the respective services are created. Go to the region directory /cd3user/tenancies/<customer_name>/terraform_files/<region>/ . It is clear that all the service-specific folders have been created successfully. Navigate to the demo-network folder. All the auto.tfvars and tfstate files related to Network service can be seen within the demo_network folder. Terraform operations like terraform init, terraform plan, terraform apply etc., will be executed from within these folders. Similarly for all the services, their respective auto.tfvars and tfstate files get grouped under their assigned directories. This makes it much easier to manage OCI resources using terraform for large-scale infrastructures. Likewise, While doing an export from OCI to terraform, update the tenancyconfig.properties file with path to outdir_structure_file.properties similar to step1 and then follow the steps to run the toolkit for Non-green field tenancies . With this, all the .sh files with import commands of a particular OCI service are grouped and can be easily managed.","title":"Grouping generated Terraform files"},{"location":"RestructuringOutDirectory/#grouping-of-generated-terraform-files-creating-independent-tfstate-files-for-each-resource","text":"The CD3 Automation Toolkit was previously built to generate all the output Terraform files within a designated region directory. OCI components like - Network, Instances, LBaaS, Databases etc., were maintained in a single tfstate file. This was not a viable option for tenancies requiring huge infrastructure. Starting with the Automation Toolkit release v10.1, it is now possible to select separate directories for each Oracle Cloud Infrastructure (OCI) service supported by the toolkit. This can be configured while connecting your container to the OCI tenancy . A new parameter 'outdir_structure_file' has been introduced in tenancyconfig.properties using which can be used to configure single outdir or different outdir for each service. To enable independent service directories for the generated Terraform files, follow the below steps: Go to /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/tenancyconfig.properties . Enter required config details. Users have the option to enable or disable multiple service outdirectories based on their specific requirements. To enable it, uncomment the outdir_structure_file parameter which has the pre-defined path to outdir_structure_file.properties . Refer to the screenshot below: 2. Under the same user-scripts folder, open outdir_structure_file.properties and modify the directory names if required. They are in the format: OCI_Service_Name=Directory_Name . Note: * Do not modify the OCI service Names specified on the left hand side.Modify the directory name specified on Right Hand Side * Directory will be created for that service under directory. Do not provide absolute path. * To make any changes to the directory structure later, it is necessary to rerun the \"createTenancy.py\" script from scratch. * It is mandatory to specify the directory name for each service. Here, the network and nsg directories have been renamed to demo_network and demo_nsg respectively. The next steps to run the toolkit remain the same as specified in Greenfield workflow Run python createTenancyConfig.py tenancyconfig.properties from user-scripts folder. Go to /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.propertiesfile and add the CD3 Excel path. Change to the below directory cd /cd3user/oci_tools/cd3_automation_toolkit/ Run the script python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties Select required options. (Here,\"Network\", \"nsg\" options have been selected to verify the files under the \"demo_network\", \"demo_nsg\" folders) auto.tfvars for the respective services are created. Go to the region directory /cd3user/tenancies/<customer_name>/terraform_files/<region>/ . It is clear that all the service-specific folders have been created successfully. Navigate to the demo-network folder. All the auto.tfvars and tfstate files related to Network service can be seen within the demo_network folder. Terraform operations like terraform init, terraform plan, terraform apply etc., will be executed from within these folders. Similarly for all the services, their respective auto.tfvars and tfstate files get grouped under their assigned directories. This makes it much easier to manage OCI resources using terraform for large-scale infrastructures. Likewise, While doing an export from OCI to terraform, update the tenancyconfig.properties file with path to outdir_structure_file.properties similar to step1 and then follow the steps to run the toolkit for Non-green field tenancies . With this, all the .sh files with import commands of a particular OCI service are grouped and can be easily managed.","title":"Grouping of generated Terraform files (Creating independent tfstate files for each resource)"},{"location":"SupportForCD3Validator/","text":"CD3 Validator Features With version v9.0 we have introduced validator for Multiple VCN CIDRs in Networking Tab. CD3 Validator helps you validate the Compartments, Groups, Policies, Network component entries, Instances, Block Volumes, FSS in your CD3 to ensure error free, smooth creation of the resources when Terraform is applied. Below is a list of checks done by the CD3 Validator: | Tab Name | Validation/Checks | |----- | --------------- | | Identity | This covers Compartments, Groups, Policies Tabs. Checks if the Region column contains Home Region Checks for mandatory columns | | VCNs | Checks if the Columns - Region and Compartment have valid entries. Checks if the VCN Names are duplicated in Column - VCN Name for the same region. Validates the VCN CIDRs (Single and Multiple) - Checks for Overlapping/Duplicate addresses. Checks the VCN CIDR ranges for host bits set. Checks if the Column - DNS Label has any special characters. Checks for NULL Values if any in all the Columns that is Required/Mandatory. Notifies the current LPG Peering status for Review (Also indicates the new set of peering that will be established based on the entries in CD3). | | Subnets | Checks if the Columns - Region and Compartment have valid entries. Checks if the VCN Names are part of VCN Tab in Column - VCN Name Checks if the Column - DNS Label has any special characters or any Duplicate Values Validates the Subnet CIDRs - Checks for Overlapping/Duplicate addresses Checks the Subnet CIDR ranges for host bits set. Checks for NULL Values if any in all the Columns that is Required/Mandatory Cross Validates entries in Subnets and DHCP Tabs for Column - DHCP Options Checks if Internet Gateways and Service Gateways are set appropriately. Cross Validates entries in Subnets and VCNsTabs for Column - Subnet CIDR (Checks if the Subnet CIDR belongs to / falls under the VCN CIDR as mentioned in the Subnet's Tab) | | DHCP | Checks if the Columns - Region and Compartment have valid entries Checks if the VCN Names are part of VCN Tab in Column - VCN Name Check if there is value for Customer DNS Column if the entered type is 'CustomDNSServer' Checks for NULL Values if any in all the Columns that is Required/Mandatory | | DRGs | Checks if the Columns - Region and Compartment have valid entries Checks if DRG Name entered is as per mentioned in VCNs tab Checks for the valid format of Attached To column and if it contains VCN Name as mentioned in VCNs Tab. Checks for valid format of column 'Import DRG Route Distribution Statements' Checks that column 'Import DRG Route Distribution Statements' cannot have any value if colum 'Import DRG Route Distribution' is empty. | | Instances | Checks if the Columns - Region and Compartment have valid entries Checks for mandatory columns - Region, Compartment Name, Availability Domain, Display Name, Subnet Name, SSH Key Var Name, Pub Address, Source Details, Shape Checks if Subnet Name specified is valid as per Subnets Tab of CD3 Checks for valid values for columns - Availability Domain, Fault Domain, Source Details, Shape Checks if the NSG names mentioned in NSGs column are part of NSGs tab of the CD3 excel. | | Block Volumes | Checks if the Columns - Region and Compartment have valid entries Checks for mandatory columns Block Name, Availability Domain, Attach Type. Checks for valid values for columns - Availability Domain, Attach Type, Attached to Instance. Checks if AD mentioned in Block Volumes sheet is same as AD mentioned in Instances sheet for the instance to which block volume is to be attached. | | FSS | Checks if the Columns - Region and Compartment have valid entries Checks for mandatory columns - Region, Compartment Name, Availability Domain, MountTarget Name, MountTarget SubnetName. Checks if Subnet Name specified is valid as per Subnets Tab of CD3 Checks if the NSG names mentioned in NSGs column are part of NSGs tab of the CD3 excel. | Log file containing CD3 validator checks is generated at: /cd3user/tenancies/ / _cd3validator.log Expected ERROR in the log file: Compartment Network does not exist in OCI.\u2192 These error mean that the component is not found in OCI. So, please make sure to create the Compartment \"Network\" before validating other tabs.","title":"CD3 Validator"},{"location":"SupportForCD3Validator/#cd3-validator-features","text":"With version v9.0 we have introduced validator for Multiple VCN CIDRs in Networking Tab. CD3 Validator helps you validate the Compartments, Groups, Policies, Network component entries, Instances, Block Volumes, FSS in your CD3 to ensure error free, smooth creation of the resources when Terraform is applied. Below is a list of checks done by the CD3 Validator: | Tab Name | Validation/Checks | |----- | --------------- | | Identity | This covers Compartments, Groups, Policies Tabs. Checks if the Region column contains Home Region Checks for mandatory columns | | VCNs | Checks if the Columns - Region and Compartment have valid entries. Checks if the VCN Names are duplicated in Column - VCN Name for the same region. Validates the VCN CIDRs (Single and Multiple) - Checks for Overlapping/Duplicate addresses. Checks the VCN CIDR ranges for host bits set. Checks if the Column - DNS Label has any special characters. Checks for NULL Values if any in all the Columns that is Required/Mandatory. Notifies the current LPG Peering status for Review (Also indicates the new set of peering that will be established based on the entries in CD3). | | Subnets | Checks if the Columns - Region and Compartment have valid entries. Checks if the VCN Names are part of VCN Tab in Column - VCN Name Checks if the Column - DNS Label has any special characters or any Duplicate Values Validates the Subnet CIDRs - Checks for Overlapping/Duplicate addresses Checks the Subnet CIDR ranges for host bits set. Checks for NULL Values if any in all the Columns that is Required/Mandatory Cross Validates entries in Subnets and DHCP Tabs for Column - DHCP Options Checks if Internet Gateways and Service Gateways are set appropriately. Cross Validates entries in Subnets and VCNsTabs for Column - Subnet CIDR (Checks if the Subnet CIDR belongs to / falls under the VCN CIDR as mentioned in the Subnet's Tab) | | DHCP | Checks if the Columns - Region and Compartment have valid entries Checks if the VCN Names are part of VCN Tab in Column - VCN Name Check if there is value for Customer DNS Column if the entered type is 'CustomDNSServer' Checks for NULL Values if any in all the Columns that is Required/Mandatory | | DRGs | Checks if the Columns - Region and Compartment have valid entries Checks if DRG Name entered is as per mentioned in VCNs tab Checks for the valid format of Attached To column and if it contains VCN Name as mentioned in VCNs Tab. Checks for valid format of column 'Import DRG Route Distribution Statements' Checks that column 'Import DRG Route Distribution Statements' cannot have any value if colum 'Import DRG Route Distribution' is empty. | | Instances | Checks if the Columns - Region and Compartment have valid entries Checks for mandatory columns - Region, Compartment Name, Availability Domain, Display Name, Subnet Name, SSH Key Var Name, Pub Address, Source Details, Shape Checks if Subnet Name specified is valid as per Subnets Tab of CD3 Checks for valid values for columns - Availability Domain, Fault Domain, Source Details, Shape Checks if the NSG names mentioned in NSGs column are part of NSGs tab of the CD3 excel. | | Block Volumes | Checks if the Columns - Region and Compartment have valid entries Checks for mandatory columns Block Name, Availability Domain, Attach Type. Checks for valid values for columns - Availability Domain, Attach Type, Attached to Instance. Checks if AD mentioned in Block Volumes sheet is same as AD mentioned in Instances sheet for the instance to which block volume is to be attached. | | FSS | Checks if the Columns - Region and Compartment have valid entries Checks for mandatory columns - Region, Compartment Name, Availability Domain, MountTarget Name, MountTarget SubnetName. Checks if Subnet Name specified is valid as per Subnets Tab of CD3 Checks if the NSG names mentioned in NSGs column are part of NSGs tab of the CD3 excel. | Log file containing CD3 validator checks is generated at: /cd3user/tenancies/ / _cd3validator.log Expected ERROR in the log file: Compartment Network does not exist in OCI.\u2192 These error mean that the component is not found in OCI. So, please make sure to create the Compartment \"Network\" before validating other tabs.","title":"CD3 Validator Features"},{"location":"SupportforAdditionalAttributes/","text":"Support for Additional Attributes Follow the below steps to add an attribute that is not present already in your Excel sheet - Add the attribute name to the CD3 Excel sheet (based on the resource the attribute belongs to) as given in Terraform Official Documentation. Uncomment the attribute in .tf files (terraform modules in outdirectory, if they are commented). Uncomment the attribute in Jinja template for the resource attribute. (Resource to Jinja template mapping is available here) Update the variable file for any additional changes like image ocids, ssh public keys, etc Example 1: To add an attribute for Instances - (preserve_boot_volume) Here is the Terraform Hashicorp documentation for instances - https://registry.terraform.io/providers/oracle/oci/latest/docs/resources/core_instance Add an additional column preserve_boot_volume to the Instances Sheet as shown below. Optionally change the underscores to spaces for better readability. Uncomment the parameter in instance.tf file if not already uncommented. Uncomment the parameter in instance-template . Any line that is between {# #} are commented in Jinja templates. From the screenshot below we note that the condition for preserve_boot_volume is within the Jinja comments. Copy the highlighted line and place it after/outside line 184 ( #}**) as per below screenshot. Before After Apart from the above changes, optionally, update the instance_ssh_keys and instance_source_ocids in your variables file before executing the toolkit to generate the auto.tfvars for instances. Exemple 2 : To Add Freeform Tags Automation Tool Kit allows the tagging of resources. To use this option, the user is required to add the below column to the appropriate CD3 sheet. Ex: To Tag your Instances, Open the \u2018Instances\u2019 sheet of your CD3 and add the below column at the end. FreeForm Tags Note The Tag Values (Default and Freeform Tags) specified will apply to all the resources in the tab. Ex: The tags applied to VCNs will not be applied to its objects like IGW, NGW, SGW, LPG, etc Empty column values are allowed for FreeForm and Defined Tags; when used it does not attach any tags to the resource. eg: Row 1 in the below example Semi Colon is used as Delimiter between multiple tag values (Example as shown below) Allowed Values for Tags include the following formats: ( Semi-colon delimited values to be entered) Example: S.No Freeform Tags Defined Tags 1. 2. Network=Test1;Network2=Test40 Operations.CostCenter=01;Users.Name=user01 3. Network=Test2; Network2=Test4 Application.Env=Dev 4. Network= OS.Version= 5. testing Platform.Usage Export of new attributes is only supported if the attribute name of Terraform documentation matches that of the Python SDK. Export may fail to fetch the data incase there is a mismatch of the variable names. Resource to Template Mapping - Added New options for CIS compliance. The following Table maps the Excel Sheet to the Resources to the Templates: CD3-CIS-template.xlsx: Tab Name/SetUpOCI Option Resource Name(OCI Console) Jinja2 Template Path Jinja2 Template Name! VCNs SubnetsDHCP RouteRulesinOCI SecRulesinOCI NSGs Networking : Virtual Cloud Networks cd3_automation_toolkit\\Network\\BaseNetwork\\templates\\ major-objects-drgs-template major-objects-igws-template major-objects-ngws-template major-objects-lpgs-template major-objects-sgws-template major-objects-vcns-template major-objects-drg-attachments-template major-objects-default-dhcp-template subnet-template custom-dhcp-template drg-data-source-template drg-route-distribution-statement-template drg-route-distribution-template drg-route-rule-template drg-route-table-template default-route-table-template route-rule-template route-table-template default-seclist-template seclist-template sec-rule-template nsg-rule-template nsg-template Tags Governance: Tag Namespace cd3_automation_toolkit\\Governance\\Tagging\\templates tags-namespaces-template tags-keys-template tags-defaults-template OSS Object Storage Bucket cd3_automation_toolkit\\Storage\\ObjectStorage\\templates oss-policy-template oss-template OKE Developer Service: Oracle Kubernetes Service cd3_automation_toolkit\\DeveloperServices\\OKE\\templates\\ cluster-template nodepool-template NLB-Listeners NLB-BackendSets-BackendServers Networking: Network Load Balancers cd3_automation_toolkit\\Networking\\LoadBalancers\\templates\\ nlb-template nlb-backend-set-template nlb-backend-server-template nlb-listener-template nlb-reserved-ips-template Logging VCN Flow Logs Object Storage Bucket Logs cd3_automation_toolkit\\ManagementServices\\Logging\\templates logging-template LB-Hostname-Certs BackendSet-BackendServer RuleSet PathRouteSet LB-Listener Networking: Load Balancers cd3_automation_toolkit\\Networking\\LoadBalancers\\templates\\ lbr-template certificate-template hostname-template cipher-suite-template backend-server-template backend-set-template rule-set-template access-control-rules-template access-method-rules-template http-header-rules-template request-response-header-rules-template uri-redirect-rules-template path-route-set-template path-route-rules-template listener-template lbr-reserved-ips-template Key Vault Key and Vault cd3_automation_toolkit\\Security\\KeyVault\\templates keys-template vaults-template FSS File Storage: File Systems cd3_automation_toolkit\\Storage\\FileStorage\\templates\\ fss-template export-resource-template export-options-template mount-target-template DedicatedVMHosts Instances Compute: Dedicated Virtual Machine Hosts Instances cd3_automation_toolkit\\Compute\\templates\\ dedicatedvmhosts-template instances-template Compartments Groups Policies Identity: Compartments Groups Dynamic Groups Policies cd3_automation_toolkit\\Identity\\Compartments\\templates\\ cd3_automation_toolkit\\Identity\\Groups\\templates\\ cd3_automation_toolkit\\Identity\\Policies\\templates\\ compartments-template groups-template policies-template Cloud Guard Cloud Guard cd3_automation_toolkit\\Security\\CloudGuard\\templates cloud-guard-config-template cloud-guard-target-template Budgets Governance: Budgets cd3_automation_toolkit\\Governance\\Billing\\templates budget-alert-rule-template budget-template BlockVolumes Block Storage: Block Volumes cd3_automation_toolkit\\Storage\\BlockVolume\\templates\\ blockvolumes-template ADB DBSystems-VM-BM EXA-Infra EXA-VMClusters Autonomous Data Warehouse Autonomous Transaction Processing Bare Metal, VM and Exadata Infra, and Exadata VM Clusters cd3_automation_toolkit\\Database\\templates\\ adb-template dbsystems-vm-bm-template exa-infra-template exa-vmclusters-template CD3-CIS-ManagementServices-template.xlsx Tab Name/SetUpOCI Option Resource Name(OCI Console) Jinja2 Template Path Jinja2 Template Name! Notifications Events Alarms ServiceConnectors Application Integration: Notification Events Service Alarms Service Connector Hub cd3_automation_toolkit\\ManagementServices\\EventsAndNotifications\\templates\\ cd3_automation_toolkit\\ManagementServices\\Monitoring\\templates\\ cd3_automation_toolkit\\ManagementServices\\ServiceConnectorHub\\templates\\ actions-template events-template notifications-topics-template notifications-subscriptions-template service-connectors-template","title":"Support for Additional Attributes"},{"location":"SupportforAdditionalAttributes/#support-for-additional-attributes","text":"Follow the below steps to add an attribute that is not present already in your Excel sheet - Add the attribute name to the CD3 Excel sheet (based on the resource the attribute belongs to) as given in Terraform Official Documentation. Uncomment the attribute in .tf files (terraform modules in outdirectory, if they are commented). Uncomment the attribute in Jinja template for the resource attribute. (Resource to Jinja template mapping is available here) Update the variable file for any additional changes like image ocids, ssh public keys, etc Example 1: To add an attribute for Instances - (preserve_boot_volume) Here is the Terraform Hashicorp documentation for instances - https://registry.terraform.io/providers/oracle/oci/latest/docs/resources/core_instance Add an additional column preserve_boot_volume to the Instances Sheet as shown below. Optionally change the underscores to spaces for better readability. Uncomment the parameter in instance.tf file if not already uncommented. Uncomment the parameter in instance-template . Any line that is between {# #} are commented in Jinja templates. From the screenshot below we note that the condition for preserve_boot_volume is within the Jinja comments. Copy the highlighted line and place it after/outside line 184 ( #}**) as per below screenshot. Before After Apart from the above changes, optionally, update the instance_ssh_keys and instance_source_ocids in your variables file before executing the toolkit to generate the auto.tfvars for instances. Exemple 2 : To Add Freeform Tags Automation Tool Kit allows the tagging of resources. To use this option, the user is required to add the below column to the appropriate CD3 sheet. Ex: To Tag your Instances, Open the \u2018Instances\u2019 sheet of your CD3 and add the below column at the end. FreeForm Tags Note The Tag Values (Default and Freeform Tags) specified will apply to all the resources in the tab. Ex: The tags applied to VCNs will not be applied to its objects like IGW, NGW, SGW, LPG, etc Empty column values are allowed for FreeForm and Defined Tags; when used it does not attach any tags to the resource. eg: Row 1 in the below example Semi Colon is used as Delimiter between multiple tag values (Example as shown below) Allowed Values for Tags include the following formats: ( Semi-colon delimited values to be entered) Example: S.No Freeform Tags Defined Tags 1. 2. Network=Test1;Network2=Test40 Operations.CostCenter=01;Users.Name=user01 3. Network=Test2; Network2=Test4 Application.Env=Dev 4. Network= OS.Version= 5. testing Platform.Usage Export of new attributes is only supported if the attribute name of Terraform documentation matches that of the Python SDK. Export may fail to fetch the data incase there is a mismatch of the variable names.","title":"Support for Additional Attributes"},{"location":"SupportforAdditionalAttributes/#resource-to-template-mapping-","text":"Added New options for CIS compliance. The following Table maps the Excel Sheet to the Resources to the Templates: CD3-CIS-template.xlsx: Tab Name/SetUpOCI Option Resource Name(OCI Console) Jinja2 Template Path Jinja2 Template Name! VCNs SubnetsDHCP RouteRulesinOCI SecRulesinOCI NSGs Networking : Virtual Cloud Networks cd3_automation_toolkit\\Network\\BaseNetwork\\templates\\ major-objects-drgs-template major-objects-igws-template major-objects-ngws-template major-objects-lpgs-template major-objects-sgws-template major-objects-vcns-template major-objects-drg-attachments-template major-objects-default-dhcp-template subnet-template custom-dhcp-template drg-data-source-template drg-route-distribution-statement-template drg-route-distribution-template drg-route-rule-template drg-route-table-template default-route-table-template route-rule-template route-table-template default-seclist-template seclist-template sec-rule-template nsg-rule-template nsg-template Tags Governance: Tag Namespace cd3_automation_toolkit\\Governance\\Tagging\\templates tags-namespaces-template tags-keys-template tags-defaults-template OSS Object Storage Bucket cd3_automation_toolkit\\Storage\\ObjectStorage\\templates oss-policy-template oss-template OKE Developer Service: Oracle Kubernetes Service cd3_automation_toolkit\\DeveloperServices\\OKE\\templates\\ cluster-template nodepool-template NLB-Listeners NLB-BackendSets-BackendServers Networking: Network Load Balancers cd3_automation_toolkit\\Networking\\LoadBalancers\\templates\\ nlb-template nlb-backend-set-template nlb-backend-server-template nlb-listener-template nlb-reserved-ips-template Logging VCN Flow Logs Object Storage Bucket Logs cd3_automation_toolkit\\ManagementServices\\Logging\\templates logging-template LB-Hostname-Certs BackendSet-BackendServer RuleSet PathRouteSet LB-Listener Networking: Load Balancers cd3_automation_toolkit\\Networking\\LoadBalancers\\templates\\ lbr-template certificate-template hostname-template cipher-suite-template backend-server-template backend-set-template rule-set-template access-control-rules-template access-method-rules-template http-header-rules-template request-response-header-rules-template uri-redirect-rules-template path-route-set-template path-route-rules-template listener-template lbr-reserved-ips-template Key Vault Key and Vault cd3_automation_toolkit\\Security\\KeyVault\\templates keys-template vaults-template FSS File Storage: File Systems cd3_automation_toolkit\\Storage\\FileStorage\\templates\\ fss-template export-resource-template export-options-template mount-target-template DedicatedVMHosts Instances Compute: Dedicated Virtual Machine Hosts Instances cd3_automation_toolkit\\Compute\\templates\\ dedicatedvmhosts-template instances-template Compartments Groups Policies Identity: Compartments Groups Dynamic Groups Policies cd3_automation_toolkit\\Identity\\Compartments\\templates\\ cd3_automation_toolkit\\Identity\\Groups\\templates\\ cd3_automation_toolkit\\Identity\\Policies\\templates\\ compartments-template groups-template policies-template Cloud Guard Cloud Guard cd3_automation_toolkit\\Security\\CloudGuard\\templates cloud-guard-config-template cloud-guard-target-template Budgets Governance: Budgets cd3_automation_toolkit\\Governance\\Billing\\templates budget-alert-rule-template budget-template BlockVolumes Block Storage: Block Volumes cd3_automation_toolkit\\Storage\\BlockVolume\\templates\\ blockvolumes-template ADB DBSystems-VM-BM EXA-Infra EXA-VMClusters Autonomous Data Warehouse Autonomous Transaction Processing Bare Metal, VM and Exadata Infra, and Exadata VM Clusters cd3_automation_toolkit\\Database\\templates\\ adb-template dbsystems-vm-bm-template exa-infra-template exa-vmclusters-template CD3-CIS-ManagementServices-template.xlsx Tab Name/SetUpOCI Option Resource Name(OCI Console) Jinja2 Template Path Jinja2 Template Name! Notifications Events Alarms ServiceConnectors Application Integration: Notification Events Service Alarms Service Connector Hub cd3_automation_toolkit\\ManagementServices\\EventsAndNotifications\\templates\\ cd3_automation_toolkit\\ManagementServices\\Monitoring\\templates\\ cd3_automation_toolkit\\ManagementServices\\ServiceConnectorHub\\templates\\ actions-template events-template notifications-topics-template notifications-subscriptions-template service-connectors-template","title":"Resource to Template Mapping -"},{"location":"Tabs/","text":"Compartments Tab Use this Tab to create compartments in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu will allow to create compartments in the OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_compartments.auto.tfvars where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity -> Compartments in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_compartments/<Date>-<Month>-<Time> . NOTE - - Automation Tool Kit generates the TF Configuration files for all the compartments in the tenancy. If some compartment was already existing in OCI then on Terraform Apply, the user will see logs which indicate creation of that compartment - this can be ignored as Terraform will only modify the existing Compartments (with additional information, if there are any eg description) and not create a new/duplicate one. - Terraform destroy on compartments or removing the compartments details from *_compartments.auto.tfvars will not delete them from OCI Console by default. Inorder to destroy them from OCI either - Add an additional column - enable_delete to Compartments Tab of CD3 Excel sheet with the value \"true\" for the compartments that needs to be deleted on terraform destroy. Execute the toolkit menu option to Create Compartments. (OR) Add enable_delete = true parameter to each of the compartment that needs to be deleted in *_compartments.auto.tfvars Groups Tab Use this Tab to create groups in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu will allow to create groups in the OCI tenancy. Automation toolkit supports creation and export of Dynamic Groups as well. Output terraform file generated: <outdir>/<region>/<prefix>_groups.auto.tfvars under where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity -> Groups in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_groups/<Date>-<Month>-<Time> . Policies Tab Use this Tab to create policies in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu will allow to create policies in the OCI tenancy. Output terraform files generated: <outdir>/<region>/<prefix>_policies.auto.tfvars where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity -> Policies in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_policies/<Date>-<Month>-<Time> . Users Tab Use this Tab to create local users in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu and \"Add/Modify/Delete Users\" submenu will allow to create users in the OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_users.auto.tfvars where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity & Security -> Users in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_users/<Date>-<Month>-<Time> . Network Sources Tab Use this Tab to create Network Source in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu and \"Add/Modify/Delete Network Sources\" submenu will allow to create Network Sources in the OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_networksources.auto.tfvars where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity & Security -> Network Sources in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_networksources/<Date>-<Month>-<Time> . Note - Network Source creation/updation is supported only in the home region. Tags Tab Use this Tab to create tags - Namespaces, Key-Value pairs, Default and Cost Tracking Tags. On choosing \"Tags\" in the SetUpOCI menu will allow to create Tags in the OCI tenancy. Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_tags-defaults.auto.tfvars ---> <outdir>/<region>/<prefix>_tags-namespaces.auto.tfvars ---> <outdir>/<region>/<prefix>_tags-keys.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Governance -> Tag Namespaces for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_Tagging/<Date>-<Month>-<Time> . a. VCNs Tab Note: Please mention value for column 'Hub/Spoke/Peer/None' in VCNs tab as None for utilizing DRGv2 functionality (where DRG is directly attached to all VCNs and hub/spoke model is not required) Declare the DRG for the VCN in 'DRG Required' column of VCNs tab and then declare the attachment in DRGs tab also. Toolkit verifies the declaration in VCNs tab and then creates the DRG while reading the DRGs tab. b. DRGs Tab Note: Only VCN and RPC attachments are supported via CD3 as of now for DRGv2. Please create attachments for VC and IPSec via OCI console. Network export will also export only VCN and RPC attachments to CD3 excel sheet as of now. You can create a Route Table for DRG which is not attached to any attachment by keeping 'Attached To' column in DRGs tab empty. You can create an Import Route Distribution which is attached to some Route Table in DRG. c. VCN Info tab This is an important tab and contains general information about networking to be setup. d. DHCP tab This contains information about DHCP options to be created for each VCN. e. SubnetsVLANs tab Notes: Name of the VCNs, subnets etc are all case-sensitive. Specify the same names in all required places. Avoid trailing spaces for a resource Name. A subnet or a vlan will be created based on the column - 'Subnet or VLAN'. When VLAN is specified, vlan tag can also be specified with sytanx as VLAN::<vlan_tag> Column NSGs is read only for type VLAN. Columns - DHCP Option Name, Seclist Names, Add Default Seclist and DNS Label are applicable only for type Subnet. Default Route Rules created are : a. Based on the values entered in columns \u2018configure SGW route\u2019, \u2018configure NGW route\u2019, \u2018configure IGW route\u2019, 'configure Onprem route' and 'configure VCNPeering route' in Subnets sheet; if the value entered is \u2018y\u2019, it will create a route for the object in that subnet eg if \u2018configure IGW\u2019 in Subnets sheet is \u2018y\u2019 then it will read parameter \u2018igw_destination\u2019 in VCN Info tab and create a rule in the subnet with destination object as IGW of the VCN and destination CIDR as value of igw_destnation field. If comma separated values are entered in the igw_destination in VCN Info tab then the tool creates route rule for each destination cidr for IGW in that subnet.Tool works similarly for \u2018configure NGW\u2019 in Subnets tab and \u2018ngw_destination\u2019 in VCN Info tab. For SGW, route rule is added either 'all services' or object storage in that region. b. For a hub spoke model, tool automatically creates route tables attached with the DRG and each LPG in the hub VCN peered with spoke VCN. \u2018onprem_destinations\u2019 in VCN Info tab specifies the On Prem Network CIDRs. The below Default Security Rules are created: a. Egress rule allowing all protocols for 0.0.0.0/0 is opened. b. Ingress rule allowing all protocols for subnet CIDR is opened. This is to allow communication between VMs with in the same subnet. Default Security List of the VCN is attached to the subnet if \u2018add_default_seclist\u2019 parameter in Subnets tab is set to \u2018y\u2019. Components- IGW, NGW, DRG, SGW, LPGs and NSGs are created in same compartment as the VCN. VCN names need to be unique for the same region. Automation ToolKit does not support duplicate values at the moment. However you can have same VCN names across different regions. Output terraform files are generated under <outdir>/<region> directory. Once terraform apply is done, you can view the resources under Networking -> Virtual Cloud Networks in OCI console. Output files generated: File name Description <prefix>_major-objects.auto.tfvars Contains TF for all VCNs and components- IGW, NGW, SGW, DRG, LPGs. <prefix>_custom-dhcp.auto.tfvars Contains TF for all DHCP options for all VCNs. <prefix>_routetables.auto.tfvars <prefix>_default-routetables.auto.tfvars <prefix>_drg-routetables.auto.tfvars <prefix>_drg-distributions.auto.tfvars <prefix>_drg-data.auto.tfvars Separate file for each route table name is created. Contains TF for route rules in each route table. <prefix>_seclists.auto.tfvars <prefix>_default-seclists.auto.tfvars Separate file for each security list name is created. Contains TF for security rules in each security list. <prefix>_subnets.auto.tfvars Contains TF for all subnets for all VCNs. <prefix>_vlans.auto.tfvars Contains TF for all VLANs for all VCNs. <prefix>_default-dhcp.auto.tfvars Contains TF for default DHCP options of each VCN in each region <prefix>_nsgs.auto.tfvars <prefix>_nsg-rules.auto.tfvars Contains TF for NSGs in each region DNS-Views-Zones-Records-Tab Below are the details about specific columns to fill the sheet for DNS-Views-Zones-Records-Tab \"Compartment Name\"- Compartment name for the Views/Zones \"View Name\"- Should be unique in a region \"Zone\" - Zone Name under the specified view \"Domain\" - Full domain name (including zone name) \"RType\" - Select the RType from the list \"RDATA\" - Provide multi values as supported by the specified RType, separated by newline. Click here to read more about RType and RDATA. \"Defined Tags\" - Specify the defined tag key and its value in the format - <Namespace>.<TagKey>=<Value> else leave it empty. Multiple Tag Key , Values can be specified using semi-colon (;) as the delimeter. Example: Operations.CostCenter=01;Users.Name=user01 There must be only Single Row for Domain and RType combination Rows are duplicated in case of multiple child resources Output terraform files are generated under <outdir>/<region> directory. Once terraform apply is done, you can view the resources under Networking -> DNS management in OCI console DNS-Resolvers-Tab Existing Resolvers need to be exported first before making any changes to those. Below are the details about specific columns to fill the sheet for DNS-Resolvers-Tab \"Compartment Name\" - Compartment name for VCN \"Display Name\" - Display Name is same as the VCN Name by default. \"Associated Private Views\" - Format: <view_compartment>@<view_name> . Multiple views are seperated by newline in the same cell(\\n is not supported). \"Endpoint Display Name\" - Provide endpoint display name, new row need to be created for each endpoint in a resolver. Duplicate Names are not allowed for a single resolver. \"Endpoint Type:IP Address\" - Format Type:IP, Type could be Forwarding or Listening. IP can be left as null if not predefined. \"Endpoint NSGs\"- NSGs attached to the endpint. \"Rules\" - Format: Type::Clients::Destination IP. Multiple rules are seperated by newline in the same cell(\\n is not supported)(Rules are processed only for Forwarding Endpoints) \"Defined Tags\" - Specify the defined tag key and its value in the format - <Namespace>.<TagKey>=<Value> else leave it empty. Multiple Tag Key , Values can be specified using semi colon (;) as the delimeter. Example: Operations.CostCenter=01;Users.Name=user01 Associated Private Views can be null/blank Output terraform files are generated under <outdir>/<region> directory. Once terraform apply is done, you can view the resources under Networking -> Virtual Cloud Network -> VCN Information in OCI console DedicatedVMHosts Tab Fill up the details in 'DedicatedVMHosts' sheet and follow the options below. On choosing \"Compute\" in the SetUpOCI menu and \"Add/Modify/Delete Dedicated VM Hosts\" submenu will allow to launch your VM on a dedicated host. Output terraform file generated: <outdir>/<region>/<prefix>_dedicatedvmhosts.auto.tfvars . Once terraform apply is done, you can view the resources under Compute -> Dedicated Virtual Machine Hosts for the region. If you want to update or add new dedicated VM hosts, update the 'DedicatedVMHosts' tab in cd3 and rerun using setUpOCI. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_dedicatedvmhosts/<Date>-<Month>-<Time> . Instances Tab CD3 Tab Specifications: \"Display Name\" column is case sensitive. Specified value will be the display name of Instance in OCI console. Optional columns can also be left blank - like Fault Domain, IP Address. They will take default values when left empty. Leave columns: Backup Policy, NSGs, DedicatedVMHost blank if instance doesn't need to be part of any of these. Instances can be made a part of Backup Policy and NSGs later by choosing appropriate option in setUpOCI menu. Note: The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the instance_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable 'instance_ssh_keys' { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use '\\n' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = \"ssh-rsa AAXXX......yhdlo\\nssh-rsa AAxxskj...edfwf\" #START_instance_ssh_keys# # exported instance ssh keys #instance_ssh_keys_END# } } Enter subnet name column value as: <vcn-name>_<subnet-name> Enter remote execute script(Ansible/Shell) name. Shell scripts should be named with .sh and ansible with .yaml or *.yml inside 'scripts' folder within the region/service dir. This feature is tested against OL8. Create a column called 'Cloud Init Script' to execute scripts (located under 'scripts' folder within the region/service dir) as part of cloud-init. Source Details column of the excel sheet accepts both image and boot volume as the source for instance to be launched. Format - image::<variable containing ocid of image> or bootVolume::<variable containing ocid of boot volume> Make sure to have an entry in variables_<region>.tf file for the value you enter in Source Details field of the Excel sheet. Ex: If you enter the Source Details as image::Linux, make an entry in variables_<region>.tf file under the instance_source_ocids variable as shown below: variable 'instance_source_ocids' { type = map(any) Linux = \"<LATEST LINUX OCID HERE>\" Windows = \"<LATEST WINDOWS OCID HERE>\" PaloAlto = \"Palo Alto Networks VM-Series Next Generation Firewall\" #START_instance_source_ocids# # exported instance image ocids #instance_source_ocids_END# } Mention shape to be used in Shape column of the excel sheet. If Flex shape is to be used format is: shape::ocpus eg: VM.Standard.E3.Flex::5 Custom Policy Compartment Name : Specify the compartment name where the Custom Policy is created. While export of instances, it will fetch details for only the primary VNIC attached to the instance On choosing \"Compute\" in the SetUpOCI menu and \"Add/Modify/Delete Instances/Boot Backup Policy\" submenu will allow to launch your VM on OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_instances.auto.tfvars and <outdir>/<region>/<prefix>_boot-backup-policy.auto.tfvars under appropriate <region> directory. Once the terraform apply is complete, view the resources under Compute -> Instances for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_instances/<Date>-<Month>-<Time> . BlockVolumes Tab This tab in cd3 excel sheet is used when you need to create block volumes and attach the same to the instances in the OCI tenancy. Automation Tool Kit does not support sharing of volumes at the moment. While export of block volumes, if the block volume is attached to multiple instances, it will just fetch details about one attachment. On choosing \"Storage\" in the SetUpOCI menu and \"Add/Modify/Delete Block Volumes/Block Backup Policy\" submenu will allow to create block volumes in OCI Tenancy. On completion of execution, you will be able to find the output terraform file generated at : -\u2192 <outdir>/<region>/<prefix>_blockvolumes.auto.tfvars -\u2192 <outdir>/<region>/<prefix>_block-backup-policy.auto.tfvars under appropriate <region> directory. Once terraform apply is done, you can view the resources under Block Storage -> Block Volumes in OCI console. On re-running the option to create Block Volumes you will find the previously existing files being backed up under directory: <outdir>/<region>/backup_blockvolumes/<Date>-<Month>-<Time> and <outdir>/<region>/backup_BlockBackupPolicy/<Date>-<Month>-<Time> . FSS Tab On choosing \"Storage\" in the SetUpOCI menu and \"Add/Modify/Delete File Systems\" submenu will allow to create file system storage on OCI tenancy. Note: Freeform and Defined Tags - If specified, applies to FSS object only and not to other components like Mount Target. Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_fss.auto.tfvars Once terraform apply is done, you can view the resources under File Storage \u2192 File Systems for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_FSS/<Date>-<Month>-<Time> . Load Balancers Automation Tool Kit allows you to create Load Balancers. Components that you can create using the Tool Kit includes: Resource Tab Name Load Balancers Hostnames Cipher Suites Certificates LB-Hostname-Certs Backend Sets and Backend Servers BackendSet-BackendServer Rule Set RuleSet Path Route Set PathRouteSet Listeners LB-Listeners NOTE : While exporting and synching the tfstate file for LBR objects, the user may be notified that a few components will be modified on apply. In such scenarios, add the attributes that the Terraform notifies to be changed to the appropriate CD3 Tab of Load Balancer and Jinja2 Templates (as a non-default attribute) and re-run the export. On choosing \"Load Balancers\" in the SetUpOCI menu will allow to create load balancers in OCI tenancy. Load Balancers, Hostnames , Certificates and Cipher Suites: Use the tab LB-Hostname-Certs of CD3 Excel to create the following components of Load Balancer: Load Balancers Hostnames Cipher Suites Certificates Certificates, Hostnames and Cipher Suites are optional. Leave the related columns empty if they are not required. LB-Hostname-Certs Tab Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_lb-hostname-certs.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking \u2192 Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_LB-Hostname-Certs/<Date>-<Month>-<Time> . LB-Backend Set and Backend Servers Tab Use the tab LB-BackendSet-BackendServer of CD3 Excel to create the following components of Load Balancer: Backend Sets Backend Servers Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_backendset-backendserver.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking\u2192Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_BackendSet-BackendServer/<Date>-<Month>-<Time> . LB-RuleSet Tab Use the tab LB-RuleSet of CD3 Excel to create the following components of Load Balancer: Rule Sets RuleSet Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_ruleset.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking\u2192Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_RuleSet/<Date>-<Month>-<Time> . LB-Path Route Set Tab Use the tab LB-PathRouteSet of CD3 Excel to create the following components of Load Balancer: Path Route Sets PathRouteSet: Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_pathrouteset.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking\u2192Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_PathRouteSet/<Date>-<Month>-<Time> . LB-Listeners Tab Use the tab LB-Listener of CD3 Excel to create the following components of Load Balancer: Path Route Sets LB-Listener: Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_lb-listener.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking\u2192Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_LB-Listener/<Date>-<Month>-<Time> . DBSystems-VM-BM Tab This helps you to create DB Systems hosted on Virtual Machine and Bare Metal. This can be configured based on the shape chosen in the tab. Note: The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the dbsystem_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable \"dbsystem_ssh_keys\" { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use ',' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = [\"ssh-rsa AAXXX......yhdlo\",\"ssh-rsa AAxxskj...edfwf\"] #START_dbsystem_ssh_keys# # exported dbsystem ssh keys #dbsystem_ssh_keys_END# } } On choosing \"Database\" in the SetUpOCI menu and \"Add/Modify/Delete Virtual Machine or Bare Metal DB Systems\" submenu will allow to create DB Systems hosted on Virtual Machine and Bare Metal. Output terraform file generated: <outdir>/<region>/<prefix>_dbsystem-vm-bm.auto.tfvars under where <region> directory is the region specified for the DB System. Once terraform apply is done, you can view the resources under Bare Metal, VM, and Exadata-> DB Systems in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_dbsystems-vm-bm/<Date>-<Month>-<Time> . ExaCS You can create ExaCS in OCI by utilizing Exa-Infra and Exa-VM Cluster tabs in CD3 excel sheet. Note: The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the exacs_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable \"exacs_ssh_keys\" { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use ',' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = [\"ssh-rsa AAXXX......yhdlo\",\"ssh-rsa AAxxskj...edfwf\"] #START_exacs_ssh_keys# # exported exacs ssh keys #exacs_ssh_keys_END# } } On choosing \"Database\" in the SetUpOCI menu and \"Add/Modify/Delete EXA Infra and EXA VM Clusters\" submenu will allow to create ExaCS in OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_exa-infra.auto.tfvars under where <region> directory is the region hosting the Exa Infra. <outdir>/<region>/<prefix>_exa-vmclusters.auto.tfvars under where <region> directory is the region hosting the Exa VM Clusters. Once terraform apply is done, you can view the resources under Bare Metal, VM, and Exadata-> Exadata Infrastructure and Exadara VM Clusters in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_exa-infra/<Date>-<Month>-<Time> and <outdir>/<region>/backup_exa-vmclusters/<Date>-<Month>-<Time> ADB Tab Use this Tab to create Autonomous Database Warehouse or Autonomous Database Transaction Processing in the OCI tenancy. On choosing \"Database\" in the SetUpOCI menu and \"Add/Modify/Delete ADBs\" submenu will allow to create Autonomous Database Warehouse or Autonomous Database Transaction Processing in the OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_adb.auto.tfvars under where <region> directory is the region hosting the respective ADB. Once terraform apply is done, you can view the resources under Oracle Database -> Autonomous Database in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_adb/<Date>-<Month>-<Time> NOTE - - Currently toolkit supports ADB creation in Shared Infra only, Notifications Tab On choosing \"Management Services\" in the SetUpOCI menu and \"Add/Modify/Delete Notification\" and \"Add/Modify/Delete Events\" submenu will allow to manage events and notifications in OCI tenancy. Output terraform file generated: <outdir>/<region>/<customer_name>_notifications.auto.tfvars and <outdir>/<region>/ _events.auto.tfvars under <region> directory. Once the terraform apply is complete, view the resources under Application Integration-> Notifications & Application Integration-> Events for the region in OCI Console. Further, on re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_events/<Date>-<Month>-<Time> or <outdir>/<region>/backup_notifications/<Date>-<Month>-<Time> Note: Notifications can not be configured for a particular resource OCID at the moment. Export of Notifications supports ONS and FAAS(will put OCID for the function in the CD3). It will skip the event export if action type is OSS. Alarms Tab Use CD3-ManagementServices-template.xlsx under example folder of GIT as input file for creating/exporting Alarms. On choosing \"Management Services\" in the SetUpOCI menu and \"Add/Modify/Delete Alarms\" submenu will allow to manage alarms in OCI tenancy. Output terraform file generated: <outdir>/<region>/<customer_name>_alarms.auto.tfvars under <region> directory. Once the terraform apply is complete, view the resources under Observability & Management\u2192 Monitoring \u2192 Alarms Definition for the region in OCI Console. Further, on re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_alarms/<Date>-<Month>-<Time> ServiceConnectors Tab Use CD3-ManagementServices-template.xlsx under example folder of GIT as input file for creating/exporting Service connectors. The service connector resources provisioning can be initiated by updating the corresponding excel sheet tab. CIS LZ recommends to create SCH to collect audit logs for all compartments, VCN Flow Logs and Object Storage Logs and send to a particular target that can be read by SIEM. CD3 SCH automation is aligned with CIZ LZ and allow the user to deploy/provision the recommended approach by filling in the suitable data in excel sheet. Output terraform file generated: <outdir>/<region>/<customer_name>_serviceconnectors.auto.tfvars under <region> directory. Once the terraform apply is complete, view the resources under service connectors window for the region in OCI Console. Further, on re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_serviceconnectors/<Date>-<Month>-<Time> Note - The service connector resources created via automation will not have the corresponding IAM policies between source and destination entities. It has to be created separately. The user will get an option to create the IAM policy when you click on Edit for the respective service connector provisioned through terraform like in below screenshot: Also, When the target kind is 'notifications' the value for formatted messages parameter is set to 'true' as default. Its set to 'false' only when the source is 'streaming'. After executing tf_import_commands during export of service connectors, the terraform plan will show log-sources ordering as changes and it rearranges the order for log-sources for that service connector if source/target kind is logging. This can be ignored and you can proceed with terraform apply. OKE Tab Use this tab to create OKE components in OCI. Note: The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the oke_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable \"oke_ssh_keys\" { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use '\\n' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = \"ssh-rsa AAXXX......yhdlo\\nssh-rsa AAxxskj...edfwf\" #START_oke_ssh_keys# #oke_ssh_keys_END# } } For source details column, the format should be as below image::<variable containing ocid of image> Make sure to have an entry in variables_<region>.tf file for the value you enter in Source Details field of the Excel sheet. Eg: If you enter the Source Details as image::Linux, make an entry in variables_<region>.tf file under the oke_source_ocids variable as shown below: variable \"oke_source_ocids\" { type = map(any) default = { Linux = \"<OKE LINUX OCID HERE>\" #START_oke_source_ocids# # exported oke image ocids #oke_source_ocids_END# } } On choosing \"Developer Services\" in the SetUpOCI menu and \"Add/Modify/Delete OKE Cluster and Nodepools\" submenu will allow to manage oke components in OCI tenancy. On completion of execution, you will be able to find the output terraform file generated at : -\u2192 <outdir>/<region>/<prefix>_oke_clusters.auto.tfvars -\u2192 <outdir>/<region>/<prefix>_oke_nodepools.auto.tfvars under appropriate <region> directory. Once terraform apply is done, you can view the resources under Developer Services -> Kubernetes Clusters (OKE) for the region in OCI console. On re-running the option to create oke clusters and noodepools you will find the previously existing files being backed up under directory: <outdir>/<region>/backup_oke/<Date>-<Month>-<Time> . Notes: Current version of the toolkit support only single availability domain placement for the nodepool. So if a cluster is exported with nodepools having multiple placement configuration, the terraform plan will show changes similar to: To avoid this, an ignore statement as shown below is added to ignore any changes to the placement configuration in nodepool. ignore_changes = [node_config_details[0].placement_configs,kubernetes_version, defined_tags[\"Oracle-Tags.CreatedOn\"], defined_tags[\"Oracle-Tags.CreatedBy\"],node_config_details[0].defined_tags[\"Oracle-Tags.CreatedOn\"],node_config_details[0].defined_tags[\"Oracle-Tags.CreatedBy\"]] Known Observed behaviours: It has been observed that the order of kubernetes labels change randomly during an export. In such situations a terraform plan detects it as a change to the kubernetes labels. VCN FLow Logs This will enable Flow logs for all the subnets mentioned in 'SubnetsVLANs' tab of CD3 Excel sheet. Log group for each VCN is created under the same compartment as specified for VCN and all subnets are added as logs to this log group. Below TF file is created: File name Description <customer_name>_vcnflow-logging.auto.tfvars TF variables file containing log group for each VCN and logs for eachsubnet in that VCN. LBaaS Logs This will enable LBaaS logs for all the LBs mentioned in 'LB-Hostname-Certs' tab of CD3 Excel sheet. Log group for each LBaaS is created under the same compartment as specified for LBaaS and access and error log types are added as logs to this log group. Below TF file is created: File name Description <customer_name>_load-balancers-logging.auto.tfvars TF variables file containing log group for each LBaaS and its error and access logs. OSS Logs This will enable OSS Bucket logs for all the buckets mentioned in 'Buckets' tab of CD3 Excel sheet. Log group for each bucket is created under the same compartment as specified for bucket and write log type is added as logs to this log group. Below TF file is created: File name Description <customer_name>_buckets-logging.auto.tfvars TF variables file containing log group for each bucket and its write logs. SDDCs Tab Use this tab to create OCVS in your tenancy. Note: As of now the toolkit supports single cluster SDDC. The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the sddc_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable \"sddc_ssh_keys\" { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use '\\n' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = \"ssh-rsa AAXXX......yhdlo\\nssh-rsa AAxxskj...edfwf\" #START_sddc_ssh_keys# #sddc_ssh_keys_END# } } Management and Workload Datastore volumes must be existing or created separately as part of BlockVolumes Tab. All the Network related information for SDDCs will be provided in SDDCs-Network , where the vlan should be created in SubnetsVLANs On choosing \"Software-Defined Data Centers - OCVS\" in setUpOCI menu, the toolkit will read SDDCs tab and SDDCs-Network tab. The output terraform files will be generated at : -\u2192 <outdir>/<region>/<prefix>_sddcs.auto.tfvars under appropriate <region> directory. Once terraform apply is done, you can view the resources under Hybrid -> Software-Defined Data Centers in OCI console. On re-running the option to create OCVS you will find the previously existing files being backed up under directory: <outdir>/<region>/backup_sddcs/<Date>-<Month>-<Time> . Buckets Tab This tab in cd3 excel sheet is used when you need to create Object storage buckets in the OCI tenancy. On choosing \"Storage\" in the SetUpOCI menu and \"Add/Modify/Delete Buckets\" submenu will allow to create buckets in OCI Tenancy. On completion of execution, you will be able to find the output terraform file generated at : -\u2192 <outdir>/<region>/<prefix>_buckets.auto.tfvars under appropriate <region> directory. Once terraform apply is done, you can view the resources under Object Storage -> Buckets in OCI console. On re-running the option to create Buckets you will find the previously existing files being backed up under directory: <outdir>/<region>/backup_buckets/<Date>-<Month>-<Time> . NOTE: Currently the creation of buckets with indefinite retention rule is not supported, only export is supported. CD3 Tab specifications: The Region, Compartment Name and Bucket Name fields are mandatory. Storage Tier: Once created, this cannot be modified unless you delete and re-create the bucket. Object Versioning: Once enabled, this can only be suspended and cannot be disabled while modifying. Retention Rule: To enable retention rule: 4.1. The versioning should be disabled. 4.2. Specify the value in the format RuleName::TimeAmount::TimeUnit::Retention Rule Lock Enabled .Multiple rules are seperated by newline in the same cell(\\n is not supported). 4.3. Retention Rule Lock Enabled: The time format of the lock should be as per RFC standards. Ex: YYYY-MM-DDThh:mm:ssZ (provide the value only if you want to have the time rule locked enabled). 4.4. TimeAmount: It should be number of Days/Years. Maximun value is 500. 4.5. TimeUnit: It should be either in DAYS and YEARS. Replication Policy: To enable replication policy: 5.1. There should be a policy in place to allow region object storage service to manage objects for the bucket. 5.2 The destination bucket should be already created in the tenancy and cannot have versioning enabled. 5.2. The destination bucket cannot have retention rules. 5.3. The format should be PolicyName::DestinationRegion::DestinationBucketName . Lifecycle Policy Name: Name of the lifecycle policy. Multiple rules can be mentioned in new rows keeping all other details same. Lifecycle Target and Action: For Multipart-uploads, Object filters are not required and Rule Period can only be in Days. NOTE: If you have Auto-tiering mode set to Enabled, you cannot create a object lifecycle policy rule with the action set as Infrequent Access. Lifecycle Rule Period: Its a combination of TimeAmount (It should be number of Days/Years) and TimeUnit (It should be either in DAYS and YEARS). The format should be TimeAmount::TimeUnit Lifecyle Exclusion Patterns/Lifecycle Inclusion Patterns/Lifecycle Inclusion Prefixes: Add the object name filter patterns here.","title":"Tabs"},{"location":"Tabs/#compartments-tab","text":"Use this Tab to create compartments in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu will allow to create compartments in the OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_compartments.auto.tfvars where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity -> Compartments in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_compartments/<Date>-<Month>-<Time> . NOTE - - Automation Tool Kit generates the TF Configuration files for all the compartments in the tenancy. If some compartment was already existing in OCI then on Terraform Apply, the user will see logs which indicate creation of that compartment - this can be ignored as Terraform will only modify the existing Compartments (with additional information, if there are any eg description) and not create a new/duplicate one. - Terraform destroy on compartments or removing the compartments details from *_compartments.auto.tfvars will not delete them from OCI Console by default. Inorder to destroy them from OCI either - Add an additional column - enable_delete to Compartments Tab of CD3 Excel sheet with the value \"true\" for the compartments that needs to be deleted on terraform destroy. Execute the toolkit menu option to Create Compartments. (OR) Add enable_delete = true parameter to each of the compartment that needs to be deleted in *_compartments.auto.tfvars","title":"Compartments Tab"},{"location":"Tabs/#groups-tab","text":"Use this Tab to create groups in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu will allow to create groups in the OCI tenancy. Automation toolkit supports creation and export of Dynamic Groups as well. Output terraform file generated: <outdir>/<region>/<prefix>_groups.auto.tfvars under where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity -> Groups in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_groups/<Date>-<Month>-<Time> .","title":"Groups Tab"},{"location":"Tabs/#policies-tab","text":"Use this Tab to create policies in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu will allow to create policies in the OCI tenancy. Output terraform files generated: <outdir>/<region>/<prefix>_policies.auto.tfvars where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity -> Policies in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_policies/<Date>-<Month>-<Time> .","title":"Policies Tab"},{"location":"Tabs/#users-tab","text":"Use this Tab to create local users in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu and \"Add/Modify/Delete Users\" submenu will allow to create users in the OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_users.auto.tfvars where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity & Security -> Users in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_users/<Date>-<Month>-<Time> .","title":"Users Tab"},{"location":"Tabs/#network-sources-tab","text":"Use this Tab to create Network Source in the OCI tenancy. On choosing \"Identity\" in the SetUpOCI menu and \"Add/Modify/Delete Network Sources\" submenu will allow to create Network Sources in the OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_networksources.auto.tfvars where <region> directory is the home region. Once terraform apply is done, you can view the resources under Identity & Security -> Network Sources in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_networksources/<Date>-<Month>-<Time> . Note - Network Source creation/updation is supported only in the home region.","title":"Network Sources Tab"},{"location":"Tabs/#tags-tab","text":"Use this Tab to create tags - Namespaces, Key-Value pairs, Default and Cost Tracking Tags. On choosing \"Tags\" in the SetUpOCI menu will allow to create Tags in the OCI tenancy. Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_tags-defaults.auto.tfvars ---> <outdir>/<region>/<prefix>_tags-namespaces.auto.tfvars ---> <outdir>/<region>/<prefix>_tags-keys.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Governance -> Tag Namespaces for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_Tagging/<Date>-<Month>-<Time> .","title":"Tags Tab"},{"location":"Tabs/#a-vcns-tab","text":"Note: Please mention value for column 'Hub/Spoke/Peer/None' in VCNs tab as None for utilizing DRGv2 functionality (where DRG is directly attached to all VCNs and hub/spoke model is not required) Declare the DRG for the VCN in 'DRG Required' column of VCNs tab and then declare the attachment in DRGs tab also. Toolkit verifies the declaration in VCNs tab and then creates the DRG while reading the DRGs tab.","title":"a. VCNs Tab"},{"location":"Tabs/#b-drgs-tab","text":"Note: Only VCN and RPC attachments are supported via CD3 as of now for DRGv2. Please create attachments for VC and IPSec via OCI console. Network export will also export only VCN and RPC attachments to CD3 excel sheet as of now. You can create a Route Table for DRG which is not attached to any attachment by keeping 'Attached To' column in DRGs tab empty. You can create an Import Route Distribution which is attached to some Route Table in DRG.","title":"b. DRGs Tab"},{"location":"Tabs/#c-vcn-info-tab","text":"This is an important tab and contains general information about networking to be setup.","title":"c. VCN Info tab"},{"location":"Tabs/#d-dhcp-tab","text":"This contains information about DHCP options to be created for each VCN.","title":"d. DHCP tab"},{"location":"Tabs/#e-subnetsvlans-tab","text":"Notes: Name of the VCNs, subnets etc are all case-sensitive. Specify the same names in all required places. Avoid trailing spaces for a resource Name. A subnet or a vlan will be created based on the column - 'Subnet or VLAN'. When VLAN is specified, vlan tag can also be specified with sytanx as VLAN::<vlan_tag> Column NSGs is read only for type VLAN. Columns - DHCP Option Name, Seclist Names, Add Default Seclist and DNS Label are applicable only for type Subnet. Default Route Rules created are : a. Based on the values entered in columns \u2018configure SGW route\u2019, \u2018configure NGW route\u2019, \u2018configure IGW route\u2019, 'configure Onprem route' and 'configure VCNPeering route' in Subnets sheet; if the value entered is \u2018y\u2019, it will create a route for the object in that subnet eg if \u2018configure IGW\u2019 in Subnets sheet is \u2018y\u2019 then it will read parameter \u2018igw_destination\u2019 in VCN Info tab and create a rule in the subnet with destination object as IGW of the VCN and destination CIDR as value of igw_destnation field. If comma separated values are entered in the igw_destination in VCN Info tab then the tool creates route rule for each destination cidr for IGW in that subnet.Tool works similarly for \u2018configure NGW\u2019 in Subnets tab and \u2018ngw_destination\u2019 in VCN Info tab. For SGW, route rule is added either 'all services' or object storage in that region. b. For a hub spoke model, tool automatically creates route tables attached with the DRG and each LPG in the hub VCN peered with spoke VCN. \u2018onprem_destinations\u2019 in VCN Info tab specifies the On Prem Network CIDRs. The below Default Security Rules are created: a. Egress rule allowing all protocols for 0.0.0.0/0 is opened. b. Ingress rule allowing all protocols for subnet CIDR is opened. This is to allow communication between VMs with in the same subnet. Default Security List of the VCN is attached to the subnet if \u2018add_default_seclist\u2019 parameter in Subnets tab is set to \u2018y\u2019. Components- IGW, NGW, DRG, SGW, LPGs and NSGs are created in same compartment as the VCN. VCN names need to be unique for the same region. Automation ToolKit does not support duplicate values at the moment. However you can have same VCN names across different regions. Output terraform files are generated under <outdir>/<region> directory. Once terraform apply is done, you can view the resources under Networking -> Virtual Cloud Networks in OCI console. Output files generated: File name Description <prefix>_major-objects.auto.tfvars Contains TF for all VCNs and components- IGW, NGW, SGW, DRG, LPGs. <prefix>_custom-dhcp.auto.tfvars Contains TF for all DHCP options for all VCNs. <prefix>_routetables.auto.tfvars <prefix>_default-routetables.auto.tfvars <prefix>_drg-routetables.auto.tfvars <prefix>_drg-distributions.auto.tfvars <prefix>_drg-data.auto.tfvars Separate file for each route table name is created. Contains TF for route rules in each route table. <prefix>_seclists.auto.tfvars <prefix>_default-seclists.auto.tfvars Separate file for each security list name is created. Contains TF for security rules in each security list. <prefix>_subnets.auto.tfvars Contains TF for all subnets for all VCNs. <prefix>_vlans.auto.tfvars Contains TF for all VLANs for all VCNs. <prefix>_default-dhcp.auto.tfvars Contains TF for default DHCP options of each VCN in each region <prefix>_nsgs.auto.tfvars <prefix>_nsg-rules.auto.tfvars Contains TF for NSGs in each region","title":"e. SubnetsVLANs tab"},{"location":"Tabs/#dns-views-zones-records-tab","text":"Below are the details about specific columns to fill the sheet for DNS-Views-Zones-Records-Tab \"Compartment Name\"- Compartment name for the Views/Zones \"View Name\"- Should be unique in a region \"Zone\" - Zone Name under the specified view \"Domain\" - Full domain name (including zone name) \"RType\" - Select the RType from the list \"RDATA\" - Provide multi values as supported by the specified RType, separated by newline. Click here to read more about RType and RDATA. \"Defined Tags\" - Specify the defined tag key and its value in the format - <Namespace>.<TagKey>=<Value> else leave it empty. Multiple Tag Key , Values can be specified using semi-colon (;) as the delimeter. Example: Operations.CostCenter=01;Users.Name=user01 There must be only Single Row for Domain and RType combination Rows are duplicated in case of multiple child resources Output terraform files are generated under <outdir>/<region> directory. Once terraform apply is done, you can view the resources under Networking -> DNS management in OCI console","title":"DNS-Views-Zones-Records-Tab"},{"location":"Tabs/#dns-resolvers-tab","text":"Existing Resolvers need to be exported first before making any changes to those. Below are the details about specific columns to fill the sheet for DNS-Resolvers-Tab \"Compartment Name\" - Compartment name for VCN \"Display Name\" - Display Name is same as the VCN Name by default. \"Associated Private Views\" - Format: <view_compartment>@<view_name> . Multiple views are seperated by newline in the same cell(\\n is not supported). \"Endpoint Display Name\" - Provide endpoint display name, new row need to be created for each endpoint in a resolver. Duplicate Names are not allowed for a single resolver. \"Endpoint Type:IP Address\" - Format Type:IP, Type could be Forwarding or Listening. IP can be left as null if not predefined. \"Endpoint NSGs\"- NSGs attached to the endpint. \"Rules\" - Format: Type::Clients::Destination IP. Multiple rules are seperated by newline in the same cell(\\n is not supported)(Rules are processed only for Forwarding Endpoints) \"Defined Tags\" - Specify the defined tag key and its value in the format - <Namespace>.<TagKey>=<Value> else leave it empty. Multiple Tag Key , Values can be specified using semi colon (;) as the delimeter. Example: Operations.CostCenter=01;Users.Name=user01 Associated Private Views can be null/blank Output terraform files are generated under <outdir>/<region> directory. Once terraform apply is done, you can view the resources under Networking -> Virtual Cloud Network -> VCN Information in OCI console","title":"DNS-Resolvers-Tab"},{"location":"Tabs/#dedicatedvmhosts-tab","text":"Fill up the details in 'DedicatedVMHosts' sheet and follow the options below. On choosing \"Compute\" in the SetUpOCI menu and \"Add/Modify/Delete Dedicated VM Hosts\" submenu will allow to launch your VM on a dedicated host. Output terraform file generated: <outdir>/<region>/<prefix>_dedicatedvmhosts.auto.tfvars . Once terraform apply is done, you can view the resources under Compute -> Dedicated Virtual Machine Hosts for the region. If you want to update or add new dedicated VM hosts, update the 'DedicatedVMHosts' tab in cd3 and rerun using setUpOCI. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_dedicatedvmhosts/<Date>-<Month>-<Time> .","title":"DedicatedVMHosts Tab"},{"location":"Tabs/#instances-tab","text":"CD3 Tab Specifications: \"Display Name\" column is case sensitive. Specified value will be the display name of Instance in OCI console. Optional columns can also be left blank - like Fault Domain, IP Address. They will take default values when left empty. Leave columns: Backup Policy, NSGs, DedicatedVMHost blank if instance doesn't need to be part of any of these. Instances can be made a part of Backup Policy and NSGs later by choosing appropriate option in setUpOCI menu. Note: The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the instance_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable 'instance_ssh_keys' { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use '\\n' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = \"ssh-rsa AAXXX......yhdlo\\nssh-rsa AAxxskj...edfwf\" #START_instance_ssh_keys# # exported instance ssh keys #instance_ssh_keys_END# } } Enter subnet name column value as: <vcn-name>_<subnet-name> Enter remote execute script(Ansible/Shell) name. Shell scripts should be named with .sh and ansible with .yaml or *.yml inside 'scripts' folder within the region/service dir. This feature is tested against OL8. Create a column called 'Cloud Init Script' to execute scripts (located under 'scripts' folder within the region/service dir) as part of cloud-init. Source Details column of the excel sheet accepts both image and boot volume as the source for instance to be launched. Format - image::<variable containing ocid of image> or bootVolume::<variable containing ocid of boot volume> Make sure to have an entry in variables_<region>.tf file for the value you enter in Source Details field of the Excel sheet. Ex: If you enter the Source Details as image::Linux, make an entry in variables_<region>.tf file under the instance_source_ocids variable as shown below: variable 'instance_source_ocids' { type = map(any) Linux = \"<LATEST LINUX OCID HERE>\" Windows = \"<LATEST WINDOWS OCID HERE>\" PaloAlto = \"Palo Alto Networks VM-Series Next Generation Firewall\" #START_instance_source_ocids# # exported instance image ocids #instance_source_ocids_END# } Mention shape to be used in Shape column of the excel sheet. If Flex shape is to be used format is: shape::ocpus eg: VM.Standard.E3.Flex::5 Custom Policy Compartment Name : Specify the compartment name where the Custom Policy is created. While export of instances, it will fetch details for only the primary VNIC attached to the instance On choosing \"Compute\" in the SetUpOCI menu and \"Add/Modify/Delete Instances/Boot Backup Policy\" submenu will allow to launch your VM on OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_instances.auto.tfvars and <outdir>/<region>/<prefix>_boot-backup-policy.auto.tfvars under appropriate <region> directory. Once the terraform apply is complete, view the resources under Compute -> Instances for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_instances/<Date>-<Month>-<Time> .","title":"Instances Tab"},{"location":"Tabs/#blockvolumes-tab","text":"This tab in cd3 excel sheet is used when you need to create block volumes and attach the same to the instances in the OCI tenancy. Automation Tool Kit does not support sharing of volumes at the moment. While export of block volumes, if the block volume is attached to multiple instances, it will just fetch details about one attachment. On choosing \"Storage\" in the SetUpOCI menu and \"Add/Modify/Delete Block Volumes/Block Backup Policy\" submenu will allow to create block volumes in OCI Tenancy. On completion of execution, you will be able to find the output terraform file generated at : -\u2192 <outdir>/<region>/<prefix>_blockvolumes.auto.tfvars -\u2192 <outdir>/<region>/<prefix>_block-backup-policy.auto.tfvars under appropriate <region> directory. Once terraform apply is done, you can view the resources under Block Storage -> Block Volumes in OCI console. On re-running the option to create Block Volumes you will find the previously existing files being backed up under directory: <outdir>/<region>/backup_blockvolumes/<Date>-<Month>-<Time> and <outdir>/<region>/backup_BlockBackupPolicy/<Date>-<Month>-<Time> .","title":"BlockVolumes Tab"},{"location":"Tabs/#fss-tab","text":"On choosing \"Storage\" in the SetUpOCI menu and \"Add/Modify/Delete File Systems\" submenu will allow to create file system storage on OCI tenancy. Note: Freeform and Defined Tags - If specified, applies to FSS object only and not to other components like Mount Target. Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_fss.auto.tfvars Once terraform apply is done, you can view the resources under File Storage \u2192 File Systems for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_FSS/<Date>-<Month>-<Time> .","title":"FSS Tab"},{"location":"Tabs/#load-balancers","text":"Automation Tool Kit allows you to create Load Balancers. Components that you can create using the Tool Kit includes: Resource Tab Name Load Balancers Hostnames Cipher Suites Certificates LB-Hostname-Certs Backend Sets and Backend Servers BackendSet-BackendServer Rule Set RuleSet Path Route Set PathRouteSet Listeners LB-Listeners NOTE : While exporting and synching the tfstate file for LBR objects, the user may be notified that a few components will be modified on apply. In such scenarios, add the attributes that the Terraform notifies to be changed to the appropriate CD3 Tab of Load Balancer and Jinja2 Templates (as a non-default attribute) and re-run the export. On choosing \"Load Balancers\" in the SetUpOCI menu will allow to create load balancers in OCI tenancy. Load Balancers, Hostnames , Certificates and Cipher Suites: Use the tab LB-Hostname-Certs of CD3 Excel to create the following components of Load Balancer: Load Balancers Hostnames Cipher Suites Certificates Certificates, Hostnames and Cipher Suites are optional. Leave the related columns empty if they are not required.","title":"Load Balancers"},{"location":"Tabs/#lb-hostname-certs-tab","text":"Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_lb-hostname-certs.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking \u2192 Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_LB-Hostname-Certs/<Date>-<Month>-<Time> .","title":"LB-Hostname-Certs Tab"},{"location":"Tabs/#lb-backend-set-and-backend-servers-tab","text":"Use the tab LB-BackendSet-BackendServer of CD3 Excel to create the following components of Load Balancer: Backend Sets Backend Servers Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_backendset-backendserver.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking\u2192Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_BackendSet-BackendServer/<Date>-<Month>-<Time> .","title":"LB-Backend Set and Backend Servers Tab"},{"location":"Tabs/#lb-ruleset-tab","text":"Use the tab LB-RuleSet of CD3 Excel to create the following components of Load Balancer: Rule Sets RuleSet Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_ruleset.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking\u2192Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_RuleSet/<Date>-<Month>-<Time> .","title":"LB-RuleSet Tab"},{"location":"Tabs/#lb-path-route-set-tab","text":"Use the tab LB-PathRouteSet of CD3 Excel to create the following components of Load Balancer: Path Route Sets PathRouteSet: Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_pathrouteset.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking\u2192Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_PathRouteSet/<Date>-<Month>-<Time> .","title":"LB-Path Route Set Tab"},{"location":"Tabs/#lb-listeners-tab","text":"Use the tab LB-Listener of CD3 Excel to create the following components of Load Balancer: Path Route Sets LB-Listener: Once this is complete you will find the generated output terraform files in location : ---> <outdir>/<region>/<prefix>_lb-listener.auto.tfvars under <region> directory. Once terraform apply is done, you can view the resources under Networking\u2192Load Balancers for the region. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_LB-Listener/<Date>-<Month>-<Time> .","title":"LB-Listeners Tab"},{"location":"Tabs/#dbsystems-vm-bm-tab","text":"This helps you to create DB Systems hosted on Virtual Machine and Bare Metal. This can be configured based on the shape chosen in the tab. Note: The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the dbsystem_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable \"dbsystem_ssh_keys\" { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use ',' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = [\"ssh-rsa AAXXX......yhdlo\",\"ssh-rsa AAxxskj...edfwf\"] #START_dbsystem_ssh_keys# # exported dbsystem ssh keys #dbsystem_ssh_keys_END# } } On choosing \"Database\" in the SetUpOCI menu and \"Add/Modify/Delete Virtual Machine or Bare Metal DB Systems\" submenu will allow to create DB Systems hosted on Virtual Machine and Bare Metal. Output terraform file generated: <outdir>/<region>/<prefix>_dbsystem-vm-bm.auto.tfvars under where <region> directory is the region specified for the DB System. Once terraform apply is done, you can view the resources under Bare Metal, VM, and Exadata-> DB Systems in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_dbsystems-vm-bm/<Date>-<Month>-<Time> .","title":"DBSystems-VM-BM Tab"},{"location":"Tabs/#exacs","text":"You can create ExaCS in OCI by utilizing Exa-Infra and Exa-VM Cluster tabs in CD3 excel sheet. Note: The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the exacs_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable \"exacs_ssh_keys\" { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use ',' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = [\"ssh-rsa AAXXX......yhdlo\",\"ssh-rsa AAxxskj...edfwf\"] #START_exacs_ssh_keys# # exported exacs ssh keys #exacs_ssh_keys_END# } } On choosing \"Database\" in the SetUpOCI menu and \"Add/Modify/Delete EXA Infra and EXA VM Clusters\" submenu will allow to create ExaCS in OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_exa-infra.auto.tfvars under where <region> directory is the region hosting the Exa Infra. <outdir>/<region>/<prefix>_exa-vmclusters.auto.tfvars under where <region> directory is the region hosting the Exa VM Clusters. Once terraform apply is done, you can view the resources under Bare Metal, VM, and Exadata-> Exadata Infrastructure and Exadara VM Clusters in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_exa-infra/<Date>-<Month>-<Time> and <outdir>/<region>/backup_exa-vmclusters/<Date>-<Month>-<Time>","title":"ExaCS"},{"location":"Tabs/#adb-tab","text":"Use this Tab to create Autonomous Database Warehouse or Autonomous Database Transaction Processing in the OCI tenancy. On choosing \"Database\" in the SetUpOCI menu and \"Add/Modify/Delete ADBs\" submenu will allow to create Autonomous Database Warehouse or Autonomous Database Transaction Processing in the OCI tenancy. Output terraform file generated: <outdir>/<region>/<prefix>_adb.auto.tfvars under where <region> directory is the region hosting the respective ADB. Once terraform apply is done, you can view the resources under Oracle Database -> Autonomous Database in OCI console. On re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_adb/<Date>-<Month>-<Time> NOTE - - Currently toolkit supports ADB creation in Shared Infra only,","title":"ADB Tab"},{"location":"Tabs/#notifications-tab","text":"On choosing \"Management Services\" in the SetUpOCI menu and \"Add/Modify/Delete Notification\" and \"Add/Modify/Delete Events\" submenu will allow to manage events and notifications in OCI tenancy. Output terraform file generated: <outdir>/<region>/<customer_name>_notifications.auto.tfvars and <outdir>/<region>/ _events.auto.tfvars under <region> directory. Once the terraform apply is complete, view the resources under Application Integration-> Notifications & Application Integration-> Events for the region in OCI Console. Further, on re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_events/<Date>-<Month>-<Time> or <outdir>/<region>/backup_notifications/<Date>-<Month>-<Time> Note: Notifications can not be configured for a particular resource OCID at the moment. Export of Notifications supports ONS and FAAS(will put OCID for the function in the CD3). It will skip the event export if action type is OSS.","title":"Notifications Tab"},{"location":"Tabs/#alarms-tab","text":"Use CD3-ManagementServices-template.xlsx under example folder of GIT as input file for creating/exporting Alarms. On choosing \"Management Services\" in the SetUpOCI menu and \"Add/Modify/Delete Alarms\" submenu will allow to manage alarms in OCI tenancy. Output terraform file generated: <outdir>/<region>/<customer_name>_alarms.auto.tfvars under <region> directory. Once the terraform apply is complete, view the resources under Observability & Management\u2192 Monitoring \u2192 Alarms Definition for the region in OCI Console. Further, on re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_alarms/<Date>-<Month>-<Time>","title":"Alarms Tab"},{"location":"Tabs/#serviceconnectors-tab","text":"Use CD3-ManagementServices-template.xlsx under example folder of GIT as input file for creating/exporting Service connectors. The service connector resources provisioning can be initiated by updating the corresponding excel sheet tab. CIS LZ recommends to create SCH to collect audit logs for all compartments, VCN Flow Logs and Object Storage Logs and send to a particular target that can be read by SIEM. CD3 SCH automation is aligned with CIZ LZ and allow the user to deploy/provision the recommended approach by filling in the suitable data in excel sheet. Output terraform file generated: <outdir>/<region>/<customer_name>_serviceconnectors.auto.tfvars under <region> directory. Once the terraform apply is complete, view the resources under service connectors window for the region in OCI Console. Further, on re-running the same option you will find the previously existing files being backed up under directory \u2192 <outdir>/<region>/backup_serviceconnectors/<Date>-<Month>-<Time> Note - The service connector resources created via automation will not have the corresponding IAM policies between source and destination entities. It has to be created separately. The user will get an option to create the IAM policy when you click on Edit for the respective service connector provisioned through terraform like in below screenshot: Also, When the target kind is 'notifications' the value for formatted messages parameter is set to 'true' as default. Its set to 'false' only when the source is 'streaming'. After executing tf_import_commands during export of service connectors, the terraform plan will show log-sources ordering as changes and it rearranges the order for log-sources for that service connector if source/target kind is logging. This can be ignored and you can proceed with terraform apply.","title":"ServiceConnectors Tab"},{"location":"Tabs/#oke-tab","text":"Use this tab to create OKE components in OCI. Note: The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the oke_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable \"oke_ssh_keys\" { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use '\\n' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = \"ssh-rsa AAXXX......yhdlo\\nssh-rsa AAxxskj...edfwf\" #START_oke_ssh_keys# #oke_ssh_keys_END# } } For source details column, the format should be as below image::<variable containing ocid of image> Make sure to have an entry in variables_<region>.tf file for the value you enter in Source Details field of the Excel sheet. Eg: If you enter the Source Details as image::Linux, make an entry in variables_<region>.tf file under the oke_source_ocids variable as shown below: variable \"oke_source_ocids\" { type = map(any) default = { Linux = \"<OKE LINUX OCID HERE>\" #START_oke_source_ocids# # exported oke image ocids #oke_source_ocids_END# } } On choosing \"Developer Services\" in the SetUpOCI menu and \"Add/Modify/Delete OKE Cluster and Nodepools\" submenu will allow to manage oke components in OCI tenancy. On completion of execution, you will be able to find the output terraform file generated at : -\u2192 <outdir>/<region>/<prefix>_oke_clusters.auto.tfvars -\u2192 <outdir>/<region>/<prefix>_oke_nodepools.auto.tfvars under appropriate <region> directory. Once terraform apply is done, you can view the resources under Developer Services -> Kubernetes Clusters (OKE) for the region in OCI console. On re-running the option to create oke clusters and noodepools you will find the previously existing files being backed up under directory: <outdir>/<region>/backup_oke/<Date>-<Month>-<Time> . Notes: Current version of the toolkit support only single availability domain placement for the nodepool. So if a cluster is exported with nodepools having multiple placement configuration, the terraform plan will show changes similar to: To avoid this, an ignore statement as shown below is added to ignore any changes to the placement configuration in nodepool. ignore_changes = [node_config_details[0].placement_configs,kubernetes_version, defined_tags[\"Oracle-Tags.CreatedOn\"], defined_tags[\"Oracle-Tags.CreatedBy\"],node_config_details[0].defined_tags[\"Oracle-Tags.CreatedOn\"],node_config_details[0].defined_tags[\"Oracle-Tags.CreatedBy\"]] Known Observed behaviours: It has been observed that the order of kubernetes labels change randomly during an export. In such situations a terraform plan detects it as a change to the kubernetes labels.","title":"OKE Tab"},{"location":"Tabs/#vcn-flow-logs","text":"This will enable Flow logs for all the subnets mentioned in 'SubnetsVLANs' tab of CD3 Excel sheet. Log group for each VCN is created under the same compartment as specified for VCN and all subnets are added as logs to this log group. Below TF file is created: File name Description <customer_name>_vcnflow-logging.auto.tfvars TF variables file containing log group for each VCN and logs for eachsubnet in that VCN.","title":"VCN FLow Logs"},{"location":"Tabs/#lbaas-logs","text":"This will enable LBaaS logs for all the LBs mentioned in 'LB-Hostname-Certs' tab of CD3 Excel sheet. Log group for each LBaaS is created under the same compartment as specified for LBaaS and access and error log types are added as logs to this log group. Below TF file is created: File name Description <customer_name>_load-balancers-logging.auto.tfvars TF variables file containing log group for each LBaaS and its error and access logs.","title":"LBaaS Logs"},{"location":"Tabs/#oss-logs","text":"This will enable OSS Bucket logs for all the buckets mentioned in 'Buckets' tab of CD3 Excel sheet. Log group for each bucket is created under the same compartment as specified for bucket and write log type is added as logs to this log group. Below TF file is created: File name Description <customer_name>_buckets-logging.auto.tfvars TF variables file containing log group for each bucket and its write logs.","title":"OSS Logs"},{"location":"Tabs/#sddcs-tab","text":"Use this tab to create OCVS in your tenancy. Note: As of now the toolkit supports single cluster SDDC. The column \"SSH Key Var Name\" accepts SSH key value directly or the name of variable declared in variables.tf under the sddc_ssh_keys variable containing the key value. Make sure to have an entry in variables_<region>.tf file with the name you enter in SSH Key Var Name field of the Excel sheet and put the value as SSH key value. For Eg: If you enter the SSH Key Var Name as ssh_public_key , make an entry in variables_<region>.tf file as shown below: variable \"sddc_ssh_keys\" { type = map(any) default = { ssh_public_key = \"<SSH PUB KEY STRING HERE>\" # Use '\\n' as the delimiter to add multiple ssh keys. # Example: ssh_public_key = \"ssh-rsa AAXXX......yhdlo\\nssh-rsa AAxxskj...edfwf\" #START_sddc_ssh_keys# #sddc_ssh_keys_END# } } Management and Workload Datastore volumes must be existing or created separately as part of BlockVolumes Tab. All the Network related information for SDDCs will be provided in SDDCs-Network , where the vlan should be created in SubnetsVLANs On choosing \"Software-Defined Data Centers - OCVS\" in setUpOCI menu, the toolkit will read SDDCs tab and SDDCs-Network tab. The output terraform files will be generated at : -\u2192 <outdir>/<region>/<prefix>_sddcs.auto.tfvars under appropriate <region> directory. Once terraform apply is done, you can view the resources under Hybrid -> Software-Defined Data Centers in OCI console. On re-running the option to create OCVS you will find the previously existing files being backed up under directory: <outdir>/<region>/backup_sddcs/<Date>-<Month>-<Time> .","title":"SDDCs Tab"},{"location":"Tabs/#buckets-tab","text":"This tab in cd3 excel sheet is used when you need to create Object storage buckets in the OCI tenancy. On choosing \"Storage\" in the SetUpOCI menu and \"Add/Modify/Delete Buckets\" submenu will allow to create buckets in OCI Tenancy. On completion of execution, you will be able to find the output terraform file generated at : -\u2192 <outdir>/<region>/<prefix>_buckets.auto.tfvars under appropriate <region> directory. Once terraform apply is done, you can view the resources under Object Storage -> Buckets in OCI console. On re-running the option to create Buckets you will find the previously existing files being backed up under directory: <outdir>/<region>/backup_buckets/<Date>-<Month>-<Time> . NOTE: Currently the creation of buckets with indefinite retention rule is not supported, only export is supported. CD3 Tab specifications: The Region, Compartment Name and Bucket Name fields are mandatory. Storage Tier: Once created, this cannot be modified unless you delete and re-create the bucket. Object Versioning: Once enabled, this can only be suspended and cannot be disabled while modifying. Retention Rule: To enable retention rule: 4.1. The versioning should be disabled. 4.2. Specify the value in the format RuleName::TimeAmount::TimeUnit::Retention Rule Lock Enabled .Multiple rules are seperated by newline in the same cell(\\n is not supported). 4.3. Retention Rule Lock Enabled: The time format of the lock should be as per RFC standards. Ex: YYYY-MM-DDThh:mm:ssZ (provide the value only if you want to have the time rule locked enabled). 4.4. TimeAmount: It should be number of Days/Years. Maximun value is 500. 4.5. TimeUnit: It should be either in DAYS and YEARS. Replication Policy: To enable replication policy: 5.1. There should be a policy in place to allow region object storage service to manage objects for the bucket. 5.2 The destination bucket should be already created in the tenancy and cannot have versioning enabled. 5.2. The destination bucket cannot have retention rules. 5.3. The format should be PolicyName::DestinationRegion::DestinationBucketName . Lifecycle Policy Name: Name of the lifecycle policy. Multiple rules can be mentioned in new rows keeping all other details same. Lifecycle Target and Action: For Multipart-uploads, Object filters are not required and Rule Period can only be in Days. NOTE: If you have Auto-tiering mode set to Enabled, you cannot create a object lifecycle policy rule with the action set as Infrequent Access. Lifecycle Rule Period: Its a combination of TimeAmount (It should be number of Days/Years) and TimeUnit (It should be either in DAYS and YEARS). The format should be TimeAmount::TimeUnit Lifecyle Exclusion Patterns/Lifecycle Inclusion Patterns/Lifecycle Inclusion Prefixes: Add the object name filter patterns here.","title":"Buckets Tab"},{"location":"Upgrade_Toolkit/","text":"Steps to Upgrade Your Toolkit (For Existing Customers using older versions): Upgrade to Release v2024.1.0 This is a major release with introduction of CI/CD using Jenkins. 1. Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying new path for to create a fresh outdir. 2. Use Non Greenfield workflow to export the required OCI services into new excel sheet and the tfvars. Run terraform import commands also. 3. Once terraform is in synch, Switch to Greenfield workflow and use for any future modifications to the infra. Upgrade to Release v12.1 from v12 Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same outdir. Copy sddc.tf from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform_files/ to /cd3user/tenancies/ /terraform_files/ / . Copy the contents of modules directory from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform_files/modules/ to /cd3user/tenancies/ /terraform_files/ . Copy the sddcs variable block from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform_files/variables_example.tf and replace it in your variables_\\ .tf file Upgrade to Release v12 Upgrade to Release v11.1 from v11 Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same 1. Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying new path for to create a fresh outdir. Use Non Greenfield workflow to export the required OCI services into new excel sheet and the tfvars. Run terraform import commands also. Once terraform is in synch, Switch to Greenfield workflow and use for any future modifications to the infra.outdir. Upgrade to Release v11 Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying new path for to create a fresh outdir. Use Non Greenfield workflow to export the required OCI services into new excel sheet and the tfvars. Run terraform import commands also. Once terraform is in synch, Switch to Greenfield workflow and use for any future modifications to the infra. Upgrade to Release v10.2 from v10.1 Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same outdir. There are minor upgrades to terraform modules. In order to use the latest modules, copy the contents(modules directory and all .tf files from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform to /cd3user/tenancies/ /terraform_files/ . Move existing variables_\\<region>.tf to some backup and Copy OCI Connect Variables block from this file into variables_example.tf file and rename it to variables_\\<region>.tf Upgrade to Release v10.1 from v10 Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same outdir. There are minor upgrades to terraform modules. In order to use the latest modules, copy the contents(modules directory and all .tf files except variables_example.tf) from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform to /cd3user/tenancies/ /terraform_files/ . Execute terraform init -upgrade from outdir If you had utilized 'Upload current terraform files/state to Resource Manager' option under 'Developer Services' to upload terraform stack to Resource Manager for v10, then you must copy existing 'rm_ocids.csv' file from /cd3user/tenancies/ /terraform_files/ to /cd3user/tenancies/ /terraform_files/ before using this option again with v10.1. Changes to the CD3 excel sheet templates include correction of the dropdowns for all tabs, few changes in Policies tab wrt policy statements. So users can keep using the v10 templates. The new release supports a separate directory for each service. In order to use this feature for existing customers, execute createTenancy.py using a new outdir with 'outdir_structure_file' parameter set and then run export of the tenanncy into this new outdir using Non Greenfield workflow. Upgrade to Release v10 from v9.2.1 Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same outdir. Use new excel sheet templates to use OKE and SCH. For existing services, user can continue using existing outdir. | Main menu | | :---- |","title":"Upgrade to latest versions"},{"location":"Upgrade_Toolkit/#steps-to-upgrade-your-toolkit-for-existing-customers-using-older-versions","text":"","title":"Steps to Upgrade Your Toolkit (For Existing Customers using older versions):"},{"location":"Upgrade_Toolkit/#upgrade-to-release-v202410","text":"This is a major release with introduction of CI/CD using Jenkins. 1. Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying new path for to create a fresh outdir. 2. Use Non Greenfield workflow to export the required OCI services into new excel sheet and the tfvars. Run terraform import commands also. 3. Once terraform is in synch, Switch to Greenfield workflow and use for any future modifications to the infra.","title":"Upgrade to Release v2024.1.0"},{"location":"Upgrade_Toolkit/#upgrade-to-release-v121-from-v12","text":"Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same outdir. Copy sddc.tf from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform_files/ to /cd3user/tenancies/ /terraform_files/ / . Copy the contents of modules directory from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform_files/modules/ to /cd3user/tenancies/ /terraform_files/ . Copy the sddcs variable block from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform_files/variables_example.tf and replace it in your variables_\\ .tf file","title":"Upgrade to Release v12.1 from v12"},{"location":"Upgrade_Toolkit/#upgrade-to-release-v12","text":"","title":"Upgrade to Release v12"},{"location":"Upgrade_Toolkit/#upgrade-to-release-v111-from-v11","text":"Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same 1. Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying new path for to create a fresh outdir. Use Non Greenfield workflow to export the required OCI services into new excel sheet and the tfvars. Run terraform import commands also. Once terraform is in synch, Switch to Greenfield workflow and use for any future modifications to the infra.outdir.","title":"Upgrade to Release v11.1 from v11"},{"location":"Upgrade_Toolkit/#upgrade-to-release-v11","text":"Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying new path for to create a fresh outdir. Use Non Greenfield workflow to export the required OCI services into new excel sheet and the tfvars. Run terraform import commands also. Once terraform is in synch, Switch to Greenfield workflow and use for any future modifications to the infra.","title":"Upgrade to Release v11"},{"location":"Upgrade_Toolkit/#upgrade-to-release-v102-from-v101","text":"Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same outdir. There are minor upgrades to terraform modules. In order to use the latest modules, copy the contents(modules directory and all .tf files from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform to /cd3user/tenancies/ /terraform_files/ . Move existing variables_\\<region>.tf to some backup and Copy OCI Connect Variables block from this file into variables_example.tf file and rename it to variables_\\<region>.tf","title":"Upgrade to Release v10.2 from v10.1"},{"location":"Upgrade_Toolkit/#upgrade-to-release-v101-from-v10","text":"Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same outdir. There are minor upgrades to terraform modules. In order to use the latest modules, copy the contents(modules directory and all .tf files except variables_example.tf) from /cd3user/oci_tools/cd3_automation_toolkit/user-scripts/terraform to /cd3user/tenancies/ /terraform_files/ . Execute terraform init -upgrade from outdir If you had utilized 'Upload current terraform files/state to Resource Manager' option under 'Developer Services' to upload terraform stack to Resource Manager for v10, then you must copy existing 'rm_ocids.csv' file from /cd3user/tenancies/ /terraform_files/ to /cd3user/tenancies/ /terraform_files/ before using this option again with v10.1. Changes to the CD3 excel sheet templates include correction of the dropdowns for all tabs, few changes in Policies tab wrt policy statements. So users can keep using the v10 templates. The new release supports a separate directory for each service. In order to use this feature for existing customers, execute createTenancy.py using a new outdir with 'outdir_structure_file' parameter set and then run export of the tenanncy into this new outdir using Non Greenfield workflow.","title":"Upgrade to Release v10.1 from v10"},{"location":"Upgrade_Toolkit/#upgrade-to-release-v10-from-v921","text":"Follow the steps in Launch Docker Container to build new image with latest code and launch the container by specifying same path for to keep using same outdir. Use new excel sheet templates to use OKE and SCH. For existing services, user can continue using existing outdir. | Main menu | | :---- |","title":"Upgrade to Release v10 from v9.2.1"},{"location":"Workflows-jenkins/","text":"Using the Automation Toolkit via Jenkins Jenkins integraton with the toolkit is provided to jump start your journey with CI/CD for IaC in OCI. A beginner level of understanding of Jenkins is required. Pre-reqs for Jenkins Configuration The configurations are done when you execute createTenancyConfig.py in Connect container to OCI Tenancy . Please validate them: jenkins.properties file is created under /cd3user/tenancies/jenkins_home as per input parameters in tenancyConfig.properties An Object Storage bucket is created in OCI in the specified compartment to manage tfstate remotely. Customer Secret Key is configured for the user for S3 credentials of the bucket. A DevOps Project, Repo and Topic are created in OCI in the specified compartment to store terraform_files. GIT is configured on the container with config file at /cd3user/.ssh/config Bootstrapping of Jenkins in the toolkit Execute below cmd to start Jenkins - /usr/share/jenkins/jenkins.sh & Access Jenkins URL using: https://<IP of the Jenkins Host>:<Port> > Notes: <Port> is the port mapped with local system while docker container creation Eg: 8443. Network Connectivity should be allowed on this host and port. Please make sure to use a private server or a bastion connected server with restricted access(i.e. not publicly available). It will prompt you to create first user to access Jenkins URL. This will be the admin user. The Automation Toolkit only supports a single user Jenkins setup in this release. Once you login, Jenkins Dashbord will be displayed.","title":"Prerequisites"},{"location":"Workflows-jenkins/#using-the-automation-toolkit-via-jenkins","text":"Jenkins integraton with the toolkit is provided to jump start your journey with CI/CD for IaC in OCI. A beginner level of understanding of Jenkins is required.","title":"Using the Automation Toolkit via Jenkins"},{"location":"Workflows-jenkins/#pre-reqs-for-jenkins-configuration","text":"The configurations are done when you execute createTenancyConfig.py in Connect container to OCI Tenancy . Please validate them: jenkins.properties file is created under /cd3user/tenancies/jenkins_home as per input parameters in tenancyConfig.properties An Object Storage bucket is created in OCI in the specified compartment to manage tfstate remotely. Customer Secret Key is configured for the user for S3 credentials of the bucket. A DevOps Project, Repo and Topic are created in OCI in the specified compartment to store terraform_files. GIT is configured on the container with config file at /cd3user/.ssh/config","title":"Pre-reqs for Jenkins Configuration"},{"location":"Workflows-jenkins/#bootstrapping-of-jenkins-in-the-toolkit","text":"Execute below cmd to start Jenkins - /usr/share/jenkins/jenkins.sh & Access Jenkins URL using: https://<IP of the Jenkins Host>:<Port> > Notes: <Port> is the port mapped with local system while docker container creation Eg: 8443. Network Connectivity should be allowed on this host and port. Please make sure to use a private server or a bastion connected server with restricted access(i.e. not publicly available). It will prompt you to create first user to access Jenkins URL. This will be the admin user. The Automation Toolkit only supports a single user Jenkins setup in this release. Once you login, Jenkins Dashbord will be displayed.","title":"Bootstrapping of Jenkins in the toolkit"},{"location":"Workflows/","text":"Using the Automation Toolkit via CLI Prepare setUpOCI.properties Current Version: setUpOCI.properties v2024.1.0 Make sure to use/modify the properties file at /cd3user/tenancies/<customer_name>/ _setUpOCI.properties during executions. [Default] #Input variables required to run setUpOCI script #path to output directory where terraform file will be generated. eg /cd3user/tenancies/<customer_name>/terraform_files outdir= #prefix for output terraform files eg <customer_name> like demotenancy prefix= # auth mechanism for OCI APIs - api_key,instance_principal,session_token auth_mechanism= #input config file for Python API communication with OCI eg /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_config; config_file= # Leave it blank if you want single outdir or specify outdir_structure_file.properties containing directory structure for OCI services. outdir_structure_file= #path to cd3 excel eg /cd3user/tenancies/<customer_name>\\CD3-Customer.xlsx cd3file= #specify create_resources to create new resources in OCI(greenfield workflow) #specify export_resources to export resources from OCI(non-greenfield workflow) workflow_type=create_resources Variable Description Example outdir Path to output directory where terraform files will be generated /cd3user/tenancies/ /terraform_files prefix Prefix for output terraform files \\ auth_mechanism Authentication Mechanism for OCI APIs api_key config_file Python config file /cd3user/tenancies/ /.config_files/ _config outdir_structure_file Parameter specifying single outdir or different for different services Blank or _outdir_structure_file.properties cd3file Path to the Excel input file /cd3user/tenancies/ /testCD3. xlsx workflow_type Create Resources in OCI or Export Resources from OCI create_resources or export_resources Automation Toolkit Workflows: CD3 Automation Tool Kit supports 2 main workflows: Create Resources in OCI (Greenfield Workflow) - Empty OCI tenancy (or) do not need to modify / use any existing resources. Export Resources from OCI (Non-Greenfield Workflow) - Need to use / manage existing resources. Export existing resources into CD3 & TF State, then use the Greenfield workflow to modify them or create more resources on top of them. Execution Steps Overview: Choose the appropriate CD3 Excel Sheet and update the setUpOCI.properties file at /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties and run the commands below: Step 1 : Change Directory to 'cd3_automation_toolkit' cd /cd3user/oci_tools/cd3_automation_toolkit/ Step 2 : Place Excel sheet at appropriate location in your container and provide the corresponding path in /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties file Step 3 Execute the setUpOCI Script: python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties \u2192 Example execution of the script: [cd3user@25260a87b137 cd3_automation_toolkit]$ python setUpOCI.py /cd3user/tenancies/demotenancy/demotenancy_setUpOCI.properties Updated OCI_Regions file !!! Script to fetch the compartment OCIDs into variables file has not been executed. Do you want to run it now? (y|n): \u2192 This prompt appears for the very first time when you run the toolkit or when any new compartments are created using the toolkit. Enter 'y' to fetch the details of compartment OCIDs into variables file. \u2192 After fetching the compartment details, the toolkit will display the menu options.","title":"Overview"},{"location":"Workflows/#using-the-automation-toolkit-via-cli","text":"","title":"Using the Automation Toolkit via CLI"},{"location":"Workflows/#prepare-setupociproperties","text":"Current Version: setUpOCI.properties v2024.1.0 Make sure to use/modify the properties file at /cd3user/tenancies/<customer_name>/ _setUpOCI.properties during executions. [Default] #Input variables required to run setUpOCI script #path to output directory where terraform file will be generated. eg /cd3user/tenancies/<customer_name>/terraform_files outdir= #prefix for output terraform files eg <customer_name> like demotenancy prefix= # auth mechanism for OCI APIs - api_key,instance_principal,session_token auth_mechanism= #input config file for Python API communication with OCI eg /cd3user/tenancies/<customer_name>/.config_files/<customer_name>_config; config_file= # Leave it blank if you want single outdir or specify outdir_structure_file.properties containing directory structure for OCI services. outdir_structure_file= #path to cd3 excel eg /cd3user/tenancies/<customer_name>\\CD3-Customer.xlsx cd3file= #specify create_resources to create new resources in OCI(greenfield workflow) #specify export_resources to export resources from OCI(non-greenfield workflow) workflow_type=create_resources Variable Description Example outdir Path to output directory where terraform files will be generated /cd3user/tenancies/ /terraform_files prefix Prefix for output terraform files \\ auth_mechanism Authentication Mechanism for OCI APIs api_key config_file Python config file /cd3user/tenancies/ /.config_files/ _config outdir_structure_file Parameter specifying single outdir or different for different services Blank or _outdir_structure_file.properties cd3file Path to the Excel input file /cd3user/tenancies/ /testCD3. xlsx workflow_type Create Resources in OCI or Export Resources from OCI create_resources or export_resources Automation Toolkit Workflows: CD3 Automation Tool Kit supports 2 main workflows: Create Resources in OCI (Greenfield Workflow) - Empty OCI tenancy (or) do not need to modify / use any existing resources. Export Resources from OCI (Non-Greenfield Workflow) - Need to use / manage existing resources. Export existing resources into CD3 & TF State, then use the Greenfield workflow to modify them or create more resources on top of them.","title":"Prepare setUpOCI.properties"},{"location":"Workflows/#execution-steps-overview","text":"Choose the appropriate CD3 Excel Sheet and update the setUpOCI.properties file at /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties and run the commands below: Step 1 : Change Directory to 'cd3_automation_toolkit' cd /cd3user/oci_tools/cd3_automation_toolkit/ Step 2 : Place Excel sheet at appropriate location in your container and provide the corresponding path in /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties file Step 3 Execute the setUpOCI Script: python setUpOCI.py /cd3user/tenancies/<customer_name>/<customer_name>_setUpOCI.properties \u2192 Example execution of the script: [cd3user@25260a87b137 cd3_automation_toolkit]$ python setUpOCI.py /cd3user/tenancies/demotenancy/demotenancy_setUpOCI.properties Updated OCI_Regions file !!! Script to fetch the compartment OCIDs into variables file has not been executed. Do you want to run it now? (y|n): \u2192 This prompt appears for the very first time when you run the toolkit or when any new compartments are created using the toolkit. Enter 'y' to fetch the details of compartment OCIDs into variables file. \u2192 After fetching the compartment details, the toolkit will display the menu options.","title":"Execution Steps Overview:"},{"location":"cli_jenkins/","text":"Switch between using the toolkit via CLI and Jenkins UI Note - It is recommended to stick to using the toolkit either via CLI or via Jenkins. There can be scenarios when you need to update the terraform_files folder manually via CLI. Below are some examples: You executed setUpOCI script to generate tfvars for some resources via CLI. You updated variables_<region>.tf file to update image OCID or SSH Key for Compute or Database etc. Please folow below steps to sync local terraform_files folder to OCI DevOps GIT Repo: cd /cd3user/tenancies/<customer_name>/terraform_files git status Below screenshot shows changes in variables_phoenix.tf file under phoenix/compute folder. git add -A . git commit -m \"msg\" git push","title":"Synchronize Changes between CLI and Jenkins"},{"location":"cli_jenkins/#switch-between-using-the-toolkit-via-cli-and-jenkins-ui","text":"Note - It is recommended to stick to using the toolkit either via CLI or via Jenkins. There can be scenarios when you need to update the terraform_files folder manually via CLI. Below are some examples: You executed setUpOCI script to generate tfvars for some resources via CLI. You updated variables_<region>.tf file to update image OCID or SSH Key for Compute or Database etc. Please folow below steps to sync local terraform_files folder to OCI DevOps GIT Repo: cd /cd3user/tenancies/<customer_name>/terraform_files git status Below screenshot shows changes in variables_phoenix.tf file under phoenix/compute folder. git add -A . git commit -m \"msg\" git push","title":"Switch between using the toolkit via CLI and Jenkins UI"},{"location":"multiple_options_GF-Jenkins/","text":"Provisioning of multiple services together Note - For any service that needs Network details eg compute, database, loadbalancers ets, 'network' pipeline needs to be executed prior to launching that service pipeline. Multiple options can be selected simultaneously while creating resources in OCI using setUpOCI pipeline . But if one of the services is dependent upon the availability of another service eg 'Network' (Create Network) and 'Compute' (Add Instances); In such scenarios, terraform-apply pipeline for compute will fail as shown in below screenshot (last stage in the pipeline) - * Clicking on 'Logs' for Stage: sanjose/compute and clicking on the pipeline will dispay below - * Clicking on 'Logs' for Stage Terraform Plan displays - This is expected because pipeline for 'compute' expects network to be already existing in OCI to launch a new instance. To resolve this, Proceed with terraform-apply pipeline for 'network' and once it is successfuly completed, trigger terraform-apply pipeline for 'compute' manually by clicking on 'Build Now' from left menu.","title":"Provision multiple services together-Jenkins"},{"location":"multiple_options_GF-Jenkins/#provisioning-of-multiple-services-together","text":"Note - For any service that needs Network details eg compute, database, loadbalancers ets, 'network' pipeline needs to be executed prior to launching that service pipeline. Multiple options can be selected simultaneously while creating resources in OCI using setUpOCI pipeline . But if one of the services is dependent upon the availability of another service eg 'Network' (Create Network) and 'Compute' (Add Instances); In such scenarios, terraform-apply pipeline for compute will fail as shown in below screenshot (last stage in the pipeline) - * Clicking on 'Logs' for Stage: sanjose/compute and clicking on the pipeline will dispay below - * Clicking on 'Logs' for Stage Terraform Plan displays - This is expected because pipeline for 'compute' expects network to be already existing in OCI to launch a new instance. To resolve this, Proceed with terraform-apply pipeline for 'network' and once it is successfuly completed, trigger terraform-apply pipeline for 'compute' manually by clicking on 'Build Now' from left menu.","title":"Provisioning of multiple services together"},{"location":"prerequisites/","text":"Prerequisites Git Any docker CLI compatible platform such as Docker or Rancher. See How to Install and Configure Rancher Desktop Part 2 for reference. Local Directory - A directory in your local system that will be shared with the container to hold the generated Terraform files. OCI Tenancy Access Requirement - Appropriate IAM policies must be in place for each of the resources that the user may try to create. Minimum requirement for the user to get started is to have the ability to read to the tenancy.","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"Git Any docker CLI compatible platform such as Docker or Rancher. See How to Install and Configure Rancher Desktop Part 2 for reference. Local Directory - A directory in your local system that will be shared with the container to hold the generated Terraform files. OCI Tenancy Access Requirement - Appropriate IAM policies must be in place for each of the resources that the user may try to create. Minimum requirement for the user to get started is to have the ability to read to the tenancy.","title":"Prerequisites"},{"location":"remote_state/","text":"Store Terraform State into Object Storage Bucket [!Caution] When utilizing remote state and deploying the stack to OCI Resource Manager through the Upload current terraform files/state to Resource Manager option under Developer Services , attempting to execute terraform plan/apply directly from OCI Resource Manager may result in below error. This option is disabled while using the toolkit via Jenkins. While using it via CLI, you will have to remove backend.tf from the directory, bring the remote state into local and then upload the stack. Toolkit provides the option to store terraform state file(tfstate) into Object Storage bucket. This can be achieved by setting use_remote_state=yes under Advanced Parameters in tenancyconfig.properties file while executing createTenancyConfig.py . Upon setting above parameter the script will - create a versioning enabled bucket in OCI tenancy in the specified region(if you don't specify anything in remote_state_bucket_name parameter to use an existing bucket) create a new customer secret key for the user, and configure it as S3 credentials to access the bucket. Before executing the createTenancyConfig.py script, ensure that the specified user in the DevOps User Details or identified by the user OCID does not already have the maximum limit of two customer secret keys assigned. backend.tf file that gets generated - terraform { backend \"s3\" { key = \"<region_name>/<service_dir_name>/terraform.tfstate\" bucket = \"<customer_name>-automation-toolkit-bucket\" region = \"<region>\" endpoint = \"https://<namespace>.compat.objectstorage.<region>.oraclecloud.com\" shared_credentials_file = \"/cd3user/tenancies/<customer_name>/.config_files/<customer_name>_s3_credentials\" skip_region_validation = true skip_credentials_validation = true skip_metadata_api_check = true force_path_style = true } } For single outdir, tfstate for all subscribed regions will be stored as <region>/terraform.tfstate eg london/terraform.tfstate for london phoenix/terraform.tfstate for phoenix. See below screenshot showing objects in the bucket storing remote state: For multi outdir, tfstate for all services in all subscribed regions will be stored as <region>/<service_dir_name>/terraform.tfstate eg london/tagging/terraform.tfstate for tagging dir in london region. See below screenshot showing objects in the bucket storing remote state:","title":"Remote Management of Terraform State"},{"location":"remote_state/#store-terraform-state-into-object-storage-bucket","text":"[!Caution] When utilizing remote state and deploying the stack to OCI Resource Manager through the Upload current terraform files/state to Resource Manager option under Developer Services , attempting to execute terraform plan/apply directly from OCI Resource Manager may result in below error. This option is disabled while using the toolkit via Jenkins. While using it via CLI, you will have to remove backend.tf from the directory, bring the remote state into local and then upload the stack. Toolkit provides the option to store terraform state file(tfstate) into Object Storage bucket. This can be achieved by setting use_remote_state=yes under Advanced Parameters in tenancyconfig.properties file while executing createTenancyConfig.py . Upon setting above parameter the script will - create a versioning enabled bucket in OCI tenancy in the specified region(if you don't specify anything in remote_state_bucket_name parameter to use an existing bucket) create a new customer secret key for the user, and configure it as S3 credentials to access the bucket. Before executing the createTenancyConfig.py script, ensure that the specified user in the DevOps User Details or identified by the user OCID does not already have the maximum limit of two customer secret keys assigned. backend.tf file that gets generated - terraform { backend \"s3\" { key = \"<region_name>/<service_dir_name>/terraform.tfstate\" bucket = \"<customer_name>-automation-toolkit-bucket\" region = \"<region>\" endpoint = \"https://<namespace>.compat.objectstorage.<region>.oraclecloud.com\" shared_credentials_file = \"/cd3user/tenancies/<customer_name>/.config_files/<customer_name>_s3_credentials\" skip_region_validation = true skip_credentials_validation = true skip_metadata_api_check = true force_path_style = true } } For single outdir, tfstate for all subscribed regions will be stored as <region>/terraform.tfstate eg london/terraform.tfstate for london phoenix/terraform.tfstate for phoenix. See below screenshot showing objects in the bucket storing remote state: For multi outdir, tfstate for all services in all subscribed regions will be stored as <region>/<service_dir_name>/terraform.tfstate eg london/tagging/terraform.tfstate for tagging dir in london region. See below screenshot showing objects in the bucket storing remote state:","title":"Store Terraform State into Object Storage Bucket"},{"location":"singleclickdeploy/","text":"Click on the above button to directly navigate to Resource Manager stack in the OCI Tenancy and fill in required details to launch CD3 container. This action will initiate the deployment of the Work VM in the tenancy and configure the Automation Toolkit on a Podman container within that VM. Note: If not logged into the OCI tenancy, the button will redirect to the Oracle Cloud initial page, prompting entry of the tenancy name and login to OCI first. Fill in the required details in the Resource manager stack and click on create. After the Apply job is successful, Click on the created Job-->Logs. Scroll down to the end and find the details for the created VM, and commands to be executed to launch the toolkit container. Follow Connect container to tenancy for next steps. If you plan to do set up the toolkit container in a local system, Please follow the instructions specified in GETTING STARTED-LOCAL SYSTEM section.","title":"Single click container launch"},{"location":"tutorials/","text":"CD3 Blogs and Tutorials CD3 Blog Using CD3 to create resources in OCI Using CD3 to export resources from OCI","title":"Tutorials"},{"location":"tutorials/#cd3-blogs-and-tutorials","text":"CD3 Blog Using CD3 to create resources in OCI Using CD3 to export resources from OCI","title":"CD3 Blogs and Tutorials"}]}